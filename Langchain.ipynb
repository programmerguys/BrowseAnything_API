{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-08T08:07:51.433824Z",
     "start_time": "2023-10-08T08:07:51.426800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 已有的代码\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 加载其他环境变量\n",
    "config_openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "config_openai_api_base = os.environ.get('OPENAI_API_BASE')\n",
    "config_openai_proxy = os.environ.get('OPENAI_PROXY')\n",
    "config_openai_model_name = os.environ.get('OPENAI_MODEL_NAME')\n",
    "config_openai_temperature = os.environ.get('OPENAI_TEMPERATURE')\n",
    "config_openai_max_tokens = os.environ.get('OPENAI_MAX_TOKENS')\n",
    "config_openai_top_p = os.environ.get('OPENAI_TOP_P')\n",
    "\n",
    "config_embed_batch_size = os.environ.get('EMBED_BATCH_SIZE')\n",
    "config_embed_model_name = os.environ.get('EMBED_MODEL_NAME')\n",
    "config_embed_mode = os.environ.get('EMBED_MODE')\n",
    "\n",
    "config_node_parser_chunk_size = os.environ.get('NODE_PARSER_CHUNK_SIZE')\n",
    "config_node_parser_chunk_overlap = os.environ.get('NODE_PARSER_CHUNK_OVERLAP')\n",
    "\n",
    "config_service_context_num_output = os.environ.get('SERVICE_CONTEXT_NUM_OUTPUT')\n",
    "config_service_context_context_window = os.environ.get('SERVICE_CONTEXT_CONTEXT_WINDOW')\n",
    "\n",
    "config_graphsignal_name_test = os.environ.get('GRAPHSIGNAL_NAME_TEST')\n",
    "config_graphsignal_name_prod = os.environ.get('GRAPHSIGNAL_NAME_PROD')\n",
    "config_graphsignal_api_key = os.environ.get('GRAPHSIGNAL_API_KEY')\n",
    "\n",
    "config_promptlayer_api_key = os.environ.get('PROMPTLAYER_API_KEY')\n",
    "\n",
    "config_zep_api_url = os.environ.get('ZEP_API_URL')\n",
    "config_zep_api_key = os.environ.get('ZEP_API_KEY')\n",
    "\n",
    "config_milvus_collection_name = os.environ.get('MILVUS_COLLECTION_NAME')\n",
    "config_milvus_host = os.environ.get('MILVUS_HOST')\n",
    "config_milvus_port = os.environ.get('MILVUS_PORT')\n",
    "config_milvus_user = os.environ.get('MILVUS_USER')\n",
    "config_milvus_password = os.environ.get('MILVUS_PASSWORD')\n",
    "config_milvus_db_name = os.environ.get('MILVUS_DB_NAME')\n",
    "\n",
    "config_miniflux_base_url = os.environ.get('MINIFLUX_BASE_URL')\n",
    "config_miniflux_api_key = os.environ.get('MINIFLUX_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import graphsignal\n",
    "\n",
    "graphsignal.configure(\n",
    "    api_key=config_graphsignal_api_key,\n",
    "    deployment=config_graphsignal_name_test\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T08:07:53.353030Z",
     "start_time": "2023-10-08T08:07:53.280290Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import openai\n",
    "\n",
    "# 配置llm模型\n",
    "_temperature = float(os.environ.get('OPENAI_TEMPERATURE'))\n",
    "_model_name = os.environ.get('OPENAI_MODEL_NAME')\n",
    "_openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "_openai_proxy = os.environ.get('OPENAI_PROXY')\n",
    "_openai_api_base = os.environ.get('OPENAI_API_BASE')\n",
    "_max_tokens = int(os.environ.get('OPENAI_MAX_TOKENS'))\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def load_llm(temperature, model_name, openai_api_key, openai_proxy, openai_api_base, max_tokens):\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "    return ChatOpenAI(\n",
    "        temperature=float(temperature),\n",
    "        model_name=model_name,\n",
    "        streaming=True,\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_proxy=openai_proxy,\n",
    "        openai_api_base=openai_api_base,\n",
    "        max_tokens=int(max_tokens),\n",
    "    )\n",
    "\n",
    "\n",
    "llm = load_llm(\n",
    "    _temperature,\n",
    "    _model_name,\n",
    "    _openai_api_key,\n",
    "    _openai_proxy,\n",
    "    _openai_api_base,\n",
    "    _max_tokens\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T08:07:55.522108Z",
     "start_time": "2023-10-08T08:07:54.203285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miniflux API is ready\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from src.core.RSS import MinifluxAPI\n",
    "\n",
    "miniflux_api = MinifluxAPI(\n",
    "    base_url=config_miniflux_base_url,\n",
    "    api_key=config_miniflux_api_key,\n",
    ")\n",
    "print(miniflux_api.is_ready())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T08:07:56.193269Z",
     "start_time": "2023-10-08T08:07:56.049394Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 85, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '0d45130a5c45d65232243b9fdcae8e4f5ab41514c2f65ed1b259961a177076d2', 'title': 'CRM系统在销售管理中如何发挥其作用呢？', 'url': 'https://juejin.cn/post/7287108511929974825', 'published_at': '2023-10-08T04:23:10Z', 'created_at': '2023-10-08T07:22:38.724289Z', 'changed_at': '2023-10-08T07:22:38.724289Z', 'content': '<p>销售是企业的命脉，做好销售管理有着重大的意义，那么CRM系统在销售管理中如何发挥其作用呢？</p>\\n<p>一、实现以业绩为中心的管理</p>\\n<p>作为一个销售总监、销售经理核心的目标就是完成上级给予的业绩指标，业绩=目标=具体数字，所以作为公司销售管理的核心就是如何使销售完成我们的销售业绩指标。</p>\\n<p>CRM系统渗透以以业绩为中心的指标理念，控制面板的业绩指标，操作界面全部是业绩的提醒，这样应用系统的人无形中形成了一个业绩为中心的工作氛围。</p>\\n<p>不管是线索，机会，预测和各种报表都是围绕业绩来谈的，使用系统的人都会潜移默化的规范自己的销售行为，从而实现业绩为中心的行为化，制度化及信息化。</p>\\n<p>CRM系统能提醒到使用的销售人员在自己的销售库中最应该关注的应该是成功率高，项目金额大，签单时间近的项目情况，无形中建立起了一个通知和要事第一的科学平台。有关系统问题欢迎看博主名字一起交流学习。</p>\\n<p>二、以科学的绩效考核合格公平的评价为基本点</p>\\n<p>在销售领域最能提升正激励的影响的是销售人员的绩效考核和对销售人员的公平的评价，好的绩效和评价可以形成赶学比超的销售氛围，管理者对公司销售管理状态起到了决定性的作用。</p>\\n<p>CRM系统提出的绩效的考核，具有一定的行业竞争性；数据说话，数据录入是自己录入的，公平的评价来源于自我，销售人员如果是为了自己的目标工作，那么积极性和主观能动性将会发挥到极致；潜移默化的激励，系统的销售业绩排名、预测、统计、分析、区域、部门等各种报表有利于这种方式的强化，自我管理是最佳方式。</p>\\n<p>三、三个控制</p>\\n<p>公司销售管理报预测，过程控制，结果评估是贯穿整个年度的重要工作，结果评估是否及时和正面，意义也十分中大，CRM作为一种管理工具，就完全可以实现大量的数据分析，大量的业务人员的管理控制，从而让您的管理变得轻松自如。</p>\\n<p>1)预测，CRM系统通过相关联的信息，经过数据分析处理，可以实现销售业绩的预测，且预测金额直接和项目挂钩，实现销售成功率和销售金额预测的加权平均。</p>\\n<p>2)销售过程监控，经常碰到一些公司销售管理人员他们的困惑：为什么销售工作总是在维持而不是良性的发展？为什么很好的营销计划总是虎头蛇尾？为什么希望看到的市场和销售业务的起色总是看不到？答案只有一个过程没有控制好。过程的控制在与执行力在与高而有效的工作落实，这样才能保证销售业绩的完成；CRM系统中非常好的协作了你进行过程管理，你对销售漏斗的科学性指导，控制“过程”就等于控制把握住了“结果”，而“过程”的状态又取决于营销团队的表现，在追求营销管理的质量与阶段点滴效果的“过程”中，团队领导就能在这个过程中对团队成员起到指点、帮助、校正和提升的作用。</p>\\n<p>3)销售结果评估，CRM准确及时科学的平评估销售结果，为公司销售管理带来了便利.</p>\\n<p>四、实现四种沟通</p>\\n<p>上级沟通多为业务汇报，和解决方案索取，平级沟通更多为协调资源，下级沟通为业务指导及项目分析，客户沟通为信息收集。</p>\\n<p>CRM系统对于四种沟通的协同起到了非常重要的作用，提高了工作效率，同时建立一起一种规范的沟通语言，达到减少内耗、目标聚焦的作用。</p>\\n<p>五、五大工作职能</p>\\n<p>销售制度的建立、销售工具包改进、业绩项目统计分析和考核、销售项目跟踪管理、销售培训，实现五种工作职责。</p>\\n<p>1)知识库=销售工具而且是具有知识传递、更新、改进的知识库。</p>\\n<p>2)机会的附件链接=知识库的最有效的应用。</p>\\n<p>3)销售制度主要表现在系统的界面数据、流程、和统计分析、还有协助。</p>\\n<p>4)项目跟踪管理：销售漏斗是最好的管理方式，既能看到项目的分布情况，又可以看到项目的进展，还能看到项目的全部概况，最重要的是可以知道销售，告诉他，你的销售漏斗是否科学，你的销售项目是否符合逻辑，你的销售内容是否完整。</p>\\n<p>5)销售培训，在CRM系统中培训无处不在啊，好的CRM系统留有销售语言的录入和销售方法的指导，实时提供支持和帮助。</p>', 'author': 'AI科技WX_kelaile520', 'reading_time': 4, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 84, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '4f89bb99ff56eed91487f311c4bc6c6453f100075b0fd32f54a1d8eea963a387', 'title': 'ai智能语音机器人运营的核心', 'url': 'https://juejin.cn/post/7287125076368949311', 'published_at': '2023-10-08T04:14:25Z', 'created_at': '2023-10-08T07:22:38.71812Z', 'changed_at': '2023-10-08T07:22:38.71812Z', 'content': '<p>\\u200b</p>\\n<p>\\xa0运营机器人核心关注的只有两个点，一个是机器人话术库的关键词是否充足，二是对录音的高要求。每一个话术，都把它当成一个重要的项目来做。比如做需求调研，学习客户的行业知识，了解他们这个行业打电话的语气语调。其它的东西，能拿出来运营的机器人产品，核心的东西都不会差到哪去。有关系统问题欢迎看博主名字一起技术交流。<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e1589ad7c9f1423bb24c7f5f6a2bcc42~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=814&amp;h=232&amp;s=42565&amp;e=png&amp;b=fbfbfb\" alt=\"\" loading=\"lazy\"/><img src=\"https://juejin.cn/post/7287125076368949311\" alt=\"\" title=\"点击并拖拽以移动\" loading=\"lazy\"/>\\u200b编辑</p>\\n<p>第一，话术库的关键词想要充足，不是个简单的事情，没办法一步到位。这就是为什么机器人刚刚上线一个新话术，会显得有点傻，你问他东，他会回答你西，就是因为这个原因。那怎么丰富关键词呢，只能靠积累，机器人先根据人工外呼的脚本编制一个话术，收集以前人工交流时，客户都会问什么问题，把这些问题先添加到关键词库。然后在进行业务呼叫的过程中，再看客户会问什么，话术库还没有的关键词，把还没有添加的词语补充进去，经过一段时间的积累，机器人的回答就会越来越灵活。但是也有一个很重要的事情，这种灵活不能无止境，咱们是为了寻找意向客户，不是陪聊解闷，所以与业务不相关的词语，咱们关键词添加的时候就要注意了，这种词走挂机，拜拜就好了。</p>\\n<p>第二，机器人真人录音。有些机器人直接采用TTS转语音的方式来做，这种虽然方便，但是客户在接听电话的时候感知太机械，不太愿意接受。那么真人录音是个不错的选择。是不是谁都可以录呢，只要声音甜美。不是的，真人录音的目的是为了让接听的客户感知这是一个人工在和他对话，那么录音的时候这句话在哪里应该停顿，哪里应该音调要仰，哪里应该要抑，就很有讲究了。而且不同的行业，说话的语速又不一样，只有深入了解这个行业的人才能录的这么真实，才能真正了解他们的客户关心什么。录音的时候还有一个环境音也要注意，太安静了肯定不行，用户一听肯定就感觉很诡异，所以旁边有其它同事的电话声，敲键盘的声音，走动的声音等，营造一个电话中就能感知到的办公室现场办公的环境。</p>\\n<p>\\u200b</p>', 'author': 'AI科技WX_kelaile520', 'reading_time': 2, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 83, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '5ec1f57af1832d6ea27ad5bfbda36d1d4c48affaf997cb7b2ac2bf3f669bb2a6', 'title': 'SIP协议是什么？', 'url': 'https://juejin.cn/post/7287111042965995556', 'published_at': '2023-10-08T03:57:02Z', 'created_at': '2023-10-08T07:22:38.703002Z', 'changed_at': '2023-10-08T07:22:38.703002Z', 'content': '<p>1，SIP是类似于HTTP的基于文本的协议。</p>\\n<p>2，SIP可以减少应用特别是高级应用的开发时间。由于基于IP协议的SIP利用了IP网络，有关系统方面问题请找博主，看他名字一起交流学习。固定网运营商也会逐渐认识到SIP技术对于他们的深远意义</p>\\n<p>SIP(SessionInitiationProtocol,会话发起协议）是由IETF推出的一种用于建立、修改和终止多媒体会话的应用层控制协议。这些多媒体会话包括Internet多媒体会议、远程教育、Internet电话等。SIP侧重于将SIP网络电话作为互联网上的一个应用，所以SIP继承了互联网协议简单、开放、灵活的特点，既保证了对用户和会话的可管理性，又使网络核心负担大大减轻，实现可伸缩性。同时，与其他互联网应用（如FTP、E-mail等）相比较，SIP还增加了信令和QoS控制的要求。</p>\\n<p>SIP是一种基于文本的协议，它的语法和消息非常类似于HTTP协议，不同之处在于SIP不仅可以用TCP,也可以用UDP封装。SIP采用统一资源定位(URL,UniformResourceLocators)来指示会话的发起方(From八当前请求的目的地(RequestURL)和最终的接收方(To)。SIP的消息分为两大类：一类为客户发往服务器的请求消息；另一类为服务器返回的响应消息。SIP在消息体中采用SDP(SessionDescriptionProtocol,会话描述协议）来描述多媒体会话的媒体信息。SIP最大的特点是仅需利用已有的消息头字段，对其进行简单必要的扩充，就能很方便地支持各项新业务和智能业务，具有很强的灵活性和可扩充性。SIP协议天然具有对移动性的支持。SIP的动态注册机制，使用户端的移动变得十分方便。SIP协议为实现固定和移动业务的无缝融合创造了条件。3GPPR5版本已经选定SIP作为3G移动通信多媒体域的信令协议来实现基于IP的移动语音和多媒体通信。SIP的这些优越特征使其在NGN中占据很重要的主导地位。下图显示了一个典型的SIP呼叫建立的信令过程，具体过程如下。</p>\\n<p>建立呼叫的SIP信令过程</p>\\n<p>1、呼叫方对某个用户SIP:<a href=\"https://link.juejin.cn?target=mailto%3AuserA%40company.com\" title=\"mailto:userA@company.com\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">userA@company.com</a>发起INVITE请求，该请求中提供了足够的信息，以便被叫方能参加该会话，包括媒体流的类型和格式以及地址和端口等信息，以SDP格式来表示，INVITE请求被送到本地的SIP代理服务器A;</p>\\n<p>2、本地的SIP代理服务器A通过DNS查询，得到域中处理SIP请求的重定向服务器的IP地址，并将接收到的请求转发到该SIP重定向服务器；</p>\\n<p>3、SIP重定向服务通过查询，了解到用户<a href=\"https://link.juejin.cn?target=mailto%3AuserA%40company.com\" title=\"mailto:userA@company.com\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">userA@company.com</a>当前并不在本地，而是以<a href=\"https://link.juejin.cn?target=mailto%3AuserB%40univeristy.edu\" title=\"mailto:userB@univeristy.edu\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">userB@univeristy.edu</a>登记(的服务器可以通过静态配置数据库入口来获得这些消息，或者用户通过使用SIPREGISTER消息动态通知重定向服务器），然后该服务器将请求重定向到university.edu;</p>\\n<p>4、本地的SIP代理服务器A通过DNS查询，得到域中处理SIP请求的代理服务器B的IP地址，并将接收到的INVITE请求转发到该SIP代理服务器B;</p>\\n<p>5、域中的SIP代理服务器B查询本地的位置服务数据库；</p>\\n<p>6、本地数据库返回有关<a href=\"https://link.juejin.cn?target=mailto%3AuserB%40univeristy.edu\" title=\"mailto:userB@univeristy.edu\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">userB@univeristy.edu</a>的消息，指示该用户在本地是以<a href=\"https://link.juejin.cn?target=mailto%3AuserC%40cs.university.edu\" title=\"mailto:userC@cs.university.edu\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">userC@cs.university.edu</a>注册的；</p>\\n<p>7、域的SIP代理服务器B将该INVITE请求转发到域中的SIP代理服务器C;</p>\\n<p>8、域中的SIP代理服务器C知道用户<a href=\"https://link.juejin.cn?target=mailto%3AuserC%40cs.university.edu\" title=\"mailto:userC@cs.university.edu\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">userC@cs.university.edu</a>当前所登记的机器的IP地址，并将该INVITE请求转发到该地址；</p>\\n<p>9、被叫方<a href=\"https://link.juejin.cn?target=mailto%3AuserC%40cs.university.edu\" title=\"mailto:userC@cs.university.edu\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">userC@cs.university.edu</a>接受该呼叫，并发回一个响应。该响应沿着10、11、12到达呼叫方，然后呼叫方返回一个确认（图中未标出），至此呼叫已经建立，可以开始在它们之间直接传输媒体流了。</p>\\n<p>我公司多年的SIP通信行业经验，为行业提供方案与产品</p>\\n<p>SIP功能</p>\\n<p>SIP被描述为用来生成、修改和终结一个或多个参与者之间的会话。这些会话包括Internet多媒体会议，Internet（或任何IP网络）电话呼叫和多媒体发布。会话中的成员能够通过多播或单播联系的网络来通信。SIP支持会话描述，它允许参与者在一组兼容媒体类型上达成一致。它同时通过代理和重定向请求到用户当前位置来支持用户移动性。SIP不与任何特定的会议控制协议捆绑。本质上，SIP提供以下功能：</p>\\n<p>•名字翻译和用户定位：无论被呼叫方在哪里都确保呼叫达到被叫方、执行任何描述信息到定位信息的映射，确保呼叫（会话）的本质细节被支持。</p>\\n<p>•特征协商：它允许与呼叫有关的组（这可以是多方呼叫）在支持的特征上达成一致（注意：不是所有各方都能够支持相同级别的特征）。例如，视频可以或不可以被支持。总之，存在很多需要协商的范围。</p>\\n<p>•呼叫参与者管理：呼叫中参与者能够引入其他用户，加入呼叫或取消到其他用户的连接。此外，用户可以被转移或置为呼叫保持。</p>\\n<p>•呼叫特征改变：用户应该能够改变呼叫过程中的呼叫特征。例如，一个呼叫可以被设置为“voice-only”，但是在呼叫过程中，用户可以开启视频功能。也就是说一个加入呼叫的第三方为了加入该呼叫可以开启不同的特征。</p>\\n<p>SIP网络单元</p>\\n<p>SIP系统基本组成</p>\\n<p>SIP协议虽然主要为IP网络设计的，但它并不关心承载网络，也可以在ATM、帧中继等承载网中工作，它是应用层协议，可以运行于TCP,UDP,SCTP等各种传输层协议之上。SIP用户是通过类似于e-mail地址的URL标识，例如：sip:<a href=\"https://link.juejin.cn?target=mailto%3Amyname%40mycompany.com\" title=\"mailto:myname@mycompany.com\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">myname@mycompany.com</a>,通过这种方式可以用一个统一名字标识不同的终端和通信方式，为网络服务和用户使用提供充分的灵活性。按逻辑功能区分，SIP系统由4种元素组成：用户代理、代理服务器、重定向服务器以及注册服务器。</p>\\n<p>1.用户代理</p>\\n<p>用户代理(UserAgent)分为两个部分：</p>\\n<p>客户端(UserAgentClient),负责发起呼叫；</p>\\n<p>用户代理服务器(UserAgentServer),负责接受呼叫并做出响应。</p>\\n<p>二者组成用户代理存在于用户终端中。用户代理按照是否保存状态可分为有状态代理、有部分状态用户代理和无状态用户代理。2.代理服务器</p>\\n<p>代理服务器(ProxyServer),负责接收用户代理发来的请求，根据网络策略将请求发给相应的服务器，并根据收到的应答对用户做出响应。它可以根据需要对收到的消息改写后再发出。</p>\\n<p>3.重定向服务器</p>\\n<p>重定向服务器(RedirectServer),用于在需要时将用户新的位置返回给呼叫方。呼叫方可根据得到的新位置重新呼叫。</p>\\n<p>4.注册服务器</p>\\n<p>注册服务器(Registrar),用于接收和处理用户端的注册请求，完成用户地址的注册。</p>\\n<p>以上几种服务器可共存于一个设备，也可以分布在不同的物理实体中。SIP服务器完全是纯软件实现，可以根据需要运行于各种工作站或专用设备中。UAC,UAS,ProxyServer,RedirectServer是在一个具体呼叫事件中扮演的不同角色，而这样的角色不是固定不变的。一个用户终端在会话建立时扮演UAS,而在主动发起拆除连接时,则扮演UAC。一个服务器在正常呼叫时作为ProxyServer,而如果其所管理的用户移动到了别处，或者网络对被呼叫地址有特别策略，则它将扮演RedirectServer,告知呼叫发起者该用户新的位置。</p>\\n<p>除了以上部件，网络还需要提供位置目录服务，以便在呼叫接续过程中定位被叫方(服务器或用户端)的具体位置。这部分协议不是SIP协议的范畴，可选用LDAP(轻量目录访问协议)等。</p>\\n<p>理论上，SIP呼叫可以只有双方的用户代理参与，而不需要网络服务器。设置服务器，主要是服务提供者运营的需要。运营商通过服务器可以实现用户认证、管理和计费等功能，并根据策略对用户呼叫进行有效的控制。同时可以引入一系列应用服务器，提供丰富的智能业务。</p>\\n<p>SIP的组网很灵活，可根据情况定制。在网络服务器的分工方面：位于网络核心的服务器，处理大量请求，负责重定向等工作，是无状态的，它个别地处理每个消息，而不必跟踪纪录一个会话的全过程；网络边缘的服务器，处理局部有限数量的用户呼叫，是有状态的，负责对每个会话进行管理和计费，需要跟踪一个会话的全过程。这样的协调工作，既保证了对用户和会话的可管理性，又使网络核心负担大大减轻，实现可伸缩性，基本可以接入无限量用户。SIP网络具有很强的重路由选择能力，具有很好的弹性和健壮性。</p>\\n<p>对话启动协议提供了一种既可在终端设备或代理服务器之间，也可向网关设备传输呼叫控制信息的方法，像许多现有的Internet协议一样，SIP中也包含了常用的HTTP协议。</p>\\n<p>SIP被认为是一种轻量级协议，因为该协议使用的是能被终端设备轻易生成并分析的简单文本命令。SIP只使用6个指令管理呼叫控制信息，SIP协议的简单易行是极低成本应用系统选择该协议的重要原因。</p>\\n<p>SIP并未对媒体传输机制加以定义，因此该协议能用在媒体传输作为专用设备的应用系统，这可以提高效率并降低成本。SIP也允许通过任意数据报协议传输呼叫控制信息，使其在非TCP/IP环境（如Novell或其他专有协议）中也能有效使用。</p>\\n<p>SIP的部分优点包括：</p>\\n<p>该协议具有可扩展特性，可以轻松定义并迅速实现新功能。可以简单易行地嵌入廉价终端用户设备。该协议可确保互操作能力，并使不同的设备进行通信。便于那些非电话领域的开发人员理解该协议。</p>\\n<p>SIP的缺点包括：</p>\\n<p>SIP问世不久，因此大多数应用尚处于原型阶段。该协议单独应用的范围较窄。但与其他协议协同使用时，具有较强的灵活性。SIP只是完整解决方案的一小部分，还需要许多其他的软件来构建完整的SIP网络电话产品。</p>\\n<p>低成本终端产品无疑是SIP最自然的应用了，像无线电话、以太网电话及其他带有有限计算和内存资源的设备都能使用该协议。由于SIP是一种优越的呼叫控制协议，因此是当前取代MGCP呼叫控制协议的首选。</p>\\n<p>上述每个协议强调了开发SIP网络电话系统所需技术的不同方面，当前开发的许多系统中至少包含其中一种协议，而且这些协议通常要求互操作性。所有这些协议在构建复杂SIP网络电话系统的过程中不断发展。各制造商都在尝试研制具备互操作性的系统，所以互操作能力将继续成为未来的一大挑战。由IETF和ITU衍生而来的新标准协议MGC（H.248/Megaco）有望成为MGCP在传输市场最强有力的竞争对手。</p>', 'author': 'AI科技WX_kelaile520', 'reading_time': 9, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 82, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '5ee6ac230d3e2b39a0e8e1c711c10fc3f1e6b7b4c09d48d1542b40aa66b05c22', 'title': 'ai智能语音机器人是怎样进行呼叫的？', 'url': 'https://juejin.cn/post/7287144182966190134', 'published_at': '2023-10-08T03:50:56Z', 'created_at': '2023-10-08T07:22:38.655784Z', 'changed_at': '2023-10-08T07:22:38.655784Z', 'content': '<p>第一步，他需要有相应的录音人员进行录音，并将优化后的录音上传到系统中。 此时机器人已经具备说话的能力，但不具备理解客户并准确回答客户的能力。 第二步这一步需要用到的是市面上各大语音识别系统中的语音听写功能。</p>\\n<p>我们需要将听到的声音转换成文字，这样只要我们在系统中设置对应的答案关键词，机器人就会匹配对应的录音。 给出准确的答案。 比如你在系统中设置关键词价格、多少、怎么卖，这三个关键词都会触发答价的录音A。 当客户说“你的房子怎么卖”时，就会触及到怎么卖的关键。 外呼系统会立即响应，并自动用录音A接听客户。以上是他的基本对话原理。</p>\\n<p>那么它已经联系了客户，我怎么知道客户有没有意向呢？</p>\\n<p>目前，AI智能机器人已有5年多的发展历史。 系统也由原来的只播语音通知进行了更新，第七版现在支持对话中断功能，让客户说话时不说话。 问题后的即时答案完全可以与真人相媲美。 系统后台还拥有更强大的硬盘存储功能，可存储超过500G的录音文件。 挂断电话后，会立即在后台生成“未查看”列表。 点击后不仅可以试听，还可以观看与客户的对话。 文字对话的形式，和我们聊天一样的对话。</p>\\n<p>AI会根据客户询问的关键词数量划分客户意向程度，这样我们在后台跟踪处理的时候就可以查看到A型B型C型客户，然后给业务人员一个个的进行跟进，把握客户的第一时间进行关键销售！人工只需要做的事情就是等待AI筛选出意向客户后进行对接，节省大量的时间和精力！</p>', 'author': 'AI科技WX_kelaile520', 'reading_time': 2, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 80, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'cb40072811ec3f1b99e4fea5f2b5a5c32e631f6c2a3d66e7f37be6174ec4ec18', 'title': '如何让 GPT 输出稳定的 JSON', 'url': 'https://juejin.cn/post/7287050634833084474', 'published_at': '2023-10-07T14:22:40Z', 'created_at': '2023-10-07T17:26:50.906972Z', 'changed_at': '2023-10-07T17:26:50.906972Z', 'content': '<p>如何让 GPT 输出稳定的 JSON 结构是一个常见的需求，根据   <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fweibo.com%2F1727858283%2FNhkVKolQ8\" title=\"https://weibo.com/1727858283/NhkVKolQ8\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">@宝玉老师</a> 的提示，发现除了用 function calling，还可以用 TypeScript 去做限制，稳定输出 JSON 结果。</p>\\n<p>比如在下面这段 prompt 中，我希望 LLM 识别用户的输入，并提取其中用户名、性别、出生日期和个人介绍，最后结果输出 JSON：</p>\\n<pre><code>你叫做“妙妙”，是一款叫做“妙语笔记”的智能助手，接下来你会分析下面用户的输入：\\n&#34;&#34;&#34;\\n我的称呼是吴楷鹏，可以叫我大帅哥，出生于香港回归的那一年，生日是 3 月 13 号，喜欢上班\\n&#34;&#34;&#34;\\n设定：\\n1. 现在是 2025.10.01 21:21，时区是 Asia/Shanghai\\n2. 提取昵称、性别、出生日期，剩余全部信息整理成个人介绍\\n3. 要求输出结构化 JSON 对象，符合下面 TypeScript：\\ninterface UserInfo {\\n  nickname?: string;\\n  gender?: &#39;male&#39;  | &#39;female&#39;;\\n  dataOfBirth?: string;\\n  bio?: string;\\n}\\n4. 这是例子：const userInfo = {\\n    &#34;nickname&#34;:&#34;董小姐&#34;,\\n    &#34;gender&#34;: &#34;female&#34;,\\n    &#34;dateOfBirth&#34;:&#34;2001-03-07&#34;,\\n    &#34;bio&#34;: &#34;家住在长沙，喜欢做饭&#34;\\n}\\n\\n接下来开始分析：const userInfo=\\n</code></pre>\\n<p>这里使用 <code>interface</code> 这样的 TypeScript 语法限定输出的 JSON 对象，并且给出少量样例学习 few-shot，GPT4 稳定输出结果不带一丝废话：</p>\\n<pre><code>{\\n&#34;nickname&#34;: &#34;大帅哥&#34;,\\n&#34;gender&#34;: &#34;male&#34;,\\n&#34;dateOfBirth&#34;: &#34;1997-03-13&#34;,\\n&#34;bio&#34;: &#34;我叫吴楷鹏，出生于香港回归的那一年，喜欢上班。&#34;\\n}\\n</code></pre>\\n<p>而对于 GPT3.5-turbo，测试十几轮下来，也是稳定输出 JSON 不带任何废话。</p>\\n<p>给到文心一言，输出结果带了一些杂质：</p>\\n<pre><code>json\\n{  \\n  &#34;nickname&#34;: &#34;大帅哥&#34;,  \\n  &#34;gender&#34;: &#34;male&#34;,  \\n  &#34;dateOfBirth&#34;: &#34;1997-03-13&#34;,  \\n  &#34;bio&#34;: &#34;喜欢上班&#34;  \\n}```\\n</code></pre>\\n<p>智谱的 chatglm_pro 废话就比较多了：</p>\\n<pre><code>根据您提供的信息，我为您生成了一个结构化 JSON 对象：\\n```typescript\\nconst userInfo = {  \\n &#34;nickname&#34;: &#34;吴楷鹏&#34;,  \\n &#34;gender&#34;: &#34;male&#34;,  \\n &#34;dateOfBirth&#34;: &#34;1997-03-13&#34;,  \\n &#34;bio&#34;: &#34;喜欢上班&#34;  \\n};  \\n```\\n请注意，我在分析时发现您提到了“出生于香港回归的那一年”，根据您给出的日期（3 月 13 号），这一年应该是 1997 年。因此，我在生成的 JSON 对象中，将出生日期设置为 1997-03-13。现在，这个对象符合您所提供的 TypeScript 接口要求。\\n</code></pre>\\n<p>讯飞星火就更离谱：</p>\\n<pre><code>解析：\\n1. 首先，我们需要从输入的文本中提取出昵称、性别、出生日期和生日。\\n2. 然后，我们需要将这些信息整理成一个个人介绍的文本。\\n3. 最后，我们需要将这个个人介绍转换为一个结构化的JSON对象。\\n\\n代码如下：\\n\\n```\\nconst text = `我的称呼是吴楷鹏，可以叫我大帅哥，出生于香港回归的那一年，生日是 3 月 13 号，喜欢上班`;\\n\\n// 提取昵称、性别、出生日期\\nconst nicknameMatch = text.match(/可以叫我(.*?)，/);\\nconst genderMatch = text.match(/(.*?)，/);\\nconst birthDateMatch = text.match(/出生于(.*?)，/);\\nconst birthdayMatch = text.match(/生日是 (.*?) 号，/);\\n\\n// 整理成个人介绍\\nlet bio = &#34;&#34;;\\nif (nicknameMatch) {\\n  bio += &#34;昵称：&#34; + nicknameMatch[1] + &#34;，&#34;;\\n}\\nbio += &#34;性别：&#34; + genderMatch[1] + &#34;，&#34;;\\nbio += &#34;出生日期：&#34; + birthDateMatch[1] + &#34;，&#34;;\\nbio += &#34;生日：&#34; + birthdayMatch[1];\\nbio += &#34;，&#34;;\\nbio += &#34;剩余全部信息整理成个人介绍&#34;;\\n\\n// 输出结构化 JSON 对象\\nconst userInfo: UserInfo = {\\n  nickname: nicknameMatch &amp;&amp; nicknameMatch[1] || &#34;&#34;,\\n  gender: genderMatch &amp;&amp; genderMatch[1] || &#34;&#34;,\\n  dataOfBirth: birthDateMatch &amp;&amp; birthDateMatch[1] || &#34;&#34;,\\n  bio: bio,\\n};\\nconsole.log(userInfo);\\n```\\n</code></pre>\\n<p>国产大模型任重而道远。</p>\\n<p>👉 博客地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwukaipeng.com%2Ftechnique%2Fai%2Foutput-json\" title=\"https://wukaipeng.com/technique/ai/output-json\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">如何让 GPT 输出稳定的 JSON</a></p>\\n<p>👉 公众号：程序员楷鹏</p>', 'author': '吴楷鹏', 'reading_time': 5, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 79, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '363919e11654f41cd990b1b5cfb34a1f67ed1586d96d26bb91c12ffd518922e0', 'title': '腾讯智影一款让小白也能利用AI技术进行视频创作的工具', 'url': 'https://juejin.cn/post/7287028255986515987', 'published_at': '2023-10-07T13:07:26Z', 'created_at': '2023-10-07T17:26:50.905017Z', 'changed_at': '2023-10-07T17:26:50.905017Z', 'content': '<p>大家好，我是Mandy。</p>\\n<p>这是我给大家分享的第26款优质软件，其他更多的软件分享，直接访问该链接即可，<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fappmsgalbum%3F__biz%3DMzI4NzE2MDI5NA%3D%3D%26action%3Dgetalbum%26album_id%3D2424371091877281793%23wechat_redirect\" title=\"https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzI4NzE2MDI5NA==&amp;action=getalbum&amp;album_id=2424371091877281793#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">链接地址</a>。</p>\\n<p>今天给大家分享的是一款AI视频创作工具，名字叫做腾讯智影。腾讯智影是一款云端智能视频创作工具，集素材搜集、视频剪辑、渲染导出和发布于一体的免费在线剪辑平台。强大的AI智能工具，支持文本配音、数字人播报、自动字幕识别、文章转视频、去水印、视频解说、横转竖等功能，拥有丰富的素材库，极大提升创作效率，帮助用户更好地进行视频创作。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4eeb7408a48c4a3b9350e2c49547d1cb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1657&amp;h=746&amp;s=364680&amp;e=png&amp;b=fbf9f9\" alt=\"\" loading=\"lazy\"/></p>\\n<p>相信大家在日常的生活中，都有刷一些短视频。并且随着各式各样的视频平台发展起来，短视频因内容丰富、短小精悍；门槛低，创作过程简单；富有创意，具有个性化；传播迅速，互动性强；观点鲜明，内容集中；目标精准，触发营销效应等特点，也越来越受到用户的喜欢。</p>\\n<p>因此短视频的市场前景也是非常不错的，也有很多的人开始投入到短视频创作行业。俗话说工欲善其事必先利其器，要进入短视频行业，不仅要选对方向，而且对于工具上的选择也是非常重要的。相信大家都知道行业内有几款不错的软件，例如国外的软件DaVinci Resolve、Adobe Premiere Pro，以及国内的剪映。这几款软件也是市面上用得非常多的软件，但也有不足的地方，入门门槛高、收费、需要自己准备素材内容等问题。这无疑是增加了初学者的学习成本。</p>\\n<p>针对此问题，我强烈的向大家推荐一款，腾讯旗下的AI智能创作视频工具。这款软件叫做腾讯智影。它不仅能做视频剪辑，更重要的是利用AI技术，可以自动生成素材，例如文本素材、音频素材、视频素材等等。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/88062ba99b6f4c2a9b3f2aac494c37fd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2485&amp;h=1323&amp;s=1137897&amp;e=png&amp;b=252525\" alt=\"Snipaste_2023-10-07_20-00-00\" loading=\"lazy\"/></p>\\n<h2>软件入门</h2>\\n<p>要使用一款软件，首先我们要知道这款软件是干什么的？能做什么？有什么好处？前面也提及到了，腾讯智影支持文本配音、数字人播报、自动字幕识别、文章转视频、去水印、视频解说、横转竖等功能，它能方便、快捷的帮助我们进行视频创作，减少了创作者的前期的学习成本。</p>\\n<p>要使用该软件，首先通过官网地址，<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzenvideo.qq.com%2F%25EF%25BC%258C%25E8%25BF%259B%25E8%25A1%258C%25E7%2599%25BB%25E5%25BD%2595%25E3%2580%2582%25E6%258C%2589%25E7%2585%25A7%25E7%2595%258C%25E9%259D%25A2%25E7%259A%2584%25E7%2599%25BB%25E5%25BD%2595%25E6%25B5%2581%25E7%25A8%258B%25E8%25BF%259B%25E8%25A1%258C%25E7%2599%25BB%25E5%25BD%2595%25EF%25BC%258C%25E5%25B0%25B1%25E5%258F%25AF%25E4%25BB%25A5%25E7%259C%258B%25E5%2588%25B0%25E4%25B8%258B%25E9%259D%25A2%25E7%259A%2584%25E7%2595%258C%25E9%259D%25A2%25E3%2580%2582%25E6%2595%25B4%25E4%25BD%2593%25E7%259A%2584%25E7%2595%258C%25E9%259D%25A2%25E4%25B9%259F%25E6%2598%25AF%25E9%259D%259E%25E5%25B8%25B8%25E7%259A%2584%25E7%259B%25B4%25E8%25A7%2582%25EF%25BC%258C%25E5%25B7%25A6%25E4%25BE%25A7%25E6%2598%25AF%25E8%258F%259C%25E5%258D%2595%25E5%258C%25BA%25E5%259F%259F%25EF%25BC%258C%25E5%258F%25B3%25E4%25BE%25A7%25E6%25B0%259B%25E5%259B%25B4%25E5%258A%259F%25E8%2583%25BD%25E5%258C%25BA%25E5%259F%259F%25E3%2580%2581%25E5%2588%259B%25E4%25BD%259C%25E8%25AE%25B0%25E5%25BD%2595%25E5%258C%25BA%25E5%259F%259F%25E5%2592%258C%25E6%25A8%25A1%25E6%259D%25BF%25E5%258C%25BA%25E5%259F%259F%25E3%2580%2582%25E5%25BD%2593%25E6%2588%2591%25E4%25BB%25AC%25E6%2583%25B3%25E5%2588%259B%25E4%25BD%259C%25E4%25B8%2580%25E4%25B8%25AA%25E8%25A7%2586%25E9%25A2%2591%25E6%2597%25B6%25EF%25BC%258C%25E5%258F%25AF%25E4%25BB%25A5%25E7%259B%25B4%25E6%258E%25A5%25E4%25BD%25BF%25E7%2594%25A8%25E5%25B9%25B3%25E5%258F%25B0%25E6%258F%2590%25E4%25BE%259B%25E7%259A%2584%25E8%25A7%2586%25E9%25A2%2591%25E6%25A8%25A1%25E6%259D%25BF%25E8%25BF%259B%25E8%25A1%258C%25E4%25BA%258C%25E6%25AC%25A1%25E5%2588%259B%25E4%25BD%259C%25E3%2580%2582\" title=\"https://zenvideo.qq.com/%EF%BC%8C%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95%E3%80%82%E6%8C%89%E7%85%A7%E7%95%8C%E9%9D%A2%E7%9A%84%E7%99%BB%E5%BD%95%E6%B5%81%E7%A8%8B%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95%EF%BC%8C%E5%B0%B1%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E4%B8%8B%E9%9D%A2%E7%9A%84%E7%95%8C%E9%9D%A2%E3%80%82%E6%95%B4%E4%BD%93%E7%9A%84%E7%95%8C%E9%9D%A2%E4%B9%9F%E6%98%AF%E9%9D%9E%E5%B8%B8%E7%9A%84%E7%9B%B4%E8%A7%82%EF%BC%8C%E5%B7%A6%E4%BE%A7%E6%98%AF%E8%8F%9C%E5%8D%95%E5%8C%BA%E5%9F%9F%EF%BC%8C%E5%8F%B3%E4%BE%A7%E6%B0%9B%E5%9B%B4%E5%8A%9F%E8%83%BD%E5%8C%BA%E5%9F%9F%E3%80%81%E5%88%9B%E4%BD%9C%E8%AE%B0%E5%BD%95%E5%8C%BA%E5%9F%9F%E5%92%8C%E6%A8%A1%E6%9D%BF%E5%8C%BA%E5%9F%9F%E3%80%82%E5%BD%93%E6%88%91%E4%BB%AC%E6%83%B3%E5%88%9B%E4%BD%9C%E4%B8%80%E4%B8%AA%E8%A7%86%E9%A2%91%E6%97%B6%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E5%B9%B3%E5%8F%B0%E6%8F%90%E4%BE%9B%E7%9A%84%E8%A7%86%E9%A2%91%E6%A8%A1%E6%9D%BF%E8%BF%9B%E8%A1%8C%E4%BA%8C%E6%AC%A1%E5%88%9B%E4%BD%9C%E3%80%82\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">zenvideo.qq.com/，进行登录。按照界面的…</a></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c04467659fd3454db2b30592d79180d8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2498&amp;h=1337&amp;s=930543&amp;e=png&amp;b=fbfafa\" alt=\"Snipaste_2023-10-07_20-03-54\" loading=\"lazy\"/></p>\\n<h2>功能使用</h2>\\n<p>本篇文章不带领大家完整体验所有功能，以数字人创作功能进行演示。首先点击功能区域的数字人播到入口，接下来就会自动跳转到下图界面。左侧是我们的功能区，在这里我们可以选择视频素材，上传自己的素材，设置背景等操作。中间部分就是实时效果预览界面。右侧可以设置播到的文本，字幕的样式等功能。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e1ce7d333a86485d808a12b11a36b767~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2498&amp;h=1332&amp;s=1037460&amp;e=png&amp;b=f8eee9\" alt=\"Snipaste_2023-10-07_20-11-26\" loading=\"lazy\"/></p>\\n<p>同时我们可以根据系统提供的人物，选择不同的音色。例如男声、女声，新闻类型、娱乐类型、足球解说类型等等不同场景下的人物特色。同时还支持语音的播放速度，自定义音色等功能。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dc86489f7d48425da06aa8c760bf41bd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2136&amp;h=1237&amp;s=494678&amp;e=png&amp;b=96908d\" alt=\"Snipaste_2023-10-07_20-13-51\" loading=\"lazy\"/></p>\\n<p>当我们选择好视频素材之后，就需要填充具体的文本内容。腾讯智影不仅支持自定义文本内容，同时也支持选择不同的场景来自动生成文本内容。这里我就选择自定义文本内容。</p>\\n<p>当你准备好内容之后，填写到右侧播到内容框中，点击保存并生成播到，就能自动将文本添加到音频轨道中。同样可以实时预览和内容的修改等操作。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/62e3eb0a02d34a81a9f39ba0887e2d84~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2487&amp;h=1333&amp;s=595414&amp;e=png&amp;b=faf6f5\" alt=\"Snipaste_2023-10-07_20-18-53\" loading=\"lazy\"/></p>\\n<p>当你创作好内容之后，选择右上角的合成视频，此时会弹出视频合成的设置菜单。在这里你可以设置视频保存的格式，视频的水印，视频的分辨率，视频片尾等等。最后点击确定按钮即可，接下来只需要耐心的等待即可。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/12b46caeb1b44eab8c7908a6d44df311~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2201&amp;h=1204&amp;s=413920&amp;e=png&amp;b=959291\" alt=\"Snipaste_2023-10-07_20-22-21\" loading=\"lazy\"/></p>\\n<h2>二次创作</h2>\\n<p>在视频合成之后，如果我们想进行二次修改，也是支持的。二次修改的界面如下，在这里你可以设置视频字幕、视频转场效果、视频背景音乐，同时还支持视频中每一帧的素材，默认的情况下系统会自动根据视频的内容，帮你自动的选择图片素材。如果需要修改为自己准备好的素材，你可以直接将视频素材替换即可。也可以在底部音频轨道，设置你素材的播放时间等等操作。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/285f1072a6724327a864f1784617ade6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2498&amp;h=1335&amp;s=986101&amp;e=png&amp;b=252525\" alt=\"Snipaste_2023-10-07_20-28-06\" loading=\"lazy\"/></p>\\n<p>最后点击视频合成按钮即可，当合成好之后，你可以选择不同的视频格式下载到自己的电脑上，然后发布到其他的短视频平台。同时，你也可以将创作的内容直接发布到腾讯智影平台。</p>\\n<p>到此，数字人播到功能就演示完毕，更多的功能也需要你自己去使用。整体来说，都是非常的简单，直接按照界面提供的功能操作即可。</p>\\n<h2>优势与劣势</h2>\\n<p>任何一款软件都有它的优点和缺点，不管是大众喜欢的还是不喜欢的。一款软件即使功能在强大，使用在方便，不可能是没有缺点的。</p>\\n<h3>优势</h3>\\n<p>1、腾讯智影视一款利用了AI技术，进行视频创作的工具。很大程度上解决了创作素材难的问题，直接使用AI生成内容，然后在进行二次修改。</p>\\n<p>2、功能强大。上面也提到了腾讯智影支持文本配音、数字人播报、自动字幕识别、文章转视频、去水印、视频解说等等功能。这些功能基本上涵盖了所有视频创作的场景。</p>\\n<p>3、基于云端处理。大多数的视频创作工具，都是客户端的形式，需要将软件包安装到我们的电脑上。但腾讯智影直接一个浏览器就能进行视频创作，而且数据也是存储在云端，这极大的方便了我们视频存储的问题，只需要一部可以联网的电脑即可。</p>\\n<h3>劣势</h3>\\n<p>从我个人使用下来，软件功能上几乎是没有找到什么问题，最大的劣势就是收费太贵。即使你开通会员功能，有些功能也不一定能使用，这是因为它的会员也分不同的等级。不管是功能、存储空间、还是视频素材等内容都会存在收费的情况。这一点不像其他的剪辑软件，基本是支付一次软件购买费用，其他的就不会花钱。</p>\\n<p>到此，关于腾讯智影视频创作工具就结束了。对于软件的选择，大家可以根据自己的情况进行使用。</p>', 'author': 'Mandy的名字被占用了', 'reading_time': 5, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 81, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '6e13e4bccdae124605b06ab839d298fcd50a6e6c1523b4b19c8a36645645fbee', 'title': 'LLM - RAG文档应用处理与召回经验之谈', 'url': 'https://juejin.cn/post/7286912324593598522', 'published_at': '2023-10-07T09:10:38Z', 'created_at': '2023-10-08T05:22:37.908128Z', 'changed_at': '2023-10-08T05:22:37.908128Z', 'content': '<h2>一、前言</h2>\\n<p>本人最近一直从事开发 RAG（Retrieval Augmented Generation） 相关应用的一些开发上。其中一款项目的主要能力是用户上传文档后再提问，最后由 LLM 分析后解答一切关于此文档相关问题。这样一款知识库应用。相信大家都有用过此类似的使用体验。</p>\\n<h3>原理与现状</h3>\\n<p>先简单介绍一下原理， 我们对于此类知识库应用，一般处理分两块。第一块是文档处理，另一块是 文档 Retriever （召回）。文档处理分为以下几步：</p>\\n<ol>\\n<li>文档解析 - 对上传的文档进行解析成文本内容</li>\\n<li>文档切片 - 切成若干 chunk</li>\\n<li>向量存储 - 利用 embedding 模型将文本转换成向量，存放在向量数据库中。</li>\\n</ol>\\n<p>文档召回分为以下几步：</p>\\n<ol>\\n<li>向量搜索 - 利用余弦相似度算法活动与提问内容相关文档 chunk</li>\\n<li>LLM请求 - 将获取到的内容，放到LLM 对话上下文中</li>\\n<li>LLM返回 - LLM 根据你的上下文来回答问题</li>\\n</ol>\\n<h3>问题</h3>\\n<p>实际上，我们如果不考虑模型因素 ，仅仅只靠上述方案做的话，会碰到很多问题：</p>\\n<ol>\\n<li>如何保证文档切片不会造成相关内容的丢失？ 比如 我有一段文本，刚好是完整的，如果从中间切开，那么则会造成信息丢失，给 LLM 的内容则不完整。</li>\\n<li>文档切片的大小如何控制？ 太小则 容易造成信息丢失，太大则不利于向量检索命中。</li>\\n<li>文档召回过程中如何保证召回内容跟问题是相关的？ embedding 模型 可能从未见过你文档的内容，也许你的文档的相似词也没有经过训练。所以不能保证召回的内容就非常准确，不准确则导致LLM回答容易产生幻觉（简而言之就是胡说八道）。</li>\\n</ol>\\n<h3>经验</h3>\\n<p>目前 LangChain 在关于上述问题上，都有一套成熟的解决方案，在此我将介绍一下，如何用 LangChain 解决上述我所提到的问题。从而让模型回答的更好一些。</p>\\n<h2>二、方案详解</h2>\\n<h3>文档处理</h3>\\n<p>首先，我们采用 LangChain 的 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Fretrievers%2Fmulti_vector\" title=\"https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">MultiVector Retriever</a> ，它的主要能力则是在做向量存储的过程进一步增强文档的检索能力。之前 LangChain 有 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Fretrievers%2Fparent_document_retriever\" title=\"https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Parent Document Retriever</a> 采用的方案是用小分块保证尽可能找到更多的相关内容，用大分块保证内容完整性， 这里的大块文档是指 Parent Document 。</p>\\n<blockquote>\\n<p>为什么需要将 chunk 拆分大块和小块。这样的好处在于， 我们检索的文档可以保持一个细粒度，通过小块容易命中关键内容，但给LLM 的文档尽量保持一个完整通顺的内容，避免模型幻觉，所以在 Retriever 阶段返回的是大块的内容。</p>\\n</blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Fretrievers%2Fmulti_vector\" title=\"https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">MultiVector Retriever</a> 在 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Fretrievers%2Fparent_document_retriever\" title=\"https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Parent Document Retriever</a> 基础之上做了能力扩充。有了 Parent Document Retriever 那么为什么还有 MultiVector Retrieve ？ 这是在因为文档处理的时候， 我们希望进一步的增强检索能力。 比如下图的 summary 和 hypothetical，这是让 LLM 在回答之前，提前对文档做一个分析。请看下图，这个时序图解释整个 MultiVector 的执行过程。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0c562e348d594dc0997e792af7d389e0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1256&amp;h=2335&amp;s=579184&amp;e=png&amp;b=fefefe\" alt=\"bd7a10da52b2010a90e4996df579c141b0126263.png@1256w_2336h_!web-article-pic.png\" loading=\"lazy\"/></p>\\n<p>对 Parent Document 内容让 LLM 提前进行总结和提出一些假设性问题。 然后再将这三块内容（小块 chunk 的文档，总结性内容 和 假设问题 ） 存入向量数据库，并用 Parent Document ID 做关联。 <strong>进一步增强向量库的检索能力。</strong></p>\\n<blockquote>\\n<p>注意：（ Parent Document ，长度控制有一定的讲究，要根据你的模型可以允许的长度来，但是不能打满，因为系统提示词占了一部分，保持Parent Document维持在 2 到3 个给 LLM）</p>\\n</blockquote>\\n<h3>文档召回</h3>\\n<p>在召回阶段，我看到这样一个开源项目很有启发 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Frun-llama%2Fsec-insights\" title=\"https://github.com/run-llama/sec-insights\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">sec-insights</a> 它会将用户问题，采用多个不同的视角去提问，然后 LLM 会得出最终结果。当然如果这样回答 LLM 要调用多次，效率不高。我尝试改变了一下。</p>\\n<p>这里可以采用 LangChain 的 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Fretrievers%2FMultiQueryRetriever\" title=\"https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">MultiQueryRetriever</a> 主要原理则是 利用 LLM 尝试生成多个不同视角的问题，然后分别用这些问题做召回，然后再汇总。</p>\\n<p>比如下面实验的这个问题 “xx有哪些最新的功能？”</p>\\n<p>就比如生成了这样一系列的问题：</p>\\n<ol>\\n<li>xx有哪些最新的功能？</li>\\n<li>最新的xx功能有哪些？</li>\\n<li>xx有什么最新的功能可以使用？</li>\\n<li>最新的功能是否已经在xx中推出？</li>\\n<li>xx的新功能有哪些值得关注？</li>\\n</ol>\\n<p><strong>这样做的目的，我觉得是大多数人在问问题的过程中，如果不懂 prompt 工程，往往不专业，要么问题过于简单化，要么有歧义，意图不明显。那么向量搜索也是不准确的，导致LLM回答的效果不好。所以需要 LLM 进行问题的修正和多方位解读。</strong></p>\\n<p>最后我们看一下，在召回阶段如何利用上面的 summary 和 hypothetical ，进一步提高召回准确率？</p>\\n<p>我是这样处理的， 根据 多个 question , 召回三份小的 chunk 数据。 见代码如下：</p>\\n<pre><code>unique_docs_hypo = qdrant.search(vectordb_hypo,query_list,0.8,1)\\nunique_docs_sum = qdrant.search(vectordb_sum,query_list,0.7,2)\\nunique_docs_vec = qdrant.search(vectordb,[question] + query_list,0.6,20)\\n\\nunique_docs = unique_docs_hypo + unique_docs_sum + unique_docs_vec\\n</code></pre>\\n<p>我们将 hypothetical 的召回分数定高一点， 命中 hypothetical ，一般是较为明确的内容，设为0.8分 （非常高了，实验结果是，假设的问题几乎一样才能命中）</p>\\n<p>分数以此类推。然后得到一个总的 chunk docs 列表。 然后反查出 Parent Document List 丢给大模型去回答。</p>\\n<h3>文档重排</h3>\\n<p>最后一步则是文档重排 来自此论文 ： <a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fabs%2F2307.03172\" title=\"https://arxiv.org/abs/2307.03172\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/abs/2307.03…</a> LLM 对位置是相对比较敏感的，得分好的放在首或尾，LLM会重点关注。那么重排后，通过实验，模型回答的效果的确要好一些。在 LangChain 中也实现了此重排的方案 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Fdocument_transformers%2Fpost_retrieval%2Flong_context_reorder\" title=\"https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LongContextReorder</a> （详细见文档）</p>\\n<h3>Embeddings</h3>\\n<p>目前在 huggingface 公开的 Embedding 不少，目前中文能力较好的不多。本文选用了目前中文 SOTA <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FBAAI%2Fbge-large-zh\" title=\"https://huggingface.co/BAAI/bge-large-zh\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">bge-large-zh</a> LangChain 已经整合此模型，可以非常简单的<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FFlagOpen%2FFlagEmbedding%2Fblob%2Fmaster%2FREADME_zh.md%23using-langchain\" title=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md#using-langchain\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">使用</a>。</p>\\n<p>LangChain 提供了 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Ftext_embedding%2Fcaching_embeddings\" title=\"https://python.langchain.com/docs/modules/data_connection/text_embedding/caching_embeddings\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">CacheBackedEmbeddings</a> ， 可以提高 embedings 的二次加载和解析的效率，首次正常速度，后续有一个 3倍效率的提升。</p>\\n<h3>流程图</h3>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4138c74669ee414daff5416bc60f03c5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3074&amp;h=1122&amp;s=148614&amp;e=png&amp;b=ffffff\" alt=\"截屏2023-10-07 下午4.49.51.png\" loading=\"lazy\"/></p>\\n<h2>总结</h2>\\n<p>其实我也准备了一些示例，来演示和对比一下改进的效果。目前只对内演讲，对外不方便透露。但在召回效果和回答上，有了很大的提升。但是我觉得还存在进一步优化的空间，比如用户的问题完全不相关怎么处理？ 文档向量搜索完全不准（大概率未训练，内容属于专业知识），如何弥补。等等。其他后续能够继续为大家带来深入的分享和交流。重点：此文章点赞过千则开源代码。</p>', 'author': 'FelixCoder', 'reading_time': 7, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 78, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '8914dc31efc3d812ddbd00860591954961471b9e53f5b6a299b0a3c20da80ce4', 'title': '厉害了！ ChatGPT 看到一张图，就能画出来！', 'url': 'https://juejin.cn/post/7286828178815057983', 'published_at': '2023-10-07T07:17:05Z', 'created_at': '2023-10-07T17:26:50.899684Z', 'changed_at': '2023-10-07T17:26:50.899684Z', 'content': '<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f7b0e2550ad047ae898b8c88b461aa73~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1200&amp;h=686&amp;s=156202&amp;e=jpg&amp;b=0f0e0e\" alt=\"\" loading=\"lazy\"/></p>\\n<p>ChatGPT 最近一系列的更新简直炸裂，新出的 DALL·E 3 独领风骚，跟 Midjourney 有得一拼。它简单易用，只要会用 ChatGPT 就会使用 DALL·E 3，让你的 Idea 通过图像轻松地呈现出来。</p>\\n<p>DALL·E 3 又可以和最近新推出的图像识别功能结合起来，让你把看到的图片上传到 ChatGPT 并生成提示词，然后把这个提示词输入 DALL·E 3 生成相似的图像。让我们一起来看如何实现这一过程吧。</p>\\n<p>下图是我上一篇文章的封面，这张图的构图挺复杂的，不知道 ChatGPT 能否精准复刻这张图，让我们拭目以待吧！</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/934df074550043c594b6ef910fe76d0f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1792&amp;h=1024&amp;s=4085927&amp;e=png&amp;b=817a72\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在上传图片之前，首先要训练 ChatGPT，让它了解什么是 DALL·E 3 以及如何写提示词。虽然 DALL·E 3 知道如何写提示词但是 GPT-4V 的训练数据目前截止到 2022 年 1 月，因此它不知道什么是 DALL·E 3。</p>\\n<p>为了训练 ChatGPT，我把下面的提示词输入 GPT-4V。之所以没有输入到 DALL·E 3，因为它目前还不支持上传图片。</p>\\n<pre><code>Act as an DALL·E 3 expert. Let me first explain what DALL·E 3 is and how you\\nDALL·E 3 is a subsequent iteration of the original DALL·E, which is a variant of the GPT-3 model by OpenAI trained specifically to generate images from textual descriptions.\\nWriting an effective prompt for DALL-E 3 is crucial for obtaining the desired image outputs. Here are some guidelines and tips to craft a good prompt: \\n\\u200b\\n1. **Be Specific and Detailed**: Instead of writing &#34;a cat,&#34; specify &#34;a fluffy orange cat with large green eyes sitting on a blue cushion.&#34; The more detailed the description, the closer the generated image will be to your vision. \\n\\u200b\\n2. **Set the Scene**: If you have a particular setting in mind, describe it. For example, &#34;A serene beach during sunset with pink and purple hues in the sky, gentle waves, and a lone palm tree on the right.&#34; \\n\\u200b\\n3. **Specify Image Type**: If you have a preference for the type of image (e.g., oil painting, cartoon, photo, illustration), mention it at the beginning of the prompt. \\n\\u200b\\n4. **Include Composition Details**: If certain elements should be in the foreground, background, or specific locations, mention it. &#34;A large mountain in the background with a clear blue lake in the foreground and a campfire on the left.&#34; \\n\\u200b\\n5. **Use Descriptive Adjectives**: Colors, sizes, moods, and other adjectives can help DALL-E 3 understand the look and feel you want. &#34;A vibrant bustling market street filled with colorful stalls and diverse shoppers.&#34; \\n\\u200b\\n6. **Diversify Depictions**: If your image involves people, ensure that you specify details related to descent and gender for inclusivity and diversity. \\n\\u200b\\n7. **Avoid Ambiguities**: Ambiguous prompts can lead to unexpected results. Be as clear as possible about what you want. \\n\\u200b\\n8. **Limit Contradictions**: Ensure your description is coherent and doesn\\n\\u200b\\n9. **Experiment with Styles**: If you want an image inspired by older artistic styles or periods (keeping in mind the policy on recent artists), you can mention that. &#34;A scene reminiscent of a Van Gogh painting showing a starry night over a quiet town.&#34; \\n\\u200b\\n10. **Iterate and Refine**: If the initial image isn\\n\\u200b\\n11. **Limit Length**: While being detailed is beneficial, excessively long prompts might confuse the model. Aim for a balance between detail and brevity. \\n\\u200b\\n12. **Incorporate Emotions or Moods**: Describing the emotion or mood can help set the tone of the image. &#34;A tranquil forest glade bathed in soft morning light, giving a sense of peace.&#34; \\n\\u200b\\n13. **Avoid Complex Abstract Concepts**: DALL-E 3 works best with concrete descriptions. If you\\n\\u200b\\nDALL-E 3 offers three resolutions to fit your artistic needs:\\n- **Square (1024x1024):** The classic choice, ideal for most images and the default setting.\\n- **Wide (1792x1024):** Crafted for sprawling landscapes, panoramic views, or any artwork that leans towards a horizontal stretch.\\n- **Tall (1024x1792):** The pick for dramatic full-body portraits, towering structures, or anything that demands a vertical flair.\\nHere\\n&gt; Prompt: Full body portrait of a cat wearing safety goggles and a construction hat, inspecting the site with a serious expression. In the background, there\\n\\u200b\\nDALL-E 3 would instinctively opt for the 1024x1792 resolution. But if you\\nCraving a wide image? No problem! Adjust your prompt like this:\\n\\u200b\\n&gt; Prompt: A panoramic view of a cat wearing safety goggles and a construction hat, standing next to a miniature construction site with toy bulldozers and cranes. The cat appears to be inspecting the site with a serious expression, while a mouse in a suit holds a tiny blueprint next to it. In the background, there\\n\\u200b\\nOr you can simply use the term &#34;wide images,&#34; and DALL-E 3 will roll out images in the 1792x1024 dimension. It\\n\\u200b\\nDo you understand your role?\\n</code></pre>\\n<p>翻译一下：</p>\\n<pre><code>请扮演 DALL-E 3 专家。让我先解释一下什么是 DALL-E 3 以及如何为它生成提示词。\\nDALL-E 3 是原始 DALL-E 的后续迭代版本，它是 OpenAI 专门训练的 GPT-3 模型的变体，用于根据文本描述生成图像。\\n为 DALL-E 3 编写有效的提示词对于获得理想的图像输出至关重要。下面是一些编写好提示语的指南和技巧： \\n\\u200b\\n1. **具体详细**： 不要写 &#34;一只猫&#34;，而要具体说明 &#34;一只毛茸茸的橙色猫，一双绿色的大眼睛，坐在一个蓝色的垫子上&#34;。描述越详细，生成的图像就越接近你的想象。\\n\\u200b\\n2. **设置场景**： 如果您心目中有特定的场景，请对其进行描述。例如，&#34;日落时分的宁静海滩，天空中呈现出粉色和紫色的色调，海浪轻柔，右边有一棵孤独的棕榈树&#34;。\\n\\u200b\\n3. **指定图片类型**： 如果您对图片类型（如油画、漫画、照片、插图）有偏好，请在提示开头提及。\\n\\u200b\\n4. **包括构图细节**： 如果某些元素应位于前景、背景或特定位置，请注明。&#34;背景是一座大山，前景是清澈湛蓝的湖水，左边是篝火&#34;。\\n\\u200b\\n5. **使用描述性形容词**： 颜色、大小、情绪和其他形容词可以帮助 DALL-E 3 理解您想要的外观和感觉。&#34;一条热闹非凡的集市街道，到处都是五颜六色的摊位和形形色色的购物者&#34;。\\n\\u200b\\n6. **多样化描绘**： 如果您的图片涉及到人，请确保您指定了与血统和性别相关的细节，以实现包容性和多样性。\\n\\u200b\\n7. **避免模棱两可**： 模棱两可的提示可能会导致意想不到的结果。请尽可能明确您的要求。\\n\\u200b\\n8. **限制矛盾**： 确保您的描述连贯一致，不包含相互矛盾的细节。\\n\\u200b\\n9. **尝试不同风格**： 如果您希望图片的灵感来源于较早的艺术风格或时期（请牢记关于近期艺术家的政策），您可以这样说。&#34;一个场景让人想起梵高的画作，展现了一个宁静小镇的星空&#34;。\\n\\u200b\\n10. **反复推敲**： 如果最初的图像不太合适，可以通过添加或更改细节来调整您的提示词，然后再试一次。\\n\\u200b\\n11. **限制篇幅**： 虽然详细是有益的，但过长的提示可能会让模特感到困惑。应力求在细节和简洁之间取得平衡。\\n\\u200b\\n12. **融入情感或情绪**： 描述情绪或心情有助于确定图片的基调。&#34;宁静的森林小径沐浴在柔和的晨光中，给人一种安详的感觉&#34;。\\n\\u200b\\n13. **避免复杂抽象的概念**： DALL-E 3 最好使用具体的描述。如果您想表达一个抽象概念，请尽量将其分解为视觉元素。\\n\\u200b\\nDALL-E 3 提供三种分辨率以满足您的艺术需求：\\n- 正方形（1024x1024）：** 经典选择，适合大多数图像，也是默认设置。\\n- 宽（1792x1024）：** 适用于广阔的风景、全景或任何倾向于水平拉伸的艺术作品。\\n- 高 (1024x1792)：** 用于拍摄戏剧性的全身肖像、高耸的建筑或任何需要垂直风格的作品。\\n\\u200b\\n神奇之处就在这里： DALL-E 3 的直观设计意味着它能根据你的提示词自动判断最佳分辨率。假设您输入的提示是 &#34;全身肖像&#34;。\\n\\u200b\\n&gt; 提示词： 一只猫的全身像，它戴着安全护目镜和施工帽，表情严肃地检查工地。背景是一个标牌，上面写着 &#34;Paws 建筑公司&#34;。\\n\\u200b\\nDALL-E 3会本能地选择 1024x1792 的分辨率。但如果你是一个喜欢发号施令的人，只需加入 &#34;垂直图像 &#34;之类的术语，或指定你想要的精确分辨率即可。\\n\\u200b\\n想要宽幅图像？没问题！像这样调整提示：\\n\\u200b\\n&gt; 提示： 一只猫的全景图，它戴着安全护目镜和建筑帽，站在一个有玩具推土机和起重机的微型建筑工地旁。这只猫似乎正表情严肃地检查工地，旁边一只穿着西装的老鼠拿着一张小蓝图。背景是一个写着 &#34;Paws Construction Co. &#34;的牌子。\\n\\u200b\\n您也可以直接使用 &#34;宽图像&#34;，DALL-E 3 将以 1792x1024 的尺寸显示图像。所有这些都是为了给你创作自由，让你去设想和执行！\\n\\u200b\\n您了解自己的角色吗？\\n</code></pre>\\n<p>输入了上面的提示词训练 ChatGPT 以后，它这样回答：</p>\\n<blockquote>\\n<p>是的，我明白我的角色。您提供了一份关于如何为 DALL-E 3 制作有效提示词的深入指南。根据这些指导原则，我将协助您制作有创意的详细提示，以便使用 DALL-E 3 制作所需的图像。请提供您的图像概念或您心中的想法，我将根据您分享的指导原则帮助您制作有效的提示。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/230d22df7b184623b926f63b9061f006~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=996&amp;h=332&amp;s=78293&amp;e=jpg&amp;b=f7f5f5\" alt=\"\" loading=\"lazy\"/></p>\\n<p>好了，既然它已经懂了，那么我们就可以上传图片并让它生成对应的提示词了。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a3087e78ca7a4d7691db9001b2a647c0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=996&amp;h=332&amp;s=78293&amp;e=jpg&amp;b=f7f5f5\" alt=\"\" loading=\"lazy\"/></p>\\n<p>上传了图片之后，我让它生成 4 个提示词，一来是因为 DALL-E 3 默认会生成 4 张图片，二来是可以从这 4 张图中挑选一张最好的。这里的一个细节是我告诉了 GPT-4V 上传的图片是一张宽幅的图片。经我测试它还无法识别图像的宽高比。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1ab23301b70e4a159e161b208641a4a8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1005&amp;h=642&amp;s=117749&amp;e=jpg&amp;b=f0ebe9\" alt=\"\" loading=\"lazy\"/></p>\\n<pre><code>1. **逼真的艺术工作室氛围**：\\n \\xa0 &gt; 一幅广角图片，捕捉到一位才华横溢的艺术家，栗色波浪卷发披散在肩上，身穿一件褪色的蓝色牛仔衬衫。她正一丝不苟地在画布上为自己绘制一幅栩栩如生的肖像，捕捉她的每一根发丝和凝视的神情。画布上的肖像反映了她在现实世界中的特征，包括她淡褐色眼睛中闪烁的柔和光芒。背景是复古的木板，增添了工作室的质朴氛围。她的调色板上点缀着各种颜色，手中的画笔蘸着天蓝色，准备下一笔。\\n\\u200b\\n2. **艺术中的自我反思**：\\n \\xa0 &gt; 艺术家迷失在创作世界中的全景。她身着休闲牛仔衬衫，丰盈的棕色波浪在微妙的灯光下闪闪发光，她在拉伸的画布上画着自画像。画布上捕捉到了她令人着迷的细节--每一个卷曲的弧度、灰色上衣轻柔的垂坠感，以及眼神的穿透力。背景中的木质镶板彰显出一种古老的魅力，而她身旁的调色板则证明了她的用心，色调使她的形象栩栩如生。\\n\\u200b\\n3. **复古工作室背景**：\\n \\xa0 &gt; 在一个宽阔的环境中，仿佛回到了永恒的艺术工作室，一位拥有一头丰盈飘逸的黑发、身着柔软的蓝色衬衫的女士正在全神贯注地作画。她的主题不是别人，正是她自己，画布上精细地描绘着她的脸和上半身。从脸部轮廓到眼睛的倒影，每一个特征都被精准捕捉。木质壁板进一步丰富了这里的氛围，她身旁的调色板上色彩斑斓，诉说着过去许多肖像画的故事。\\n\\u200b\\n4. **追求完美的艺术**：\\n \\xa0 &gt; 艺术家的天堂全景：一位拥有一头浓密棕色波浪卷发、身着舒适蓝色牛仔衬衫的女士正在进行她的艺术之旅。她正在画一幅自画像，画布上展现的是一张与她一模一样的脸，从她富有表情的眼睛到布料的垂感，每一个细微之处都与她一模一样。质朴的木质背景，加上她色彩斑斓的调色板，增加了画面的层次感，描绘出一幅纯粹的艺术沉浸式场景。\\n</code></pre>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9370417e268f4c9281e42af990cb73b0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=995&amp;h=2043&amp;s=474360&amp;e=jpg&amp;b=f7f5f5\" alt=\"\" loading=\"lazy\"/></p>\\n<p>万事俱备！现在可以进入 DALL-E 3 并让它根据这 4 个提示词生成图像啦。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/65c4b147721b40108289f40aa9d854ee~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1471&amp;h=887&amp;s=302223&amp;e=jpg&amp;b=26231f\" alt=\"\" loading=\"lazy\"/></p>\\n<p>我觉得最后一幅图是比较接近原图的，放大来看下。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a0f7823c172e495eb05b0168cc647e9b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1792&amp;h=1024&amp;s=338002&amp;e=jpg&amp;b=171614\" alt=\"\" loading=\"lazy\"/></p>\\n<p>想继续了解如何使用 DALL·E 3 ，可以看这篇文章：</p>\\n<p>👉 <a href=\"https://juejin.cn/post/7286127842420752420\" title=\"https://juejin.cn/post/7286127842420752420\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">重磅！ChatGPT 可以进行 AI 绘图了，来看教程吧</a></p>\\n<p>想了解 DALL·E 3 和 Midjourney 之间的差异，可以阅读这篇文章：</p>\\n<p>👉 <a href=\"https://juejin.cn/post/7285346707138658358\" title=\"https://juejin.cn/post/7285346707138658358\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Midjourney 要完蛋了吗？DALL·E 3 效果如何？</a></p>\\n<p>有任何疑问或想法，欢迎评论区进行探讨！</p>', 'author': '我的AI力量', 'reading_time': 15, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 72, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '1c00daeba093b7ae645d8dd4d2f723c83da098bc1a2ec5260c08d62b9e1e90c1', 'title': '大模型遇上数智化，腾讯云与行业专家共探行业AI发展之路', 'url': 'https://juejin.cn/post/7286870395101282341', 'published_at': '2023-10-07T07:01:30Z', 'created_at': '2023-10-07T09:03:25.148401Z', 'changed_at': '2023-10-07T09:03:25.148401Z', 'content': '<h2><strong>引言</strong></h2>\\n<p>自去年底 OpenAI 推出 ChatGPT 起，大模型作为新的生产工具登上了新一轮生产力革命的舞台。事实上，数十年来历经了多次起落的 AI 技术，尽管一直被赋予着极高的期待，但在落地产业端时，却总显得差强人意。大模型的爆发究竟能否给产业端带来真正的变革？又会发生哪些巨变？</p>\\n<p>另一方面，企业的数字化转型已经是近年来全行业发展的重要战略，而数字化转型中的关键一环便是在最大程度上实现智能化。如何通过以大模型为代表的 AI 技术赋能企业数字化转型数智化？</p>\\n<p>带着以上问题，9 月 22 日，腾讯云TVP与腾讯智慧零售 、蒙牛集团 联合主办的“ TVP 走进蒙牛”闭门交流会成功举行。50 余位来自腾讯、蒙牛、中顺洁柔等企业的专家，一同分享了关于企业数智化转型的观点和实践。此外，来自不同行业的与会嘉宾也在最后的分组讨论环节中，对一些行业热点话题发表了各自的看法。</p>\\n<p>本次会议由清华大学五道口金融学院 “数字中国”项目创始人、腾讯云 TVP 行业大使唐鑫龙老师主持：“如今，大模型的技术发展越发火热，除了 C 端的应用层出不穷外，在 B 端大模型与各个行业的结合上也越发紧密。在接下来的分享和讨论中，希望听到来自各行各业的声音，把大家的视角从产业视角、技术视角进行自由分享和深度研讨。”</p>\\n<h2><strong>通用型的AI驱动将是有效抓手</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1b7687bb3cbe4109800c5e5b784c4d32~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=400&amp;s=22428&amp;e=webp&amp;b=329bcf\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在开场环节中，腾讯智慧零售技术架构与交付总经理 程伟表示，之所以此次腾讯云 TVP 走进蒙牛，主要因为蒙牛在整个 AI 数字化产业上位居前列，包括公司想要从奶制品生产商升级为每个人的健康顾问，这样的转型正需要 AI 来驱动。</p>\\n<p>事实上，当下零售行业面临着三大融合趋势：线上与线下融合、品牌与渠道融合，以及产品力的融合。结合未来社交化趋势愈显，微信、小程序、视频号的流量和活跃度都在持续增长，在如此多样化的场景中，零售企业的数字化转型又将面临哪些新的挑战？</p>\\n<p>程伟认为，AI 驱动以及行业的数字化将是非常好的抓手：“在零售业未来五年的发展当中，会有一个确定性的关键点，那就是营销的内容以及触达的方式非常重要。这些触达方式不再依托于个性化的技术和产品开发者，通用型的 AI 驱动将是非常有效的抓手。”</p>\\n<h2><strong>100人，100万吨产能，100亿产值</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9454b4fe2c1141dd956db3fdb2bbe246~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=400&amp;s=25888&amp;e=webp&amp;b=3199cd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>第二位嘉宾是蒙牛集团助理副总裁、腾讯云TVP行业大使 张决。据她介绍，蒙牛目前在全球有68家工厂，其中8家已完成数字化建厂。值得一提的是，今年5月刚刚建成投产的宁夏灵武工厂已经实现了“三个100”的纯数字化工厂目标：100个人，100万吨产能，实现100亿产值。</p>\\n<p>在蒙牛“FIRST发展战略”中，“T”即Technology。从成立之初，公司就非常注重信息化建设，从OA到SAP，再到LIMS，都进行了系统化的布局。</p>\\n<p>具体来看，2016年是蒙牛开启数字化建设的元年。公司先用三年时间进行了信息化升级，包括财务共享、智能制造（数智化工厂），这段时期被称为“一滴奶”的数智化治理1.0阶段。</p>\\n<p>从2020年开始，进入到2.0和3.0的建设阶段。其中，2.0的评价标准包括业务中台和数据中台的建设，以及管理数字化、能力内化等。3.0时代则是通过AI来驱动双飞轮（供给侧飞轮+消费侧飞轮），蒙牛集团CDO李琤洁女士在今年8月4日代表蒙牛发布全球首个营养健康领域模型MENGNIU.GPT，让数据资产继续高效发挥洞察和决策作用。</p>\\n<p>“通过与腾讯混元在内的多家大模型合作，我们内部已经完成了一些基于模型的产品构建，比如在数据层我们形成了蒙牛的营养健康知识图谱，已经可以在Wow小程序上进行对话。同时也开启了领域模型的建设，包括管理、营销、供应链等细分领域。在员工生产力的提升上，也开通了蒙牛自己的Copilot。”张决表示说。</p>\\n<h2><strong>数据+AI+大模型：构建一体化智能营销</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ce090973b4464f75b1fe48da66fce0ae~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=400&amp;s=20558&amp;e=webp&amp;b=329bcf\" alt=\"\" loading=\"lazy\"/></p>\\n<p>紧接着，腾讯企点副总经理 喻帅为现场嘉宾带来了主题为《智能应用在零售行业的分析》的分享。</p>\\n<p>自从零售业诞生以来，它的发展经历了以实体门店为主到平台电商，再到品牌社交兴起的三个阶段。如今，越来越多的线上商务平台利用微信等社交软件的推广做出了很好的营销业绩。不得不说，零售业的历史沿革见证了数据在沉淀和应用的过程中，是如何一步步走向数字化变革的。</p>\\n<p>“从数据沉淀到数字化是一个让人欣喜的自然趋势，但我们还是面临一个很大的问题，就是在真正进行数字化的建设当中，数据究竟能发挥什么价值？在社交电商或者说社交私域的运营过程中，能够带来哪些效果上的提升？这个问题目前还没有标准答案。”喻帅表示。</p>\\n<p>在他看来，一方面，数据作为生产资料，大量沉淀非常重要；另一方面，数据的有效性则需要通过 AI 来挖掘。换句话说，数据是“增长引擎”，AI 则是“创新引擎”。那么，落地到具体的营销场景中，AI 能够如何提升效率，同时能够带来哪些业务形式的创新呢？</p>\\n<p>很重要的一点，是解决数据和通路彼此割裂的问题。对此，喻帅先用四个“合适”来说明何谓“精准推荐”：“就是要把合适的内容，在合适的时间，通过合适的渠道，推给合适的人。”然而，现实却没有这么理想，面临着多重割裂：不同产品和模块数据的割裂，以及用户不同渠道中的身份割裂。</p>\\n<p>为解决这一问题，腾讯提供了“两化”方案：数据一体化和数据的场景化。首先，在数据一体化的建设上，整合“两平台、5A、5M”。两平台分别是指底层的 CDP 客户数据平台，以及顶层的 AB 试验平台。5A、5M 分别为分析应用和营销应用；其次，数据场景化则是借助大数据与 AI 技术，将品牌进行多触点的数据整合，从而精确识别用户身份和旅程阶段，在不同的场景下提供差异化的营销内容与策略。</p>\\n<p>此外，通过大模型的应用，腾讯也在进一步提升客户体验，提高营销转化效果。喻帅认为“我们的业务分析都很依赖数据的处理者，在人的能力参差不齐的情况下，很可能得出的结论，或者效率和方向都有偏差。希望通过我们的大模型，在给到充足语料的情况下，可以提供给用户相对及格的答案。”同时，喻帅也指出，仅仅是回答仍是不够的，如何通过大模型实现文生图、文生乐、文生视频，以及和数智人的联动，甚至生成风格化的 UGC 创意内容，也是未来腾讯探索的重点。</p>\\n<h2><strong>把握新技术机遇，未来是产业链和生态圈的竞争</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e22fab0de9c54dd2924b00d39cee2689~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=400&amp;s=28728&amp;e=webp&amp;b=3397d1\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在对“蒙牛的AI创新探索”这个主题的诠释中，蒙牛开放式创新总监 高璟琳首先介绍了蒙牛数智化战略3.0——AI驱动双飞轮的核心架构。基于AI中台，在蒙牛内部已经打造出牛慧问、牛慧画、牛魔王等一系列应用，经过提示词工程师培训考核的员工，已经在AI场景工厂上构建出400+多张场景卡片。</p>\\n<p>在外部，蒙牛在今年发布了营养健康模型MENGNIU.GPT以及AI营养师“蒙蒙”。并与腾讯数智人技术结合生成了生动的3D卡通形象，为给客户提供营养健康咨询服务。</p>\\n<p>通过回顾数智化创新探索的历程，高璟琳试图解答这样一个问题：当新技术出现时，如何才能把握住机会？</p>\\n<ul>\\n<li>首先需要行业洞察：正视消费者营养健康需求的猛增和营养健康知识供给的不足。经济的持续增长推动了人们对于营养健康的需求，而相关知识的供给方式单一、供给量不足，如中国营养师的供给缺口大概有400万。而AIGC技术的出现，对打破这种不平衡提供了新的可能。</li>\\n<li>其次把握技术趋势：我们又一次站在了历史的拐点，将见证和亲历新一轮的“所有行业都值得重做一遍”。大语言模型将成为底层新基建（MaaS），但需要在专业领域训练才能发挥更大价值。</li>\\n</ul>\\n<p>“一方面，大语音模型未来会成为新的基础建设；另一方面，要真正落地到专业领域，仅靠大语言模型的原生能力是不够的，需要在行业知识上进行更多专业的训练，这样才能发挥出更大的价值。”</p>\\n<p>在如此判断的基础上，蒙牛坚定地选择了全面拥抱AI，联手微软、智谱AI、腾讯、阿里等国内外的科技企业，共同训练调优模型，整合了市场上多个算法。并联合了一批营养健康领域的专家学者，与多个营养健康权威机构，将高质量内容作为训练素材一并灌入。此外，蒙牛还开放营养健康领域模型MENGNIU.GPT能力, 与更多的创新生态伙伴一起自由地探索和创造场景。</p>\\n<p>最后，高璟琳从封闭式创新和开放式创新两个维度，介绍了不同创新的路径选择。在他看来，封闭式创新和开放式创新是两种不同的范式：封闭式创新的典范如贝尔实验室，能产出影响人类文明的伟大创造；而开放式创新更活跃，讲求生态化，关乎到产业链和生态圈的竞争，是诸多国际大型企业的创新范式。</p>\\n<p>“产业链、供应链最终决定产品能做到什么程度，而生态圈则是更高维的竞争方式。希望通过开放式创新为蒙牛建立出这样一个生态，以MENGNIU.GPT为基础链接更多的可能性，共同创造数智营养健康的新体验。”</p>\\n<h2><strong>从成本中心到利润中心，未来是投资中心</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a79a25a52d684dac8cf033cb3ed81203~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=400&amp;s=20672&amp;e=webp&amp;b=3199cd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>对于“AI 时代的企业如何布局”，包括算法和大模型在零售行业的应用，中顺洁柔 CTO 杨森林分享了一些不同看法：“首先，我认为从当下的环境来讲，改变这个时代的不是 AI，而是驾驭 AI 的人。从技术角度来看，我们中国实际上没有自己的技术平台，也没有自己的技术语言和操作系统。就是说，一直以来我们都是技术的应用者或者说组装者，这才是我们的真实现状。所以，过于复杂的技术，像 AI，如果不是因为 ChatGPT 大火，可能这两个字都不会列在企业的日程上。”</p>\\n<p>而当进一步谈到企业数字化转型的现状，杨森林的看法是：行业整体上做得都不好。他向与会者展示了麦卡锡的一份报告：核心数据是企业数字化转型成功率仅为 20%。“实际上，20% 都是高估了，我们还是需要给自己清晰定位，通过应用AI想做什么，能做什么。”</p>\\n<p>那么，企业数字化转型的关键在于哪些因素？主要在于三点：系统、流程和人。</p>\\n<ul>\\n<li>系统：没有一个系统是第一版就能达到最佳使用的，持续不断的优化和迭代是打造各类系统的关键；</li>\\n<li>流程：再好的系统，也只能管理 50%-80% 的流程，仍然有 20%-50% 的流程、沟通、决策是在系统外做出的，将线下和线上的流程进行有效及高效对接是项目的核心挑战之一；</li>\\n<li>人：即使具备了好的系统和流程，缺乏可以高效使用的人，就像歼-20 飞机遇如果让普通人来开，起飞都困难。系统和流程应用者的思想转变，如何走出固有思维，拥抱新的方法，才是企业数字化转型能否成功的最关键因素。</li>\\n</ul>\\n<p>对于如何通过 AI 赋能数字化，他的观点是：相较于拥抱 AI，更应该驾驭 AI。据其观察，当下 AI 的价值产出主要在于以下几点：</p>\\n<ul>\\n<li>降低成本：包括优化算法、分布式训练加速和模型压缩；</li>\\n<li>提升易用性：通过完成直观、易用的用户界面设计，搭建简单、轻松上手的开发工具和平台，降低用户的使用门槛；</li>\\n<li>安全可解释：可以提升数据质量，同时增强鲁棒性，实现持续监控和升级；</li>\\n<li>数据安全：实现数据加密，访问控制和身份认证，同时进行安全审计和监控。</li>\\n</ul>\\n<p>基于“人”在 AI 赋能数字化转型中的关键作用，杨森林认为，作为企业数字化的掌舵者，CTO、CDO、CIO 应该成为“六边形战士”，需要懂战略、懂业务、懂技术、懂数据、懂流程、懂创新。</p>\\n<p>“如果我们对底层生意不了解，会发现很多规划，不管是模型、算法，还是系统，都会在天上飘着，落不了地。很多时候我们幻想出的业务场景和真实场景是完全不一样的，在数字化转型时应该时刻以企业自身情况为核心点，将数字化部门完成从成本中心到利润中心的转变，未来还可能进一步成为投资中心。”</p>\\n<h2><strong>技术架构演进原动力：生产工具、生产力和生产效率</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d43eb4f9b44f43edb1132c2271fe35bc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=400&amp;s=19302&amp;e=webp&amp;b=3199cd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>最后出场的分享嘉宾是腾讯云大数据产品总监 吕潇，他的演讲主题是《腾讯新一代大数据技术架构演进与探索》。</p>\\n<p>分享开始，他首先介绍了技术架构演进的原动力，主要在于三点：生产工具的安全稳定和灵活便捷，生产力上要求技术的先进性和低成本使用，生产效率上需要保证高效和易用。</p>\\n<p>“腾讯大数据技术发展经历过几个阶段，在第一阶段，腾讯内部在零几年的时候就用大规模分布式技术解决了很多在线业务大规模并行计算和扩展的诉求。从PC端转到移动端之后，我们通过AI能力展开了用户画像、商品推荐等业务，形成了各种各样的模型，这是第二个阶段。”</p>\\n<p>到后来，当团队发现传统AI技术在计算效率和准确度上无法满足业务需求时，便开始引入机器学习，通过深度学习和大模型快速构建了腾讯云的框架和体系，能够每天支持几百万次的模型迭代，从而更加高效和精准地完成用户画像。</p>\\n<p>“目前我们进入到第四个阶段，更多是把云、大数据、数据湖仓，甚至是一些跨源、协同计算、多边计算、安全计算、隐私计算等技术应用到企业各种各样的场景当中，不断进行技术革新。”</p>\\n<p>从2009年开始，腾讯自研大数据平台TBDS开始不断演进迭代，从3.0版正式商用，到4.0版的规模化突破，直至5.0版完成流批一体、湖仓一体和实时数据湖。如今该平台迭代到最新版本TBDS 5.3，实现了云原生和存算分离的技术架构升级。</p>\\n<p>具体来说，相较之前版本，TBDS 5.3在多个方面进行了升级：</p>\\n<ul>\\n<li>架构灵活提升——存算分离：相较于传统存算一体，存算分离可以让数据剥离，实现计算无状态化，达成秒级弹性、灵活伸缩；</li>\\n<li>生产效率提升——云原生化：实现资源的弹性伸缩，提高资源利用率，进行有效的资源隔离，达成敏捷、高效的部署和管理，统一生产、开发、测试等应用环境；</li>\\n<li>生产力提升——湖仓一体化：开放敏捷、弹性伸缩、混合负载、节约成本；</li>\\n<li>生产工具升级——数据开发治理平台 Wedata：达成全链路覆盖、效率提升和多团队协同。</li>\\n</ul>\\n<p>“（Wedata）从数据采集到数据开发，在数据建模、加工过程当中落地企业的数据标准落地，同时我们会对所以数据进行质量勘察，形成事前、事中、事后不间断的数据质量反馈和提升。最终能够帮助我们快速完成针对不同目标、不同组织和数据资产的业务重塑，这是我们目前提供的一整套数据全生命周期的能力。”</p>\\n<p>在当前国产化的浪潮下，腾讯在自主可控和技术领先性上完成了相当的沉淀，包括在大数据领域申请150+专利，大数据产品软著也达到70+。在行业影响和生态建设上，全面适配国产芯片、OS、服务器20+种，同时参与了国家/行业/团体标准制定30+项。在信创建设上，据吕潇介绍：“无论是政务，还是企事业单位，对信创的要求都非常高，我们在国税总局、中国银行这些非常大的单位都有大规模的落地。”</p>\\n<h2><strong>分组讨论</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/69758cf76ff24e56b9115309ec807267~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=400&amp;s=28586&amp;e=webp&amp;b=3199cd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>为了让本次活动与会嘉宾都能参与到“数字化转型和 AI 赋能”的话题探讨，在主会嘉宾演讲结束后，主持人唐鑫龙也组织了现场嘉宾的分组讨论环节。到场嘉宾被分为四组，分别对不同的议题进行了深入探讨，并结合自身企业实践，汇总了不同观点。</p>\\n<h3>未来大模型将会如何发展？有哪些期待？</h3>\\n<p>现场嘉宾认为，未来的大模型会分成三层：最顶层是像腾讯这样的技术公司，提供最普适的基础通用大模型能力；中间一层是像蒙牛这类行业头部公司，依据行业内积累，提供行业专属模型；最后则是一些行业重点企业，会结合自己的数据与需求，形成最后落地的模型。就是基础模型、行业模型和企业自己的模型。未来也希望更多的行业可以做到模型的标准化。</p>\\n<h3>大模型百花齐放，未来这一技术是否给全行业带来深远影响？</h3>\\n<p>现场嘉宾指出，今年年初 ChatGPT 火了之后，众多企业都开始做 AIGC 方面的尝试，也发现其实大家对生成式 AI 的期望经历了一条曲线式的变化，最初认为它什么都能干，几乎可以替换掉所有的岗位，但在实践过程中，这样的想法很快就遇到了上升瓶颈，虽然在生产力上确实有推动，但要是说能够替代哪个岗位还是距离很远的，所以也就慢慢冷静了。未来如何能够在私有大模型上做得更好，这是需要更多的企业共同去一道探索的。</p>\\n<h3>作为大健康领域的服务企业，新时代下对数据的获取和应用方式提出了哪些新要求？</h3>\\n<p>现场嘉宾认为，在大模型的时代，大健康服务企业数字化的关键在于数据、算法和算力三个方面，目前国内企业在算法上不差，和国外的差距主要体现在数据和算力上。腾讯云可以给我们提供算力上的支持，但在数据层还是需要我们企业自己解决。</p>\\n<p>那么，我们在预训练的过程中究竟需要什么样的数据？现在大部分的数据都是结果型，中间的过程数据还是缺失的，但实际上中间的推理型数据对我们的训练才是至关重要的，也是未来企业应该重点发力的方向。</p>\\n<h3>AI/大模型的落地还有哪些困难？</h3>\\n<p>针对该问题，现场嘉宾指出，关于 AI 我们有很多畅想，但要真正落地其实还面临两个比较大的困难。</p>\\n<p>第一个困难在于 AI 在正确率上的不确定性。我们一直说 ChatGPT 如何厉害，但主要是在于它能发挥的不涉及到正确率上的能力，比如一篇作文就不会说正不正确，只有好还是不好，是感受性的。我们不介意 AI 的准确率高不高，但是如果不能确定正确与否的问题，这就是一个很大的痛点。</p>\\n<p>第二个困难刚才大家也谈了不少了，就是行业模型还是很缺乏的。要做行业模型还是需要沉淀，只有沉淀进行了足够的技术与数据积累，才能厚积薄发，迎来新一轮更高的增长。</p>\\n<h2><strong>结语</strong></h2>\\n<p>人工智能时代的新一轮发展，为传统企业的数字化转型带来了又一次的增长推动力。该如何正确看待大模型技术的运用，在未来为企业带来的无限可能；又该在技术上如何更好地运用大模型技术，跟上技术发展，助推企业高效增长，这将是每一位企业技术管理者在未来深入研究分析的关键。</p>\\n<p>腾讯云 TVP 自成立之初，便怀揣着「用科技影响世界」的美好愿景，践行科技向善的初衷与本心，希望凝结更多专家的实践与思考，推动各行业数字化建设迈向新高度。</p>\\n<h2><strong>现场花絮集锦</strong></h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3b383d33677c46e8845dc688ef4d61fa~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=480&amp;s=43570&amp;e=webp&amp;b=b4c3d3\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8837ef87790b4cd2a8fdbd8e801461f5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=480&amp;s=40152&amp;e=webp&amp;b=b9c8d6\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bed68b39bb9d43aeade2c9e837a06bf9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=480&amp;s=41894&amp;e=webp&amp;b=c5cbd3\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8b64500a8d454848a2668b6988e6685d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=480&amp;s=34808&amp;e=webp&amp;b=b6c7d2\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f86ca23b50e047aba186582d6247e977~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=480&amp;s=39110&amp;e=webp&amp;b=b6c5d9\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b500a5e949164fdc8d21ad24ff3ff2af~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=480&amp;s=39994&amp;e=webp&amp;b=c8d1e1\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dae1d1e8384b45e1aee1bcb506fde105~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=480&amp;s=47978&amp;e=webp&amp;b=b3c2d4\" alt=\"\" loading=\"lazy\"/></p>', 'author': '腾讯云开发者', 'reading_time': 15, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 71, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '7671979506e9826e0c71da7e4ecf95d47e799fe80697d6246eea067e616fb41b', 'title': '大语言模型黑盒被打破；Meta 元宇宙硬件亏损可能高于市场共识丨 RTE 开发者日报 Vol.60', 'url': 'https://juejin.cn/post/7286870395101265957', 'published_at': '2023-10-07T06:59:35Z', 'created_at': '2023-10-07T09:03:25.145467Z', 'changed_at': '2023-10-07T09:03:25.145467Z', 'content': '<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f2cd3be6e522480a9c48323070a522de~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=1920&amp;s=441425&amp;e=png&amp;b=fbfbfb\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<p>开发者朋友们大家好：</p>\\n<p>这里是\\u3000<strong>「RTE 开发者日报」</strong>\\u3000，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE （Real Time Engagement） 领域内「有话题的\\u3000<strong>新闻</strong>\\u3000」、「有态度的\\u3000<strong>观点</strong>\\u3000」、「有意思的\\u3000<strong>数据</strong>\\u3000」、「有思考的\\u3000<strong>文章</strong>\\u3000」、「有看点的\\u3000<strong>会议</strong>\\u3000」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p>\\n<p><em>本期编辑：@Asui，@CY，@墨镜</em></p>\\n\\n<h2>01有话题的新闻</h2>\\n<p><strong>1、雷军王小川牵手，小米大模型投资布局首次曝光</strong></p>\\n<p>雷军和王小川的创业公司百川智能已成为雷军大模型布局新宠。雷军通过小米集团和顺为资本注资百川智能，并在其两轮融资中起关键作用。百川智能由搜狗创始人王小川于年内创建，专注于大模型开发。报告称，雷军对大模型的变革非常重视，他的策略是复制小米在移动互联网时代的策略，即投资并发展内部研究的同时推动外部投资。（@量子位）</p>\\n<p><strong>2、腾讯机器人实验室一号员工创业</strong></p>\\n<p>据报道，腾讯 RobticsX 机器人实验室一号员工来杰创业的仿生机器人项目有了新的进展。据悉，其创业公司名为星尘智能，主打项目是以轮式为底盘、以人形为上身的一体化设计仿生机器人。市场消息称，该公司今年的主要目标，是与头部 AI 公司合作，在年内推出原型机，而后在明年商业化落地，具体领域瞄准商用立体清洁、新零售和分拣等。</p>\\n<p>2018 年，腾讯 RoboticsX 机器人实验室成立，与 AI Lab 一同成为腾讯 AI 产业的两个支撑部门。当年来杰就以 RoboticsX 一号员工的身份，从百度转投腾讯门下。公开资料显示，这一阶段，来杰负责嵌入式和软件方向，管自平衡轮式机器人和轮腿式机器人两个项目（RoboticsX 迄今为止共发布过 4 个项目，分别为自平衡轮式机器人、轮腿式机器人、多模态四组机器人、触觉执行器）。（@量子位）</p>\\n<p><strong>3、小米官网暗示：小米汽车有望推出增程车型</strong></p>\\n<p>近日，小米社招官网上线了几个跟汽车相关的职位，而且都跟增程式汽车有关。具体来看，此次上线的职位包括增程系统设计开发工程师、燃油系统工程师、发动机悬置衬套工程师等。通过上述招聘可以看出，小米汽车有想要推出增程车型的想法，但目前无法确定是在第一代车型上推出增程版还是在第二款车型上使用。据上证报此前报道，小米汽车通州生产基地已在今年 9 月进入生产调试冲刺阶段。小米集团董事长雷军近期已带领小米汽车高层在新疆完成夏季新车路测，以争取在获得相关批文后尽快进入新车量产。（@IT之家）</p>\\n<p><strong>4、OpenAI 或将制造自己的 AI 芯片，已评估潜在收购目标</strong></p>\\n<p>媒体报道称，OpenAI 正在探索制造自研人工智能芯片，并已开始评估潜在的收购目标。报道称，至少从去年开始，OpenAI 就已讨论各种方案，以解决 AI 芯片短缺问题。OpenAI 已将获取更多 AI 芯片列为公司首要任务，讨论方案包括自研 AI 芯片，与包括英伟达在内的其他芯片制造商更密切地合作，以及在英伟达之外实现供应商多元化。</p>\\n<p>OpenAI CEO Altman 此前曾公开抱怨 AI 芯片的短缺，该市场目前由英伟达主导。外界分析认为，OpenAI 自研芯片旨在解决 GPU 的短缺，同时降低硬件运行所需的“令人眼花缭乱”的成本。目前还不清楚 OpenAI 是否会持续推进自研芯片的计划。业内资深人士表示，这一巨额投资每年的成本可能高达数亿美元。（@华尔街见闻）</p>\\n<p><strong>5、扎克伯格在元宇宙里一小时的“真人对话”，惊艳了全世界</strong></p>\\n<p>美国著名播客 Lex Fridman 与扎克伯格在元宇宙进行了一次对话，让距离上百英里的两人有了面对面的感觉。这主要归功于照片级精度的 3D 模型技术。他们的对话一小时视频短短时间播放量就冲上千万。Meta 的 Codec 虚拟现实化身使用扫描技术构建用户面部的 3D 模型，由 AI 驱动。扎克伯格也提到他们计划在未来几年让人们能够使用手机进行快速的扫描。（@腾讯科技）</p>\\n<p><strong>6、“作图新神器”Dall-E 3 引爆 Bing，微软周末紧急上线数千台服务器</strong></p>\\n<p>OpenAI 新一代作图模型——DALL·E 3 与多模态 AI 合并，能用简单的指令创造插画，制作复杂动图。模型接入 Bing 搜索引擎，增加了 Bing 的访问量，使其成为生产高质量 AI 图像的简单工具。多模态 AI 能够处理多种信息类型，推动智能系统向人工智能（AGI）发展，同时也增加了算力需求。未来几年，多模态 GPT 的发展将推动 AI 泛化能力提升，使得通用视觉、通用机械臂、行业服务机器人等进入生活。（@华尔街见闻）</p>\\n<p><strong>7、谷歌推出全球最大通用大模型之一 RT-X，并开放训练数据集！</strong></p>\\n<p>谷歌 AI 研究机构 DeepMind 发布全球最大通用大模型之一 RT-X 并开放训练数据集 Open X-Embodiment。RT-X 由控制模型 RT-1-X 和视觉模型 RT-2-X 组成，在特定任务的工作效率是同类型机器人的 3 倍，可执行未训练动作。Open X-Embodiment 由全球 33 家顶级学术实验室合作整合了来自 22 种不同机器人类型的数据开发。上海交通大学也参与了该项目。（@AIGC开放社区）</p>\\n<p><strong>8、未能战胜 Clubhouse，亚马逊宣布关闭旗下音频社交竞品应用 Amp</strong></p>\\n<p>10 月 6 日消息，Clubhouse 音频社交应用在 2021 年前后一度大火，亚马逊此前推出了一款名为 Amp 的同类应用，以试图与 Clubhouse 竞争。根据亚马逊数字音乐总监 Steve Boom 内部分发的消息，该应用目前已经正式关闭，仅运营了 18 个月。</p>\\n<p>在 2022 年 7 月，外媒 TechCrunch 声称该应用拥有超过 70 万月活跃用户，不过在 2022 年下旬，用户对此类音频社交应用的兴趣减退，Amp 应用用户没有迎来后续实质性的增长，反而逐步降低，在亚马逊去年的大规模裁员中，几乎有一半的 Amp 员工被解雇。（@IT之家）</p>\\n<p><strong>9、大语言模型黑盒被 CMU 等机构学者打破，LLM 矩阵全破解</strong></p>\\n<p>最近，来自 CAIS、CMU、斯坦福、康奈尔、马里兰、宾大等机构的学者有了令人震惊的发现——在大语言模型内部具有可解释的内部表征，人类可以控制这些表征来影响大模型的幻觉、偏见、危害，甚至可以做到监测出他们是否在撒谎。他们的研究探索同时也释放出人们对 AI 系统理解和控制会越来越精准，AI 系统也会因此大概率愈加安全可信的信号。（@新智元）</p>\\n<p><strong>10、杭州亚运会闭幕式将盛开 AR「记忆之花」</strong></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fd0e1a0580f9414d88e1a4ac9a5cee8f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=712&amp;s=1387634&amp;e=png&amp;b=d9d164\" alt=\"微信图片_20231007135149.png\" loading=\"lazy\"/></p>\\n<p>10 月 8 日，杭州亚运会闭幕式将在杭州奥体中心体育场「大莲花」举行。近日，杭州亚运会闭幕式总导演沙晓岚透露，整场闭幕式时长约 75 分钟，突出嘉年华氛围，邀请运动员和志愿者、工作人员等共同走进美丽大花园。\\n沙晓岚说，杭州亚运会闭幕式上仪式与演出将交替进行，由《璀璨共此时》《荷桂共生辉》《心心共相融》以及 4 个短片组成。4 个短片分别是《亚运聚欢潮》《精彩瞬间》《感动瞬间》《最忆是杭州》，其中只有 120 秒的《精彩瞬间》，收录了许多运动员的风采。</p>\\n<p>闭幕式采用去篇章化方式，突出「聚」（亚洲花园中的欢乐相聚）、「忆」（荷桂交接中的攀花赠友）、「惜」（主火炬熄灭时的依依不舍）、「和」（花车巡游中的和美共盼），以四类情感的递进为自然串联，从期盼到相会，从重逢到告别，以情感人、以情动人，留下别样精彩的杭州亚运记忆。\\n值得一提的是，总导演团队还通过 AR 技术，计算千余种荷花形态，形成独特的「记忆之花」，在大花园中绽放，体现「和合之美」。（@中国青年报）</p>\\n\\n<h2>02有态度的观点</h2>\\n<p><strong>1、孙正义：AI 智能程度或将是人类智力总和 10 倍</strong></p>\\n<p>当地时间 10月4日，软银集团创始人兼 CEO 孙正义表示，他相信通用人工智能(AGI)将在 10 年内成为现实，AGI 即“几乎在所有领域都超越人类智慧的 AI”。当天，孙正义在软银世界企业大会上发表演讲时预测，未来 AI 的智能程度将是人类智力总和的 10 倍。（@第一财经）</p>\\n<p><strong>2、郭明錤：Meta 元宇宙硬件亏损可能高于市场共识</strong></p>\\n<p>分析师郭明錤发文表示，META 的头戴装置（元宇宙）硬件事业因需求疲软造成的亏损可能高于市场共识。郭明錤最新调查指出，META 的头戴装置/元宇宙硬件出货量持续显著下滑，故缩编头戴装置/元宇宙事业对改善亏损帮助有限。Quest 3 最初的出货预估为在 2H23 达到 700 万部以上，但因预期需求疲软，故目前对今年下半年出货预估为 200–250 万部，2024 年出货量则约 100 万部。Quest 的出货量在 2023 将进一步显著同比下滑约 50% 至 350 万部，2024 年出货量不排除还有同比衰退可能。（@财联社）</p>\\n\\n<p><strong>写在最后：</strong></p>\\n<p>我们欢迎更多的小伙伴参与\\u3000<strong>「RTE 开发者日报」</strong>\\u3000内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p>\\n<p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p>\\n\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/07a3d5c1821b40029f6a007072094cbd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=241&amp;s=12390&amp;e=png&amp;a=1&amp;b=000000\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<p>素材来源官方媒体/网络新闻</p>', 'author': '声网', 'reading_time': 8, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 77, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '117ac637304cbb22ea34c7177ffcf22401b52662f0e322a614efe3d7bbc91473', 'title': '谷歌与OpenAI合作，用Google Search为大模型刷新！', 'url': 'https://juejin.cn/post/7286870395101102117', 'published_at': '2023-10-07T06:42:15Z', 'created_at': '2023-10-07T17:26:50.895485Z', 'changed_at': '2023-10-07T17:26:50.895485Z', 'content': '<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3d612db3f1eb460a9064c17e9785a249~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=608&amp;s=810103&amp;e=png&amp;b=666b5a\" alt=\"图片\" loading=\"lazy\"/>夕小瑶科技说 原创<br/>\\n编译 | \\xa0奶茶子</p>\\n<p>最近一项由Google、University of Massachusetts Amherst、OpenAI联合发布的研究探讨了大型语言模型（LLMS）面临的一个重要挑战，即它们在适应不断变化的世界时的困难。研究作者引入了一个新的动态问答基准测试，称为FRESHQA，旨在深入研究LLMS生成文本的准确性。他们对各种不同的LLMS在FRESHQA上的表现进行了评估，发现这些模型在处理涉及迅速变化的知识和纠正错误前提的问题时存在一定的限制。</p>\\n<p>此外，为了解决这一问题，研究团队提出了一种简单而高效的方法，即FRESHPROMPT。该方法通过从搜索引擎中获取相关和最新的信息，并将其融入LLMS的提示中，显著改善了模型在FRESHQA基准测试上的性能。</p>\\n<h3>大模型研究测试传送门</h3>\\n<p>GPT-4能力研究传送门（遇浏览器警告点高级/继续访问即可）：</p>\\n<p><a href=\"https://link.juejin.cn/?target=https%3A%2F%2Fgpt4test.com\" title=\"https://link.juejin.cn/?target=https%3A%2F%2Fgpt4test.com\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">gpt4test.com</a></p>\\n<p>**论文题目：<br/>\\n**<em><strong>FRESHLLMS: Refreshing Large Language Models with Search Engine Augmentation</strong></em></p>\\n<p>**论文链接：<br/>\\n**<em><a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2310.03214.pdf\" title=\"https://arxiv.org/pdf/2310.03214.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/pdf/2310.03…</a></em></p>\\n<h2>FRESHQA的构建</h2>\\n<p>大部分大型语言模型（LLMS）只训练一次，不再更新，因此无法动态适应不断变化的世界。为了研究了LLM生成的文本在回答测试当前世界知识问题时的真实性。作者提出了一个新颖的动态问答基准FRESHQA，其中包括600个问题，根据答案性质分为四个主要类别：</p>\\n<ul>\\n<li>永不变化，答案几乎不会改变</li>\\n<li>缓慢变化，答案通常在数年内发生变化</li>\\n<li>快速变化，答案通常在一年或更短时间内发生变化</li>\\n<li>虚假前提，包括前提事实不正确，因此必须被驳斥的问题</li>\\n</ul>\\n<p>作者要求注释员以两种不同的难度级别编写问题：</p>\\n<p><strong>单跳问题</strong>：其中问题明确提到了回答问题所需的所有相关信息，因此不需要额外的推理（例如，“谁是Twitter的CEO”）；</p>\\n<p><strong>多跳问题</strong>：其中问题需要进行一项或多项额外的推理步骤，以收集回答问题所需的所有相关信息（例如，“世界上最高建筑的总高度是多少”）。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/add9f95f31eb4f2d8c7949f766250529~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1055&amp;h=637&amp;s=230339&amp;e=png&amp;b=faebe8\" alt=\"图片\" loading=\"lazy\"/></p>\\n<h2>基于FRESHQA的测评结果</h2>\\n<p>作者通过向不同的LLM提出问题，并可选地提供一些问题-答案示范，然后对响应进行采样，来评估它们在FRESHQA上的表现。包括基本的预训练模型T5，PALM和PALMCHILLA等，一些指令调优模型FLAN-T5和FLAN-PALM，以及OpenAI的GPT-3.5，CODEX，CHATGPT和GPT-4。作者对模型响应事实准确性进行了两种模式的评估：<strong>RELAXED，仅衡量主要答案是否正确；</strong> \\xa0<strong>STRICT，衡量响应中所有声明是否事实和最新（即没有虚构）。</strong></p>\\n<blockquote>\\n<p>在评估前，两位作者独立地在这两种模式下评估了100个答案的子集，结果显示&#34;RELAXED&#34;模式下的一致性为99%，&#34;STRICT&#34;模式下的一致性为96%。这表明评估标准在比较不同LLMS时是可靠的。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/47f0fa7278b44394a83a8206262abde8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=759&amp;h=330&amp;s=89774&amp;e=png&amp;b=fbfbfb\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>▲图2</p>\\n<p>图2中展示了不同LLM在FRESHQA上的准确性，包括两种评估模式。首先，明显可见所有模型在FRESHQA上都面临着挑战。在STRICT模式下，总体准确率在0.8%到32.0%之间，在RELAXED模式下在0.8%到46.4%之间。将评估模式从RELAXED切换到STRICT模式会导致CHATGPT和GPT-4的准确率显著下降。主要原因是由于它们无法访问实时信息，因此产生了“过时”的答案，而在许多情况下，它们会“拒绝”提供答案（例如，“作为人工智能语言模型，我无法提供实时信息”）。同样，在STRICT模式下，PALM的准确性（跨模型大小）也显著下降。其中很大一部分下降是由于一些异常，比如具有意外特殊标记的类似对话的响应（例如，对话结束标记[eot]），以及虚构。相比之下，FLAN-PALM和CODEX由于其简明直接的答案几乎没有虚构。</p>\\n<h2>FRESHPROMPT后的测评</h2>\\n<p>在FRESHQA数据集上的低准确性在很大程度上并不令人意外，因为所有作者评估的模型都没有访问实时信息的能力。为了解决这个问题，研究团队提出了一种名为FRESHPROMPT的方法，通过将从搜索引擎（GOOGLE SEARCH）检索到的相关和最新信息合并到提示中，显著提高了LLM在FRESHQA上的性能。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/888c3a8539e64211864ab09c805742c1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=234&amp;s=66993&amp;e=png&amp;b=f8efec\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>将所有检索到的内容都转化为一个统一的格式，包括来源网页、日期、标题、文本摘录和高亮词语（左侧），向模型展示了一个示例问题以及问题的一系列检索到的证据，然后对这些证据进行推理以找出最相关和最新的答案（右侧）。</p>\\n<p>FRESHPROMPT方法利用文本提示来实现两个目标：</p>\\n<p>(1) 从搜索引擎中引入与上下文相关和最新的信息（包括与相关问题的答案），提供给预训练的LLM</p>\\n<p>(2) 教导模型对检索到的证据进行推理</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d27dad7a885840359567ada4c9ddc6af~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=641&amp;h=518&amp;s=158571&amp;e=png&amp;b=fdfdfd\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>▲表1</p>\\n<p>FRESHPROMPT显著提高了LLMs在FRESHQA的准确性，表1呈现了在STRICT模式下的具体实验数据，FRESHPROMPT在各个方面都显著提高了GPT-3.5和GPT-4的性能。在STRICT和RELAXED模式下，GPT-4 + FRESHPROMPT的绝对准确率分别提高了47%和31.4%。STRICT和RELAXED之间的绝对准确率差距减小（从17.8%减小到2.2%），这也表明FRESHPROMPT显著减少了过时和虚构答案的存在。除此之外，作者发现一些LLM具备推翻虚假前提问题的能力，例如在回答之前请检查问题是否包含有效前提。若将这个前提检查添加到GPT-3.5和GPT-4会在STRICT模式下分别提高虚假前提问题的准确率+23.4%和+6.4%（在RELAXED模式下分别提高+22.6%和+11.3%）。</p>\\n<h2>结语</h2>\\n<p>这种研究方法的确为整合动态变化的知识到语言模型中提供了一个创新方案，允许从Google Search中获取相关证据是一个强大的功能，可以使模型保持其答案的时效性，尤其在当前的快速变化的信息时代。但是该方法的一个主要限制是需要定期更新答案。这意味着，为了保持答案的准确性和时效性，必须持续投入资源和时间。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cbc3017df2294dc4b4e03190e80e29c0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=2300&amp;s=3050994&amp;e=png&amp;b=f8f7f7\" alt=\"image.png\" loading=\"lazy\"/></p>', 'author': '夕小瑶', 'reading_time': 6, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 76, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'f18c6eecd3e27a9cb8fe25e86dffc15d67134a1ccdd1e2c1f453e36b3351829d', 'title': '1007 早早聊 AGI 资讯｜StreamingLLM 框架突破文本长度限制、微软发布 GPT-4V 报告、Reka 推出多模态 AI 助手Yasa-1...', 'url': 'https://juejin.cn/post/7286883119112732724', 'published_at': '2023-10-07T06:02:15Z', 'created_at': '2023-10-07T17:26:50.889107Z', 'changed_at': '2023-10-07T17:26:50.889107Z', 'content': '<p>👉👉<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzaozaoliao.feishu.cn%2Fwiki%2FVXSBwl3RGirF9QksFVhcICjTnPg\" title=\"https://zaozaoliao.feishu.cn/wiki/VXSBwl3RGirF9QksFVhcICjTnPg\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>原文链接</strong></a>👈👈</p>\\n<h2><strong>「 行业动态 」</strong></h2>\\n<p><strong>◇ StreamingLLM 框架问世，号称“可让大模型处理无限长度文本” 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FXjvg_ifh5lPkoQ2gkhY2BQ\" title=\"https://mp.weixin.qq.com/s/Xjvg_ifh5lPkoQ2gkhY2BQ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>Meta、MIT、CMU 的研究者最近刚刚发表了一篇论文，介绍了一种名为「StreamingLLM」的方法，该方法允许有限上下文能力的语言模型支持几乎无限的上下文窗口。利用窗口注意力和注意力池等方法，将注意力集中在少量初始 token 上，从而克服了大型语言模型上下文长度的限制问题，同时通过注意力池的优化，保持了模型性能的稳定性，而且速度比重新计算滑动窗口快多达 22.2 倍。这一技术提供了改善模型性能和评估困惑度的方法，并建议在训练未来的 LLM 时使用接收器 token，以优化 LLM Stream 的部署。</p>\\n<ul>\\n<li>paper: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2309.17453.pdf\" title=\"https://arxiv.org/pdf/2309.17453.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/pdf/2309.17…</a></li>\\n<li>项目地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fmit-han-lab%2Fstreaming-llm\" title=\"https://github.com/mit-han-lab/streaming-llm\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/mit-han-lab…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fa30ad5b83b9440aac528a9406942340~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=301&amp;s=6958831&amp;e=gif&amp;f=124&amp;b=000002\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 微软发布 GPT-4V 报告，高级用户必读 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F8FtR6JcEFVcRLWCaANXQ6g\" title=\"https://mp.weixin.qq.com/s/8FtR6JcEFVcRLWCaANXQ6g\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>微软发布长达 166 页的报告，深入研究 GPT-4V 的功能和用途，强调其多模态能力和广泛的应用前景。 报告共分 11 章，重点关注 GPT-4V（ision）模型，探讨多模态大型模型（LMM）的理解和应用。介绍了 GPT-4V 的多种任务，包括对多模态输入的处理和通用性，以及视觉参考提示等新的人机交互方法。报告提出未来研究方向，强调 GPT-4V 的潜力和多领域应用，如医疗、汽车、保险等。还讨论了 GPT-4V 在图像理解、生成、智能体交互、GUI 交互等领域的应用。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2c556a4de7b14a4695896dd6dc7ea9c0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=1064&amp;s=899323&amp;e=png&amp;b=f7f0e8\" alt=\"\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c7cf2eb9403b4ebab6c640fbd8c905f1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=1512&amp;s=1121125&amp;e=png&amp;b=fcf6f2\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ Reka 推出多模态</strong> <strong>AI</strong> <strong>助手 Yasa-1，挑战</strong> <strong>ChatGPT</strong> <strong>🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fthe-decoder.com%2Fyasa-1-is-a-multimodal-ai-assistant-that-aims-to-compete-with-chatgpt%2F\" title=\"https://the-decoder.com/yasa-1-is-a-multimodal-ai-assistant-that-aims-to-compete-with-chatgpt/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>Reka 公司的多模态 AI 助手 Yasa-1 已推出，其能理解文本、图像、视频和音频，并与 OpenAI 的 ChatGPT 竞争。Yasa-1 支持 20 种语言，处理长上下文文档的速度比 Claude 2 快 8 倍，准确度相当。它可以进行定制以处理私人数据集，并可广泛应用于各种领域。它可以从互联网获取最新搜索结果，具有多模态功能，可以文本提示与多媒体结合，如社交媒体帖子或声音识别。Yasa-1 还能理解视频内容，预测下一个可能的动作，并支持编程任务，执行算术运算、分析表格或创建数据可视化。由于其可能存在不准确性和限制，不宜完全依赖</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eec00a51c95b44e299b58dce3492f014~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=973&amp;h=405&amp;s=193448&amp;e=png&amp;b=0a1424\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 芯片短缺、成本高昂，</strong> <strong>OpenAI</strong> <strong>考虑打造自己的</strong> <strong>AI</strong> <strong>芯片 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fthe-decoder.com%2Fopenai-considers-building-its-own-ai-chips-amid-chip-shortage-and-high-costs%2F\" title=\"https://the-decoder.com/openai-considers-building-its-own-ai-chips-amid-chip-shortage-and-high-costs/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>据路透社消息称，OpenAI 正在考虑开发自己的人工智能芯片，甚至评估了潜在的收购目标。面对芯片短缺和高成本，OpenAI 一直在探索各种选择，包括开发自己的 AI 芯片、与 Nvidia 等芯片制造商更紧密地合作以及实现供应商多元化。开发自己的人工智能芯片可能需要大量投资并需要数年时间。尽管尚未做出任何决定，但出于对 GPU 稀缺性以及运行其软件和硬件成本高昂的担忧，OpenAI 追求更多的 AI 芯片。</p>\\n</blockquote>\\n<p><strong>◇ Canva 发布全套</strong> <strong>AI</strong> <strong>设计工具 Magic Studio 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.chinaz.com%2F2023%2F1007%2F1564603.shtml\" title=\"https://www.chinaz.com/2023/1007/1564603.shtml\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>Canva 发布了一款名为 Magic Studio 的 AI 设计工具，旨在让内容创作更加易于访问，无需设计经验。Magic Studio 引入了 Magic Switch 等功能，可自动将设计转换为不同媒体格式，还支持将设计翻译成 100 多种语言。此外，Magic Media 工具新增了文本转视频功能，Magic Grab 可自动分离图像中的主题，Magic Expand 可将图像扩展到框架之外。Canva 还推出了 Canva Shield，以防止 AI 被用于创建不安全或不适当的内容，并计划在未来三年内拨款 2 亿美元，以支付设计师的补偿计划。</p>\\n<ul>\\n<li>体验地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmagicstudio.com%2Fzh%2F\" title=\"https://magicstudio.com/zh/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">magicstudio.com/zh/</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/184b2ed759e942b4b5029ac0d1119bc1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=605&amp;h=337&amp;s=328065&amp;e=png&amp;b=d0ebf3\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇</strong> <strong>ChatGPT</strong> <strong>：口袋里的医生 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2FSaboo_Shubham_%2Fstatus%2F1710171316819476872\" title=\"https://twitter.com/Saboo_Shubham_/status/1710171316819476872\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>ChatGPT 现在可以查看 X 光片、处方或医疗报告，并在几秒钟内回答任何问题。</p>\\n</blockquote>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/251d7468e4584fd897e54dd4614197cd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1002&amp;h=1069&amp;s=286230&amp;e=png&amp;b=302f3a\" alt=\"截屏2023-10-07 13.56.08.png\" loading=\"lazy\"/></p>\\n<p><strong>◇ EPIK 凭借怀旧年鉴照片功能登顶 App Store 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftechcrunch.com%2F2023%2F10%2F06%2Fai-app-epik-hits-no-1-on-the-app-store-for-its-viral-yearbook-photo-feature%2F\" title=\"https://techcrunch.com/2023/10/06/ai-app-epik-hits-no-1-on-the-app-store-for-its-viral-yearbook-photo-feature/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>EPIK 是一款照片编辑应用程序，提供怀旧年鉴照片编辑功能，在美国 App Store 中排名第一，在 Google Play 中排名第 37，全球范围内受到关注，特别在印度市场表现突出。自 2021 年 8 月以来，总安装量为 9230 万次，其中美国下载量为 470 万次。用户需要上传 8-12 张自拍照，由人工智能生成图像，可以选择不同的交付速度和价格。应用程序由 Naver 子公司韩国 Snow Corporation 开发。由于需求激增，目前可能存在延迟和交付问题。</p>\\n<ul>\\n<li>官网：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fepik.snow.me%2F\" title=\"https://epik.snow.me/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">epik.snow.me/</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1c736295df244a2cb8d804f0514d5a6a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1390&amp;h=891&amp;s=1152560&amp;e=png&amp;b=fbf5f5\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ CMU 华人打破大模型黑盒，Llama 2 撒谎被一眼看穿！ 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F0fPd86dMUaJmLAC3S_RZAw\" title=\"https://mp.weixin.qq.com/s/0fPd86dMUaJmLAC3S_RZAw\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>来自 CAIS、CMU、斯坦福、康奈尔、马里兰、宾大等机构的学者发现大型语言模型 （LLM） 不再是黑盒，其内部有可解释的表征，可以通过控制其行为和检测撒谎。研究者使用脑部扫描技术观察 LLM 在真理和撒谎情境下的大脑活动，并发现 LLM 内部具有一致的内部信念，可以用于提取真理概念。LLM 的表现可以通过刺激大脑区域来控制，包括影响其诚实度。该研究使 LLM 不再是黑盒，向更具解释性和可控性的方向迈出了一步，并探讨了概率和风险、情绪控制、无害指令遵循、偏见与公平、知识和模型编辑、记忆和未来展望等问题。未来工作可以研究表示的轨迹、流形和状态空间，以进一步改进 AI 系统的理解和控制，并使 AI 系统变得更可信和安全。</p>\\n<ul>\\n<li>paper: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2310.01405.pdf\" title=\"https://arxiv.org/pdf/2310.01405.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/pdf/2310.01…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c40e814861a841afbd7e593f23b637a5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=595&amp;s=455088&amp;e=png&amp;b=fcf7f7\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇</strong> <strong>GPU</strong> <strong>在哪里，</strong> <strong>AI</strong> <strong>的机会就在哪里 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FXrBuPKXBwYBIKKaKiYp2_w\" title=\"https://mp.weixin.qq.com/s/XrBuPKXBwYBIKKaKiYp2_w\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>AI 科技巨头的军备竞赛正在推动行业快速发展，市场规模超过每年 2000 亿美元。GPU 在 AI 领域的重要性日益凸显，与 AI 发展机会紧密相连。生成式 AI 技术的迅速发展为英伟达等公司带来了可观的盈利。为了维持高投资回报率，行业需要满足巨大的 GPU 和 AI 模型训练需求。财报显示，大量科技巨头正在承担数据中心扩建支出，英伟达的客户包括众多知名公司。AI 行业面临巨大的收入空缺，初创公司有机会填补这一差距。AI 行业基础设施建设的过度发展将促进未来的创新。未来 AI 基础设施不再是行业限制，关键问题是如何利用这一基础设施来改善人们的生活。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a18966c9f7fa4633bdc5c4b238109217~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=859&amp;h=485&amp;s=56606&amp;e=png&amp;b=faf6ee\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 用于科学应用的可扩展球形</strong> <strong>CNN</strong> <strong>🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2FGoogleAI%2Fstatus%2F1709641818747425228\" title=\"https://twitter.com/GoogleAI/status/1709641818747425228\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>ICML 2023 提出了名为&#34;Scaling Spherical CNNs&#34;的开源库，旨在解决处理球形信号的深度学习挑战，包括采样问题和旋转问题。该库实现了球形卷积，着重于提高速度，支持分布式训练，允许训练更大的模型。这一技术应用于天气预报和分子特性预测等任务，在分子属性回归和气象预测方面取得了出色的性能，同时也有潜力在其他科学应用、计算机视觉和 3D 视觉领域发挥作用。</p>\\n<ul>\\n<li>blog: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Ft.co%2FwXdIpkmtDy\" title=\"https://t.co/wXdIpkmtDy\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">goo.gle/46z3vD7</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2b923f5272644bf681bb52751e081dd9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1409&amp;h=786&amp;s=48590&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇</strong> <strong>LLMs</strong> <strong>和人工智能将为企业带来比移动或互联网革命更大的变革 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2Fbindureddy%2Fstatus%2F1710057623993635011\" title=\"https://twitter.com/bindureddy/status/1710057623993635011\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>如何为组织创建人工智能大脑，将大型语言模型（LLMs）与检索增强生成（RAG）相结合。员工可以使用定制的 ChatGPT 来查询内部公司数据，从代码问题到市场趋势和业务绩效。实现这些 ChatLLMs 或定制 ChatGPTs 涉及多个步骤，包括连接各种数据源、数据提取以及使用向量数据库进行数据检索和提示增强。最重要的是，RAG 是一种简单但高效的方式，可以使用 LLMs，而无需进行复杂的精细调整。这种简单而有效的方法可以为企业提供有力的工具，尽管不能实现通用人工智能，但有望改变业务运营方式并加速进步。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b8d73ccf06f14d46897fcfe39699043b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=680&amp;h=299&amp;s=180295&amp;e=png&amp;b=fefcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 大型语言模型能够理解空间和时间 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2Fbindureddy%2Fstatus%2F1709756676059062437\" title=\"https://twitter.com/bindureddy/status/1709756676059062437\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>现代大型语言模型（LLM）不仅仅是“随机模仿鹦鹉”，而实际上学会了线性表示空间和时间等基本维度。通过分析 LLM 在空间和时间数据集上学到的表示，研究者发现它们具备结构化知识，能够稳健地处理各种提示变化，并在不同实体类型之间保持一致。这一研究揭示了 LLM 如何以结构化和有意义的方式学习和理解复杂的现实世界信息，有助于增进对其性能的理解。</p>\\n<ul>\\n<li>paper: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fabs%2F2310.02207\" title=\"https://arxiv.org/abs/2310.02207\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/abs/2310.02…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1f62edb323634b47a5e841b810194344~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1199&amp;h=824&amp;s=1061452&amp;e=png&amp;b=fcfbfb\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 调整块大小是改进 RAG 应用程序时应该解决的首要问题之一 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2Fjerryjliu0%2Fstatus%2F1710300984654663962\" title=\"https://twitter.com/jerryjliu0/status/1710300984654663962\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>在改进 RAG 应用程序中，调整块大小是首要任务之一，但这并不总是直观的。增加块的数量并不一定意味着更好的性能，可能会导致中间问题或上下文溢出。此外重新排列检索到的块不一定会改善结果，事实上可能会使情况变得更糟。要确定最佳的块大小，需要定义一个评估基准，并进行块大小和前 k 值的扫描。</p>\\n<ul>\\n<li>Colab 笔记本：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Ft.co%2FpGZNGxeWJ7\" title=\"https://t.co/pGZNGxeWJ7\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">colab.research.google.com/drive/1Siuf…</a></li>\\n<li>幻灯片：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Ft.co%2FedICh3lNaC\" title=\"https://t.co/edICh3lNaC\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">docs.google.com/presentatio…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d016c8d3ae684e8a8546d499bcc20ab0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=680&amp;h=448&amp;s=121031&amp;e=png&amp;b=fcfcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 论文投 Nature 先问问</strong> <strong>GPT-4</strong> <strong>！ 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fposts.careerengine.us%2Fp%2F651f91454a1a5c29298acfaf\" title=\"https://posts.careerengine.us/p/651f91454a1a5c29298acfaf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Link</a></strong></p>\\n<blockquote>\\n<p>研究表明，GPT-4 在论文评审方面具有一定的潜力，但仍需要人类反馈辅助。斯坦福等大学的研究人员测试了 GPT-4 的论文评审能力，发现其提供的评审意见与人类评审员的一致率超过 50%，并且超过 82.4%的作者认为 GPT-4 的意见很有帮助。GPT-4 创建了一个自动 pipeline 来提取论文信息并提供评审意见，但在定量实验中，其意见与人类评审员的一致率在不同情况下有所变化。GPT-4 能够提供非通用的、有针对性的评审反馈，并且在重要、普遍问题上与人类评审员一致。用户研究发现，超过一半的研究员认为 GPT-4 的反馈很有帮助，但只有 82.4%认为它比至少一些人类评审员的反馈更有益。GPT-4 的局限性在于缺少特定技术领域的深度建议。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5536bc90eb494e05ba40f7980bdf65e6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=493&amp;s=83240&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 如何降低视觉 Transformer 计算成本？ 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fposts.careerengine.us%2Fp%2F651f912732f42c28be46fad2\" title=\"https://posts.careerengine.us/p/651f912732f42c28be46fad2\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Link</a></strong></p>\\n<blockquote>\\n<p>威斯康星大学麦迪逊分校的研究团队提出了 Eventful Transformer，它通过利用时间冗余来提高计算效率，从而降低了视觉 Transformer 的计算成本。该方法包含门控模块和缓冲器模块，用于感知时间冗余。门控模块选择性更新 token，以降低计算成本。适应性设计允许实时控制计算成本。实验结果表明，Eventful Transformer 不仅节省了计算成本，而且保持了高准确性，特别适用于视频识别任务。</p>\\n<ul>\\n<li>paper: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2308.13494.pdf\" title=\"https://arxiv.org/pdf/2308.13494.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/pdf/2308.13…</a></li>\\n<li>项目地址：<a href=\"https://link.juejin.cn?target=http%3A%2F%2Fwisionlab.com%2Fproject%2Feventful-transformers\" title=\"http://wisionlab.com/project/eventful-transformers\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">wisionlab.com/project/eve…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe6bc600e79c4e469e7a7c68ba4ebe61~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=646&amp;h=1011&amp;s=367735&amp;e=png&amp;b=fdfcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 国产独立</strong> <strong>AI 网站流量情况-月度 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2FFinanceYF5%2Fstatus%2F1710230053358080428\" title=\"https://twitter.com/FinanceYF5/status/1710230053358080428\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5c26a62bc99b40878a9abcd6c2621b31~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=833&amp;h=709&amp;s=166529&amp;e=png&amp;b=fffefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ 提升 DALL·E 3 提示技巧的 7 个技巧 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2FFinanceYF5%2Fstatus%2F1710147717962559885\" title=\"https://twitter.com/FinanceYF5/status/1710147717962559885\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<ol>\\n<li>详细提示：使用具体的描述，让人工智能更清晰地理解您的请求。</li>\\n<li>挑战想象：不要局限于现实，尝试抽象、超现实的概念。</li>\\n<li>调整和重复：如果结果不理想，调整提示并尝试多次。</li>\\n<li>创意场景：创建富有想象力的场景来引导生成图像。</li>\\n<li>视角和风格：指定特定的视角或艺术风格以影响输出。</li>\\n<li>平衡创造力和功能：理解 DALL·E 3 的能力和限制。</li>\\n<li>大胆尝试：不局限于传统提示，勇敢尝试各种领域和非传统想法。</li>\\n</ol>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ab80567b18ea40a2952d74ec65647982~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=680&amp;h=680&amp;s=788034&amp;e=png&amp;b=1b2a37\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ DSPy：将声明性语言模型调用编译成自我改进的流水线 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2F_akhaliq%2Fstatus%2F1710105224843501937\" title=\"https://twitter.com/_akhaliq/status/1710105224843501937\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>DSPy 是一个编程模型，用于开发和优化语言模型（LM）流水线，将其抽象为文本转换图，并允许 LM 通过声明性模块调用。DSPy 模块是可参数化的，可以学习如何应用不同的技术。该研究展示了 DSPy 的优势，通过几行代码，GPT-3.5 和 llama2-13b-chat 能够创建自助流水线，胜过标准的提示方法，甚至在竞争性 LM 上也表现出色。</p>\\n<ul>\\n<li>paper: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Ft.co%2FaZqkDZgU4Q\" title=\"https://t.co/aZqkDZgU4Q\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">huggingface.co/papers/2310…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/02db4211cbab475fb1c3379f30357142~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=669&amp;h=680&amp;s=458841&amp;e=png&amp;b=fcfcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ MathCoder：</strong> <strong>LLMs</strong> <strong>中无缝集成代码以增强数学推理 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2F_akhaliq%2Fstatus%2F1710109911944679575\" title=\"https://twitter.com/_akhaliq/status/1710109911944679575\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>最近发布的 GPT-4 Code Interpreter 在解决复杂数学问题方面表现出色，主要因为它可以无缝处理自然语言、生成代码、执行代码并继续推理。论文介绍了一种 fine-tune 开源语言模型的方法，使其能够使用代码进行数学建模和推导，从而提升数学推理能力。还提出了 MathCodeInstruct 数据集生成方法，其中每个解决方案都结合了自然语言、代码和执行结果。通过引入自定义的监督 fine-tuning 和推理方法，他们创建了 MathCoder 模型，这一系列模型在 MATH 和 GSM8K 数据集上取得了开源 LLM 中的最高分，明显优于其他开源替代品。</p>\\n<ul>\\n<li>paper: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Ft.co%2FCVxOodVMMf\" title=\"https://t.co/CVxOodVMMf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">huggingface.co/papers/2310…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/26b52bb0027f45099388a0c75390d1ae~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1200&amp;h=1128&amp;s=722837&amp;e=png&amp;b=fcfcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>◇ GPT-4-Vision 的新开源竞争对手 LLaVA v1.5 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2Fmattshumer_%2Fstatus%2F1710375301937615117\" title=\"https://twitter.com/mattshumer_/status/1710375301937615117\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>LLaVA 是一种结合视觉编码器和大型语言模型 Vicuna 的多模态模型，进行了端到端训练。它与多模态 GPT-4 相媲美的多模态聊天能力和在科学问题回答任务中达到的 92.53%的最新准确率。LLaVA 是由威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学进行的合作研究项目。基于 COCO 数据集，研究团队与 GPT-4 进行交互，总共收集了 158K 个多模态语言图像指令跟踪样本。LLaVA 使用简单的投影矩阵连接预训练的 CLIPViT-L/14 视觉编码器和大型语言模型 Vicuna，并通过两阶段的指令调整过程进行特征对齐的预训练和端到端微调。</p>\\n<ul>\\n<li>地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fllava-vl.github.io%2F\" title=\"https://llava-vl.github.io/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">llava-vl.github.io/</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/874cc6df251749178a9cda2622e3f3b0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2602&amp;h=1266&amp;s=1181587&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>「 机器人领域 」</h2>\\n<p><strong>◇ 与机器人 Ameca 对话 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2FCurieuxExplorer%2Fstatus%2F1710320226343297258\" title=\"https://twitter.com/CurieuxExplorer/status/1710320226343297258\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6a3e7225f10044a19f89420448fc4083~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=602&amp;h=1076&amp;s=595219&amp;e=png&amp;b=8b8f94\" alt=\"截屏2023-10-07 13.57.58.png\" loading=\"lazy\"/></p>\\n<p>  </p>\\n<p><strong>◇ 现在 robots 可以打开门 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2Fsegundoatdell%2Fstatus%2F1710309298989838740\" title=\"https://twitter.com/segundoatdell/status/1710309298989838740\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8df217d5e64c456fa63071bb23e56e85~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1910&amp;h=1077&amp;s=1054758&amp;e=png&amp;b=8a8b90\" alt=\"截屏2023-10-07 13.58.22.png\" loading=\"lazy\"/></p>\\n<h2><strong>「 融资快讯 」</strong></h2>\\n<p><strong>◇</strong> <strong>AI</strong> <strong>停车平台「Metropolis 筹集」1.7B 美元收购 SP Plus 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftechcrunch.com%2F2023%2F10%2F05%2Fai-powered-parking-platform-metropolis-raises-1-7b-to-acquire-sp-plus%2F\" title=\"https://techcrunch.com/2023/10/05/ai-powered-parking-platform-metropolis-raises-1-7b-to-acquire-sp-plus/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">News</a></strong></p>\\n<blockquote>\\n<p>Metropolis 筹集了 17 亿美元，Eldridge Capital 和 3L Capital 领投，多家投资机构也参与了融资，以股权和债务方式收购停车设施管理服务提供商 SP Plus。Metropolis 提供人工智能驱动的计算机视觉系统，用于简化停车设施的用户体验。SP Plus 拥有美国和加拿大的 200 万个停车位和 160 个机场的停车和班车运营。这次收购价格每股 54 美元，溢价 52%。交易预计将于 2024 年完成，前提是获得监管批准和 SP Plus 股东批准。这标志着 Metropolis 向垂直整合迈出的一步，加强了其在停车设施领域的地位。</p>\\n</blockquote>\\n<h2>「 早点趣玩 」</h2>\\n<p><strong>◇ Manga-Image-Translator：漫画图片翻译器 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2Fxiaohuggg%2Fstatus%2F1710264008140619781\" title=\"https://twitter.com/xiaohuggg/status/1710264008140619781\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Twitter</a></strong></p>\\n<blockquote>\\n<p>能够自动翻译漫画图像中的文本，普通的图片也可以，翻译完成后在图像在原位置替换译文。干净简洁，速度很快，主要支持日语，汉语、英文和韩语。</p>\\n<ul>\\n<li>地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fcotrans.touhou.ai%2F\" title=\"https://cotrans.touhou.ai/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">cotrans.touhou.ai/</a></li>\\n<li>Github: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Ft.co%2FaIJLdyQKMJ\" title=\"https://t.co/aIJLdyQKMJ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/zyddnys/man…</a></li>\\n</ul>\\n</blockquote>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/521f1080a820486eabf880cf29a6f754~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1117&amp;h=1073&amp;s=1101597&amp;e=png&amp;b=ffffff\" alt=\"截屏2023-10-07 13.59.49.png\" loading=\"lazy\"/></p>\\n<h2>「 技术阅读 」</h2>\\n<p><strong>◇ 优秀的机器学习系统和实证科学论文（2023 年 9 月） 🔗</strong> <strong><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdblalock.substack.com%2Fp%2F2023-9-arxiv-roundup-a-bunch-of-good\" title=\"https://dblalock.substack.com/p/2023-9-arxiv-roundup-a-bunch-of-good\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Link</a></strong></p>', 'author': '前端早早聊', 'reading_time': 13, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 60, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'e659350970746edd834c755ee28d68d14f4a733657c7cce32c216caeddabdba4', 'title': '告别人山人海，AI绘画让你的照片更美！', 'url': 'https://juejin.cn/post/7286669548740296761', 'published_at': '2023-10-07T04:34:40Z', 'created_at': '2023-10-07T07:03:23.147197Z', 'changed_at': '2023-10-07T07:03:23.147197Z', 'content': '<p>快乐总是短暂的，十一假期结束了，想必大家都拍了不少好看的照片，但是景点也经常是人山人海，照片中不免会出现其它不相干的人或物，以前想抹除这些人或物得找专业的美工或者图像处理专家P掉才行，现在AI绘画来了，只需要简单的几步就可以完美P图，普通人也可以自己搞起来。</p>\\n<p>首先你需要一个 Stable Duiffusion 的运行环境，可以看我这两篇文章：</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTUxNjU2OQ%3D%3D%26mid%3D2247483860%26idx%3D1%26sn%3Dfb71bbcbb2c0b8738ba9565ae2b5b0a1%26chksm%3Dc15cbc22f62b35342f2259114c194997282817aaf7de3d8fa318faf6a6f23f1fc31756c3106e%26scene%3D21%23wechat_redirect\" title=\"https://mp.weixin.qq.com/s?__biz=MzkxNTUxNjU2OQ==&amp;mid=2247483860&amp;idx=1&amp;sn=fb71bbcbb2c0b8738ba9565ae2b5b0a1&amp;chksm=c15cbc22f62b35342f2259114c194997282817aaf7de3d8fa318faf6a6f23f1fc31756c3106e&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">手把手教你在本机安装Stable Diffusion秋叶整合包</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTUxNjU2OQ%3D%3D%26mid%3D2247484398%26idx%3D1%26sn%3Ddef2821fa07a7af16967763ea0fbc537%26chksm%3Dc15cbe18f62b370ec47084e3815bf5178de49f84fce76ba8097bf53fe804352955f3930305de%26token%3D2130349979%26lang%3Dzh_CN%23rd\" title=\"https://mp.weixin.qq.com/s?__biz=MzkxNTUxNjU2OQ==&amp;mid=2247484398&amp;idx=1&amp;sn=def2821fa07a7af16967763ea0fbc537&amp;chksm=c15cbe18f62b370ec47084e3815bf5178de49f84fce76ba8097bf53fe804352955f3930305de&amp;token=2130349979&amp;lang=zh_CN#rd\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">手把手教你在云服务器部署Stable Diffusion WebUI</a></p>\\n<p>然后我们就开始愉快的P图之旅了。</p>\\n<h2>图生图-局部绘制</h2>\\n<p>首先采用最简单的“图生图”-“局部绘制”功能，也就是通过局部的重新绘制来将照片中的人或者物替换掉，具体操作方式请参考下面的示例图。</p>\\n<p>因为我们要处理的图片都是真实的照片，所以大模型需要尽量选择一些真实系的模型，比如我这里用的 realisticVisionV51。</p>\\n<p>提示词和反向提示词用来控制重绘部分的内容。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b83506160ae644e59b2c7f45ee2f746e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1668&amp;h=754&amp;s=162208&amp;e=png&amp;b=161c28\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在“局部绘制”的图片区域上传需要P图的照片，我这里从小红书下载了一张图片，把需要重绘的部分用画笔涂抹上。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7bc469f3d60f4846b461c71f8dfa1f90~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1266&amp;h=1496&amp;s=2394920&amp;e=png&amp;b=0d121b\" alt=\"\" loading=\"lazy\"/></p>\\n<p>蒙版模式设置为“绘制蒙版内容”，但是绘制区域选择“全图”，这样图片衔接的会比较好。</p>\\n<p>采样器基本上可以任意选择，注意搭配的采样步数。我这里选择最新的 DPM++ 3M SDE Karras，采样步数30。</p>\\n<p>画布尺寸保持和原始图片一致。</p>\\n<p>重绘强度可以设置的大一点，建议0.7以上，让AI发挥的更自由一些。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/339085c2ea9b4c6b91b098e0e27c6184~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1394&amp;h=1568&amp;s=369639&amp;e=png&amp;b=0c0f18\" alt=\"\" loading=\"lazy\"/></p>\\n<p>看下AI绘画P图的效果：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/43141a62ef5a4bbabf4a0c51500c608e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1648&amp;h=1078&amp;s=3238865&amp;e=png&amp;b=cebdb5\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>ControlNet-局部绘制</h2>\\n<p>除了图生图的局部绘制能力，我们还可以使用ControlNet的局部绘制模型。</p>\\n<p>关于ControlNet的基本使用，可以看这篇文章：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTUxNjU2OQ%3D%3D%26mid%3D2247484198%26idx%3D1%26sn%3D7957b78f46239a7fdd40c3431d032495%26chksm%3Dc15cbed0f62b37c66d758124a06a7b78d861c72c276469fca495c4b5f7fec48a92fa39bb9c25%26scene%3D21%23wechat_redirect\" title=\"https://mp.weixin.qq.com/s?__biz=MzkxNTUxNjU2OQ==&amp;mid=2247484198&amp;idx=1&amp;sn=7957b78f46239a7fdd40c3431d032495&amp;chksm=c15cbed0f62b37c66d758124a06a7b78d861c72c276469fca495c4b5f7fec48a92fa39bb9c25&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ControlNet的安装与基本使用方法</a>。</p>\\n<p>这个模型可以用在“图生图”，也可以用在“文生图”。ControlNet的设置请看下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bec2924a1d544f52924cabc5bc530a7e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1208&amp;h=1294&amp;s=320053&amp;e=png&amp;b=111622\" alt=\"\" loading=\"lazy\"/></p>\\n<p>注意这里的预处理器有三个：</p>\\n<ul>\\n<li>inpaint_only 只绘制蒙版遮罩的部分。</li>\\n<li>inpaint_only+lama 相比inpaint_only随机对象更少，画面更稳定。</li>\\n<li>inpaint_global_harmonious 全局协调，让局部重绘的画面和整体更协调，注意在文生图中使用时会更改模版之外的内容。</li>\\n</ul>\\n<p>以文生图为例：希望去掉下面图片中圈出来的人体部分。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/32887f9d4ebf471cb77c71e37d71d710~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2078&amp;h=1558&amp;s=4046512&amp;e=png&amp;b=c5dcf3\" alt=\"\" loading=\"lazy\"/></p>\\n<p>Stable Diffusion的参数设置如下：</p>\\n<p>模型还是选择 realisticVisionV51，通过提示词控制绘制内容，画布的宽高设置为原始图片的宽高，可以将提示词引导系数调的高一些，以便加强绘制相关内容。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e6e48fbd17254f8d8e11ab3e9b5444b6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1440&amp;h=1546&amp;s=300083&amp;e=png&amp;b=121823\" alt=\"\" loading=\"lazy\"/></p>\\n<p>文生图时需要在 ControlNet 中上传这张图片，并使用画笔绘制模版：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/294ec829667041f890693cfb5c5c0b2a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1346&amp;h=750&amp;s=599651&amp;e=png&amp;b=0c1019\" alt=\"\" loading=\"lazy\"/></p>\\n<p>看看最后的生图效果：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/22200991de084b7b87b7b85ce72ef66d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1280&amp;h=960&amp;s=201462&amp;e=jpg&amp;b=c3def6\" alt=\"\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f93856085c08443da59c35c698a2f067~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1280&amp;h=960&amp;s=1479622&amp;e=png&amp;b=c3dcf6\" alt=\"\" loading=\"lazy\"/></p>\\n<p>如果生成的图片只是某些部分P的不好，可以把这张图片再次进行局部重绘，并且每次仅绘制有问题的部分。</p>\\n<h2>资源下载</h2>\\n<p>本系列使用的模型、插件，生成的图片，都已经上传到我整理的 Stable Diffusion 绘画资源中，后续也会持续更新，如有需要，请关/注/公/众/号：萤火遛AI（yinghuo6ai），发消息：SD，即可获取下载地址。</p>\\n\\n<p>以上就是本文的主要内容了，如有问题，欢迎留言沟通交流。</p>', 'author': '萤火架构', 'reading_time': 3, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 59, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '9148811cd142d32e49716a9b19bb2335e2f792855b63a9057fcc91d6ee5d768c', 'title': 'AIGC革新，将文字或者LOGO融入AI视频基于PIKA-labs(Python3.10)', 'url': 'https://juejin.cn/post/7286679458131673122', 'published_at': '2023-10-07T04:12:37Z', 'created_at': '2023-10-07T07:03:23.143922Z', 'changed_at': '2023-10-07T07:03:23.143922Z', 'content': '<p>很多平台都会禁止用户使用带有网址或者二维码的头像以及文章配图，这样可以有效的防止用户的一些“导流”行为。当然，头像、文章或者视频现在都是AI来审，毕竟现在人工的成本实在太高，但是如果我们把文字元素直接融入图像或者视频之中，如此一来，AI也会很难识别出一些“导流”的元素。</p>\\n<p>本次我们依靠PIKA-labs平台，无需本地环境，直接简单粗暴输出带有文字元素的光影视频效果，基于Python3.10。</p>\\n<h2>文字/图片生成视频</h2>\\n<p>首先通过Discord平台订阅PIKA频道，频道地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdiscord.com%2Fchannels%2F1123665496148017235\" title=\"https://discord.com/channels/1123665496148017235\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">discord.com/channels/11…</a></p>\\n<p>目前有两种方式来生成AI视频，分别是文字生成视频和图片生成视频：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/90ecfe6bc3b946fabfd79cad419aded0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1336&amp;h=326&amp;s=43286&amp;e=png&amp;b=2a2c30\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在对话框内输入 /encrypt_text 我们就可以通过描述词来生成对应的AI视频。</p>\\n<p>主要参数为想要嵌入的文本（Message）以及提示词(Prompt):</p>\\n<p>比如我们输入：</p>\\n<pre><code>/encrypt_text Message:v3u.cn Prompt:colorful paints\\n</code></pre>\\n<p>意思是生成一段视频，包含v3u.cn的文本，并且主题是彩色绘画。</p>\\n<p>生成的视频如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1e448284fca045baaa4af30afbe9d8a9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1754&amp;h=851&amp;s=404164&amp;e=png&amp;b=e1cb0a\" alt=\"\" loading=\"lazy\"/></p>\\n<p>如果对效果不满意，也可以通过其他参数来进行微调，比如 -w 为权重，阈值范围是0-2，可以动态调整文本内容的权重，-font 参数可以对文本的字体进行调整。</p>\\n<p>比如我们生成一个油画风格的，权重略低，防止被审核识别出来的LOGO：</p>\\n<pre><code>/encrypt_text Message:v3u.cn Prompt:Oil painting -w 1\\n</code></pre>\\n<p>效果如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5de767bf05ae42be8aa1475b22783915~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=797&amp;h=640&amp;s=757343&amp;e=png&amp;b=171317\" alt=\"\" loading=\"lazy\"/></p>\\n<p>除此之外，也可以通过/encrypt_image命令直接上传Logo图像来进行生成操作。</p>\\n<h2>基于Python3.10</h2>\\n<p>如果不想每次都登录Discord平台，我们也可也通过Pyhon命令行来进行生成，<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fv3u.cn%2Fa_id_200\" title=\"https://v3u.cn/a_id_200\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">首先确保本地安装好Python3.10版本的开发环境</a>，随后安装Discord库：</p>\\n<pre><code>pip3 install -U discord.py\\n</code></pre>\\n<p>随后编写Discord登录代码：</p>\\n<pre><code>import discord  \\n  \\nclass MyClient(discord.Client):  \\n    async def on_ready(self):  \\n        print(&#39;Logged on as&#39;, self.user)  \\n  \\n    async def on_message(self, message):  \\n        \\n        if message.author == self.user:  \\n            return  \\n  \\n        if message.content == &#39;ping&#39;:  \\n            await message.channel.send(&#39;pong&#39;)  \\n  \\nintents = discord.Intents.default()  \\nintents.message_content = True  \\nclient = MyClient(intents=intents)  \\n  \\nclient.run(&#39;您的discord token&#39;)\\n</code></pre>\\n<p>这里我们构建了一个客户端类，一旦链接成功就会触发异步方法on_ready输出当前登录的用户，注意这就是订阅了pika频道的用户，而不是机器人，程序输出：</p>\\n<pre><code>2023-10-07 12:36:44 INFO     discord.client logging in using static token\\nLogged on as , v3ucn\\n</code></pre>\\n<p>Discord平台的用户token可以通过浏览器的开发者模式进行抓包获取，这里不再赘述。</p>\\n<p>如果登录成功，我们就可以通过客户端给当前订阅的频道发消息了，改写异步方法on_ready:</p>\\n<pre><code>@client.event  \\nasync def on_ready():  #  Called when internal cache is loaded  \\n     channel = client.get_channel(1134375457236725770) #  Gets channel from internal cache  \\n     await channel.send(&#34;/encrypt_text Message:v3u.cn Prompt:Oil painting -w 1&#34;) #  Sends message to channel\\n</code></pre>\\n<p>这里通过get_channel方法来获取订阅的频道，1134375457236725770是pika-labs的频道id，随后通过send方法直接发送即可。</p>\\n<h2>结语</h2>\\n<p>文字融入AI视频可以为创作者提供更多的艺术表达形式。通过巧妙运用文字的排版、动画效果和视觉设计，可以创造出独特的艺术风格和视觉冲击力，使得作品更具创意和吸引力，当然了，相应的，也为平台的审核系统创造了新的挑战。</p>', 'author': '刘悦的技术博客', 'reading_time': 5, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 75, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '367cb34a2fb780d9966001ecebe703e22e390728867c13879d12e379a5379139', 'title': '大模型推理框架概述', 'url': 'https://juejin.cn/post/7286676030965317668', 'published_at': '2023-10-07T04:06:01Z', 'created_at': '2023-10-07T17:26:50.880539Z', 'changed_at': '2023-10-07T17:26:50.880539Z', 'content': '<p>从 ChatGPT 面世以来，引领了大模型时代的变革，除了大模型遍地开花以外，承载大模型进行推理的框架也是层出不穷，大有百家争鸣的态势。本文主要针对业界知名度较高的一些大模型推理框架进行相应的概述。</p>\\n<h2>vLLM</h2>\\n<ul>\\n<li>GitHub: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm\" title=\"https://github.com/vllm-project/vllm\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/vllm-projec…</a></li>\\n</ul>\\n<h3>简介</h3>\\n<p>vLLM是一个开源的大模型推理加速框架，通过PagedAttention高效地管理attention中缓存的张量，实现了比HuggingFace Transformers高14-24倍的吞吐量。</p>\\n<p>PagedAttention 是 vLLM 的核心技术，它解决了LLM服务中内存的瓶颈问题。传统的注意力算法在自回归解码过程中，需要将所有输入Token的注意力键和值张量存储在GPU内存中，以生成下一个Token。这些缓存的键和值张量通常被称为KV缓存。</p>\\n<h3>主要特性</h3>\\n<ul>\\n<li>通过PagedAttention对 KV Cache 的有效管理</li>\\n<li>传入请求的continus batching，而不是static batching</li>\\n<li>支持张量并行推理</li>\\n<li>支持流式输出</li>\\n<li>兼容 OpenAI 的接口服务</li>\\n<li>与 HuggingFace 模型无缝集成</li>\\n</ul>\\n<h3>与其他框架（HF、TGI）的性能对比</h3>\\n<p>vLLM 的吞吐量比 HF 高 14 - 24 倍，比 TGI 高 2.2 - 2.5 倍。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bba59bd5541b4ffcaeb5e786ac823212~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1530&amp;h=616&amp;s=126561&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>存在的问题</h3>\\n<ul>\\n<li>同样的模型、参数和prompt条件下，vLLM推理和Huggingface推理结果不一致。具体请参考：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F658780653\" title=\"https://zhuanlan.zhihu.com/p/658780653\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">zhuanlan.zhihu.com/p/658780653</a></li>\\n</ul>\\n<h3>业界案例</h3>\\n<p>vLLM 已经被用于 Chatbot Arena 和 Vicuna 大模型的服务后端。</p>\\n<h2>HuggingFace TGI</h2>\\n<ul>\\n<li>GitHub: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftext-generation-inference\" title=\"https://github.com/huggingface/text-generation-inference\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/huggingface…</a></li>\\n</ul>\\n<h3>简介</h3>\\n<p>Text Generation Inference（TGI）是 HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理。</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2a9691f2e10a4f00a6f6b685096753a6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1177&amp;h=720&amp;s=2547229&amp;e=png&amp;b=fcfafa\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>主要特性</h3>\\n<ul>\\n<li>支持张量并行推理</li>\\n<li>支持传入请求 Continuous batching 以提高总吞吐量</li>\\n<li>使用 flash-attention 和 Paged Attention 在主流的模型架构上优化用于推理的 transformers 代码。<strong>注意：并非所有模型都内置了对这些优化的支持</strong>。</li>\\n<li>使用bitsandbytes(<code>LLM.int8()</code>)和GPT-Q进行量化</li>\\n<li>内置服务评估，可以监控服务器负载并深入了解其性能</li>\\n<li>轻松运行自己的模型或使用任何 HuggingFace 仓库的模型</li>\\n<li>自定义提示生成：通过提供自定义提示来指导模型的输出，轻松生成文本</li>\\n<li>使用 Open Telemetry，Prometheus 指标进行分布式跟踪</li>\\n</ul>\\n<h3>支持的模型</h3>\\n<ul>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fbigscience%2Fbloom\" title=\"https://huggingface.co/bigscience/bloom\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">BLOOM</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fgoogle%2Fflan-t5-xxl\" title=\"https://huggingface.co/google/flan-t5-xxl\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">FLAN-T5</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ffacebook%2Fgalactica-120b\" title=\"https://huggingface.co/facebook/galactica-120b\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Galactica</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FEleutherAI%2Fgpt-neox-20b\" title=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">GPT-Neox</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fllama\" title=\"https://github.com/facebookresearch/llama\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Llama</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ffacebook%2Fopt-66b\" title=\"https://huggingface.co/facebook/opt-66b\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">OPT</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fbigcode%2Fsantacoder\" title=\"https://huggingface.co/bigcode/santacoder\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">SantaCoder</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fbigcode%2Fstarcoder\" title=\"https://huggingface.co/bigcode/starcoder\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Starcoder</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ftiiuae%2Ffalcon-7b\" title=\"https://huggingface.co/tiiuae/falcon-7b\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Falcon 7B</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ftiiuae%2Ffalcon-40b\" title=\"https://huggingface.co/tiiuae/falcon-40b\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Falcon 40B</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmosaicml%2Fmpt-30b\" title=\"https://huggingface.co/mosaicml/mpt-30b\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">MPT</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmeta-llama\" title=\"https://huggingface.co/meta-llama\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Llama V2</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fcodellama\" title=\"https://huggingface.co/codellama\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Code Llama</a></li>\\n</ul>\\n<h3>适用场景</h3>\\n<p>依赖 HuggingFace 模型，并且不需要为核心模型增加多个adapter的场景。</p>\\n<h2>FasterTransformer</h2>\\n<ul>\\n<li>GitHub: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2FFasterTransformer\" title=\"https://github.com/NVIDIA/FasterTransformer\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/NVIDIA/Fast…</a></li>\\n</ul>\\n<h3>简介</h3>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2FFasterTransformer%2F\" title=\"https://github.com/NVIDIA/FasterTransformer/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">NVIDIA FasterTransformer (FT)</a>\\xa0是一个用于实现基于Transformer的神经网络推理的加速引擎。它包含Transformer块的高度优化版本的实现，其中包含编码器和解码器部分。使用此模块，您可以运行编码器-解码器架构模型（如：T5）、仅编码器架构模型（如：BERT）和仅解码器架构模型（如： GPT）的推理。</p>\\n<p>FT框架是用C++/CUDA编写的，依赖于高度优化的 cuBLAS、cuBLASLt 和 cuSPARSELt 库，这使您可以在 GPU 上进行快速的 Transformer 推理。</p>\\n<p>与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理。</p>\\n<p>下图显示了如何使用张量并行 (TP) 和流水线并行 (PP) 技术将基于Transformer架构的神经网络拆分到多个 GPU 和节点上。</p>\\n<ul>\\n<li>当每个张量被分成多个块时，就会发生张量并行，并且张量的每个块都可以放置在单独的 GPU 上。在计算过程中，每个块在不同的 GPU 上单独并行处理；最后，可以通过组合来自多个 GPU 的结果来计算最终张量。</li>\\n<li>当模型被深度拆分，并将不同的完整层放置到不同的 GPU/节点上时，就会发生流水线并行。</li>\\n</ul>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/748aaae6515441f79a0a944518dce9d6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等。因此，使用FasterTransformer，您可以在多个 GPU 上以张量并行运行大型Transformer，以减少计算延迟。同时，TP 和 PP 可以结合在一起，在多 GPU 节点环境中运行具有数十亿、数万亿个参数的大型 Transformer 模型。</p>\\n<p>除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点。</p>\\n<h3>FasterTransformer 中的优化技术</h3>\\n<p>与深度学习训练的通用框架相比，FT 使您能够获得更快的推理流水线以及基于 Transformer 的神经网络具有更低的延迟和更高的吞吐量。 FT 对 GPT-3 和其他大型 Transformer 模型进行的一些优化技术包括：</p>\\n<ol>\\n<li>层融合（Layer fusion）</li>\\n</ol>\\n<p>这是预处理阶段的一组技术，将多层神经网络组合成一个单一的神经网络，将使用一个单一的核（kernel）进行计算。 这种技术减少了数据传输并增加了数学密度，从而加速了推理阶段的计算。 例如， multi-head attention 块中的所有操作都可以合并到一个核（kernel）中。</p>\\n<ol>\\n<li>自回归模型的推理优化(激活缓存)</li>\\n</ol>\\n<p>为了防止通过Transformer重新计算每个新 token 生成器的先前的key和value，FT 分配了一个缓冲区来在每一步存储它们。</p>\\n<p>虽然需要一些额外的内存使用，但 FT 可以节省重新计算的成本。该过程如下图所示，相同的缓存机制用于 NN 的多个部分。</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e1e46625612b413996b500b78a991933~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>内存优化</li>\\n</ol>\\n<p>与 BERT 等传统模型不同，大型 Transformer 模型具有多达数万亿个参数，占用数百 GB 存储空间。即使我们以半精度存储模型，GPT-3 175b 也需要 350 GB。因此有必要减少其他部分的内存使用。</p>\\n<p>例如，在 FasterTransformer 中，我们在不同的解码器层重用了激活/输出的内存缓冲（buffer）。由于 GPT-3 中的层数为 96，因此我们只需要 1/96 的内存量用于激活。</p>\\n<ol>\\n<li>使用 MPI 和 NCCL 实现节点间/节点内通信并支持模型并行</li>\\n</ol>\\n<p>FasterTransormer 同时提供张量并行和流水线并行。 对于张量并行，FasterTransformer 遵循了\\xa0<a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1909.08053.pdf\" title=\"https://arxiv.org/pdf/1909.08053.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Megatron</a>\\xa0的思想。 对于自注意力块和前馈网络块，FT 按行拆分第一个矩阵的权重，并按列拆分第二个矩阵的权重。 通过优化，FT 可以将每个 Transformer 块的归约（reduction）操作减少到两次。</p>\\n<p>对于流水线并行，FasterTransformer 将整批请求拆分为多个微批，隐藏了通信的空泡（bubble）。 FasterTransformer 会针对不同情况自动调整微批量大小。</p>\\n<ol>\\n<li>MatMul 核自动调整（GEMM 自动调整）</li>\\n</ol>\\n<p>矩阵乘法是基于 Transformer 的神经网络中最主要和繁重的操作。 FT 使用来自 CuBLAS 和 CuTLASS 库的功能来执行这些类型的操作。 重要的是要知道 MatMul 操作可以在“硬件”级别使用不同的底层（low-level）算法以数十种不同的方式执行。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdocs.nvidia.com%2Fcuda%2Fcublas%2Findex.html%23cublas-GemmBatchedEx\" title=\"https://docs.nvidia.com/cuda/cublas/index.html#cublas-GemmBatchedEx\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><code>GemmBatchedEx</code></a>\\xa0函数实现了 MatMul 操作，并以<code>cublasGemmAlgo_t</code>作为输入参数。 使用此参数，您可以选择不同的底层算法进行操作。</p>\\n<p>FasterTransformer 库使用此参数对所有底层算法进行实时基准测试，并为模型的参数和您的输入数据（注意层的大小、注意头的数量、隐藏层的大小）选择最佳的一个。 此外，FT 对网络的某些部分使用硬件加速的底层函数，例如： <code>__expf</code>、<code>__shfl_xor_sync</code>。</p>\\n<ol>\\n<li>低精度推理</li>\\n</ol>\\n<p>FT 的核（kernels）支持使用 fp16 和 int8 等低精度输入数据进行推理。 由于较少的数据传输量和所需的内存，这两种机制都会加速。 同时，int8 和 fp16 计算可以在特殊硬件上执行，例如：Tensor Core（适用于从 Volta 开始的所有 GPU 架构）。</p>\\n<p>除此之外还有<strong>快速的 C++ BeamSearch 实现</strong>、当模型的权重部分分配到八个 GPU 之间时，<strong>针对 TensorParallelism 8 模式优化的 all-reduce</strong>。</p>\\n<h3>支持的模型</h3>\\n<p>目前，FT 支持了 Megatron-LM GPT-3、GPT-J、BERT、ViT、Swin Transformer、Longformer、T5 和 XLNet 等模型。您可以在 GitHub 上的\\xa0<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2FFasterTransformer%23support-matrix\" title=\"https://github.com/NVIDIA/FasterTransformer#support-matrix\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">FasterTransformer</a>库中查看最新的支持矩阵。</p>\\n<h3>与其他框架（PyTorch）的性能对比</h3>\\n<p>FT 适用于计算能力 &gt;= 7.0 的 GPU，例如: V100、A10、A100 等。</p>\\n<p>下图展示了 GPT-J 6B 参数的模型推断加速比较：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d22815ddfcbe4860b2887a4e47c59651~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>存在的问题</h3>\\n<ul>\\n<li>英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了。</li>\\n</ul>\\n<h2>DeepSpeed-MII</h2>\\n<ul>\\n<li>GitHub: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fmicrosoft%2FDeepSpeed-MII\" title=\"https://github.com/microsoft/DeepSpeed-MII\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/microsoft/D…</a></li>\\n</ul>\\n<h3>简介</h3>\\n<p>DeepSpeed-MII 是 DeepSpeed 的一个新的开源 Python 库，旨在使模型不仅低延迟和低成本推理，而且还易于访问。</p>\\n<ul>\\n<li>MII 提供了对数千种广泛使用的深度学习模型的高度优化实现。</li>\\n<li>与原始PyTorch实现相比，MII 支持的模型可显著降低延迟和成本。</li>\\n<li>为了实现低延迟/低成本推理，MII 利用 DeepSpeed-Inference 的一系列广泛优化，例如：transformers 的深度融合、用于多 GPU 推理的自动张量切片、使用 ZeroQuant 进行动态量化等。</li>\\n<li>MII 只需几行代码即可通过 AML 在本地和 Azure 上低成本部署这些模型。</li>\\n</ul>\\n<h3>MII 工作流程</h3>\\n<p>下图显示了 MII 如何使用 DS-Inference 自动优化 OSS 模型；然后，使用 GRPC 在本地部署，或使用 AML Inference 在 Microsoft Azure 上部署。</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/56b59d67c9284c92a867bf2720b50978~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3394&amp;h=1735&amp;s=286885&amp;e=png&amp;a=1&amp;b=4c83ca\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>MII 的底层由 DeepSpeed-Inference 提供支持。 根据模型类型、模型大小、批量大小和可用硬件资源，MII 自动应用 DeepSpeed-Inference 中的一组适当的系统优化，以最大限度地减少延迟并最大限度地提高吞吐量。它通过使用许多预先指定的模型注入策略之一来实现这一点，该策略允许 MII 和 DeepSpeed-Inference 识别底层 PyTorch 模型架构并用优化的实现替换它。在此过程中，MII 使 DeepSpeed-Inference 中一系列的优化自动可用于其支持的数千种流行模型。</p>\\n<h3>支持的模型和任务</h3>\\n<p>MII 目前支持超过 50,000 个模型，涵盖文本生成、问答、文本分类等一系列任务。 MII 加速的模型可通过 Hugging Face、FairSeq、EluetherAI 等多个开源模型存储库获取。我们支持基于 Bert、Roberta 或 GPT 架构的稠密模型，参数范围从几亿参数到数百亿参数。除此之外，MII将继续扩展该列表，支持即将推出的大规模千亿级以上参数稠密和稀疏模型。</p>\\n<p>目前 MII 支持以下 HuggingFace Transformers 模型系列：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>model family</th><th>size range</th><th>~model count</th></tr></thead><tbody><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dllama\" title=\"https://huggingface.co/models?other=llama\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">llama</a></td><td>7B - 65B</td><td>1,500</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dbloom\" title=\"https://huggingface.co/models?other=bloom\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">bloom</a></td><td>0.3B - 176B</td><td>480</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dstable-diffusion\" title=\"https://huggingface.co/models?other=stable-diffusion\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">stable-diffusion</a></td><td>1.1B</td><td>3,700</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dopt\" title=\"https://huggingface.co/models?other=opt\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">opt</a></td><td>0.1B - 66B</td><td>460</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dgpt_neox\" title=\"https://huggingface.co/models?other=gpt_neox\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">gpt_neox</a></td><td>1.3B - 20B</td><td>850</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dgptj\" title=\"https://huggingface.co/models?other=gptj\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">gptj</a></td><td>1.4B - 6B</td><td>420</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dgpt_neo\" title=\"https://huggingface.co/models?other=gpt_neo\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">gpt_neo</a></td><td>0.1B - 2.7B</td><td>700</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dgpt2\" title=\"https://huggingface.co/models?other=gpt2\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">gpt2</a></td><td>0.3B - 1.5B</td><td>11,900</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dxlm-roberta\" title=\"https://huggingface.co/models?other=xlm-roberta\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">xlm-roberta</a></td><td>0.1B - 0.3B</td><td>4,100</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Droberta\" title=\"https://huggingface.co/models?other=roberta\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">roberta</a></td><td>0.1B - 0.3B</td><td>8,700</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Ddistilbert\" title=\"https://huggingface.co/models?other=distilbert\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">distilbert</a></td><td>0.1B - 0.3B</td><td>4,700</td></tr><tr><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dbert\" title=\"https://huggingface.co/models?other=bert\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">bert</a></td><td>0.1B - 0.3B</td><td>23,600</td></tr></tbody></table>\\n<h3>与其他框架（PyTorch）的性能对比</h3>\\n<p>MII 将 Big-Science Bloom 176B 模型的延迟降低了 5.7 倍，同时将成本降低了 40 倍以上。同样,它将部署 Stable Diffusion 的延迟和成本降低了 1.9 倍。</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/165a34083e6f4a50b8080ab2474a48b9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3141&amp;h=2488&amp;s=362214&amp;e=png&amp;a=1&amp;b=595959\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h2>FlexFlow Server</h2>\\n<ul>\\n<li>GitHub: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fflexflow%2FFlexFlow%2Ftree%2Finference\" title=\"https://github.com/flexflow/FlexFlow/tree/inference\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/flexflow/Fl…</a></li>\\n</ul>\\n<h3>简介</h3>\\n<p>FlexFlow Serve 是一个开源编译器和分布式系统，用于低延迟、高性能 LLM 服务。</p>\\n<h3>主要特征</h3>\\n<h4>投机（Speculative） 推理</h4>\\n<p>使 FlexFlow Serve 能够加速 LLM 服务的一项关键技术是Speculative推理，它结合了各种集体boost-tuned的小型投机模型 (SSM) 来共同预测 LLM 的输出；</p>\\n<p>预测被组织为token树，每个节点代表一个候选 token 序列。 使用一种新颖的基于树的并行解码机制，根据 LLM 的输出并行验证由 token 树表示的所有候选 token 序列的正确性。</p>\\n<p>FlexFlow Serve 使用 LLM 作为 token 树验证器而不是增量解码器，这大大减少了服务生成 LLM 的端到端推理延迟和计算要求，同时，可证明保持模型质量。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8470b30bcf844e769420d8eb5b7f5c8b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=990&amp;h=511&amp;s=82119&amp;e=png&amp;b=fefbfb\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h4>CPU Offloading</h4>\\n<p>FlexFlow Serve 还提供基于Offloading的推理，用于在单个 GPU 上运行大型模型（例如：llama-7B）。</p>\\n<p>CPU Offloading是将张量保存在CPU内存中，并且在计算时仅将张量复制到GPU。</p>\\n<blockquote>\\n<p>注意：</p>\\n<p>现在我们有选择地offload最大的权重张量（线性、注意力中的权重张量）。 此外，由于小模型占用的空间要少得多，如果不构成GPU内存瓶颈，offload会带来更多的运行空间和计算成本，因此，我们只对大模型进行offload。 可以通过启用 -offload 和 -offload-reserve-space-size 标志来运行offloading。</p>\\n</blockquote>\\n<h4>支持量化</h4>\\n<p>FlexFlow Serve 支持 int4 和 int8 量化。 压缩后的张量存储在CPU端， 一旦复制到 GPU，这些张量就会进行解压缩并转换回其原始精度。</p>\\n<h3>支持的 LLMs 和 SSMs</h3>\\n<p>FlexFlow Serve 当前支持以下模型架构的所有Hugingface模型：</p>\\n<ul>\\n<li><code>LlamaForCausalLM</code> / <code>LLaMAForCausalLM</code> (例如：LLaMA/LLaMA-2, Guanaco, Vicuna, Alpaca, ...)</li>\\n<li><code>OPTForCausalLM</code> (OPT家族模型)</li>\\n<li><code>RWForCausalLM</code> (Falcon家族模型)</li>\\n<li><code>GPTBigCodeForCausalLM</code> (Starcoder家族模型)</li>\\n</ul>\\n<p>以下是我们已经测试过并且可以使用 SSM 的模型列表：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>模型</th><th>在 HuggingFace 中的模型 id</th><th>Boost-tuned SSMs</th></tr></thead><tbody><tr><td>LLaMA-7B</td><td>decapoda-research/llama-7b-hf</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-68m\" title=\"https://huggingface.co/JackFram/llama-68m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-68M</a> , <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-160m\" title=\"https://huggingface.co/JackFram/llama-160m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-160M</a></td></tr><tr><td>LLaMA-13B</td><td>decapoda-research/llama-13b-hf</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-68m\" title=\"https://huggingface.co/JackFram/llama-68m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-68M</a> , <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-160m\" title=\"https://huggingface.co/JackFram/llama-160m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-160M</a></td></tr><tr><td>LLaMA-30B</td><td>decapoda-research/llama-30b-hf</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-68m\" title=\"https://huggingface.co/JackFram/llama-68m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-68M</a> , <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-160m\" title=\"https://huggingface.co/JackFram/llama-160m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-160M</a></td></tr><tr><td>LLaMA-65B</td><td>decapoda-research/llama-65b-hf</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-68m\" title=\"https://huggingface.co/JackFram/llama-68m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-68M</a> , <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-160m\" title=\"https://huggingface.co/JackFram/llama-160m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-160M</a></td></tr><tr><td>LLaMA-2-7B</td><td>meta-llama/Llama-2-7b-hf</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-68m\" title=\"https://huggingface.co/JackFram/llama-68m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-68M</a> , <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-160m\" title=\"https://huggingface.co/JackFram/llama-160m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-160M</a></td></tr><tr><td>LLaMA-2-13B</td><td>meta-llama/Llama-2-13b-hf</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-68m\" title=\"https://huggingface.co/JackFram/llama-68m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-68M</a> , <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-160m\" title=\"https://huggingface.co/JackFram/llama-160m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-160M</a></td></tr><tr><td>LLaMA-2-70B</td><td>meta-llama/Llama-2-70b-hf</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-68m\" title=\"https://huggingface.co/JackFram/llama-68m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-68M</a> , <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FJackFram%2Fllama-160m\" title=\"https://huggingface.co/JackFram/llama-160m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">LLaMA-160M</a></td></tr><tr><td>OPT-6.7B</td><td>facebook/opt-6.7b</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ffacebook%2Fopt-125m\" title=\"https://huggingface.co/facebook/opt-125m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">OPT-125M</a></td></tr><tr><td>OPT-13B</td><td>facebook/opt-13b</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ffacebook%2Fopt-125m\" title=\"https://huggingface.co/facebook/opt-125m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">OPT-125M</a></td></tr><tr><td>OPT-30B</td><td>facebook/opt-30b</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ffacebook%2Fopt-125m\" title=\"https://huggingface.co/facebook/opt-125m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">OPT-125M</a></td></tr><tr><td>OPT-66B</td><td>facebook/opt-66b</td><td><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2Ffacebook%2Fopt-125m\" title=\"https://huggingface.co/facebook/opt-125m\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">OPT-125M</a></td></tr><tr><td>Falcon-7B</td><td>tiiuae/falcon-7b</td><td></td></tr><tr><td>Falcon-40B</td><td>tiiuae/falcon-40b</td><td></td></tr><tr><td>StarCoder-15.5B</td><td>bigcode/starcoder</td><td></td></tr></tbody></table>\\n<h3>与其他框架（vLLM、TGI、FasterTransformer）的性能对比</h3>\\n<p>FlexFlow Serve 在单节点多 GPU 推理方面比现有系统高 1.3-2.0 倍，在多节点多 GPU 推理方面比现有系统高 1.4-2.4 倍。</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d5abe24e29f0495f995fbed37c8dc765~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1241&amp;h=480&amp;s=60989&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>提示数据集</h3>\\n<p>FlexFlow 提供了五个用于评估 FlexFlow Serve 的提示数据集：</p>\\n<ul>\\n<li>Chatbot 指令提示：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fspecinfer.s3.us-east-2.amazonaws.com%2Fprompts%2Fchatbot.json\" title=\"https://specinfer.s3.us-east-2.amazonaws.com/prompts/chatbot.json\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">specinfer.s3.us-east-2.amazonaws.com/prompts/cha…</a></li>\\n<li>ChatGPT 提示：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fspecinfer.s3.us-east-2.amazonaws.com%2Fprompts%2Fchatgpt.json\" title=\"https://specinfer.s3.us-east-2.amazonaws.com/prompts/chatgpt.json\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">specinfer.s3.us-east-2.amazonaws.com/prompts/cha…</a></li>\\n<li>WebQA：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fspecinfer.s3.us-east-2.amazonaws.com%2Fprompts%2Fwebqa.json\" title=\"https://specinfer.s3.us-east-2.amazonaws.com/prompts/webqa.json\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">specinfer.s3.us-east-2.amazonaws.com/prompts/web…</a></li>\\n<li>Alpaca：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fspecinfer.s3.us-east-2.amazonaws.com%2Fprompts%2Falpaca.json\" title=\"https://specinfer.s3.us-east-2.amazonaws.com/prompts/alpaca.json\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">specinfer.s3.us-east-2.amazonaws.com/prompts/alp…</a></li>\\n<li>PIQA：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fspecinfer.s3.us-east-2.amazonaws.com%2Fprompts%2Fpiqa.json\" title=\"https://specinfer.s3.us-east-2.amazonaws.com/prompts/piqa.json\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">specinfer.s3.us-east-2.amazonaws.com/prompts/piq…</a></li>\\n</ul>\\n<h3>未来的规划</h3>\\n<p>FlexFlow Serve 正在积极开发中，主要专注于以下任务：</p>\\n<ul>\\n<li>AMD 基准测试。目前正在积极致力于在 AMD GPU 上对 FlexFlow Serve 进行基准测试，并将其与 NVIDIA GPU 上的性能进行比较。</li>\\n<li>Chatbot prompt 模板和多轮对话</li>\\n<li>支持 FastAPI</li>\\n<li>与LangChain集成进行文档问答</li>\\n</ul>\\n<h2>LMDeploy</h2>\\n<ul>\\n<li>GitHub: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FInternLM%2Flmdeploy\" title=\"https://github.com/InternLM/lmdeploy\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/InternLM/lm…</a></li>\\n</ul>\\n<h3>简介</h3>\\n<p>LMDeploy 由\\xa0<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fopen-mmlab%2Fmmdeploy\" title=\"https://github.com/open-mmlab/mmdeploy\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">MMDeploy</a>\\xa0和\\xa0<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fopen-mmlab%2Fmmrazor\" title=\"https://github.com/open-mmlab/mmrazor\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">MMRazor</a>\\xa0团队联合开发，是涵盖了 LLM 任务的全套轻量化、部署和服务解决方案。 这个强大的工具箱提供以下核心功能：</p>\\n<ul>\\n<li><strong>高效推理引擎 TurboMind</strong>：基于\\xa0<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2FFasterTransformer\" title=\"https://github.com/NVIDIA/FasterTransformer\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">FasterTransformer</a>推理引擎，实现了高效推理引擎 TurboMind，支持 InternLM、LLaMA、vicuna等模型在 NVIDIA GPU 上的推理。</li>\\n<li><strong>交互推理方式</strong>：通过缓存多轮对话过程中 attention 的 k/v，记住对话历史，从而避免重复处理历史会话。</li>\\n<li><strong>多 GPU 部署和量化</strong>：提供了全面的模型部署和量化（支持使用AWQ算法对模型权重进行 INT4 量化，支持 KV Cache INT8 量化）支持，已在不同规模上完成验证。</li>\\n<li><strong>persistent batch 推理</strong>：进一步优化模型执行效率。</li>\\n<li>支持张量并行推理（注意：量化部署时不支持进行张量并行）</li>\\n</ul>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/757afd57a68f4d698559033e3af35f83~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=990&amp;h=460&amp;s=42660&amp;e=png&amp;b=fcf8f7\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>支持的模型</h3>\\n<p><code>LMDeploy</code> 支持 <code>TurboMind</code> 和 <code>Pytorch</code> 两种推理后端。</p>\\n<p><strong>TurboMind</strong></p>\\n<blockquote>\\n<p><strong>注意：</strong><br/>\\nW4A16 推理需要 Ampere 及以上架构的 Nvidia GPU</p>\\n</blockquote>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>模型</th><th>模型并行</th><th>FP16</th><th>KV INT8</th><th>W4A16</th><th>W8A8</th></tr></thead><tbody><tr><td>Llama</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>Llama2</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>InternLM-7B</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>InternLM-20B</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>QWen-7B</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td></tr><tr><td>Baichuan-7B</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>Baichuan2-7B</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr><tr><td>Code Llama</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr></tbody></table>\\n<p><strong>Pytorch</strong></p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>模型</th><th>模型并行</th><th>FP16</th><th>KV INT8</th><th>W4A16</th><th>W8A8</th></tr></thead><tbody><tr><td>Llama</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr><tr><td>Llama2</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr><tr><td>InternLM-7B</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr></tbody></table>\\n<h3>与其他框架（HF、DeepSpeed、vLLM）的性能对比</h3>\\n<p><strong>场景一</strong>: 固定的输入、输出token数（1,2048），测试 output token throughput</p>\\n<p><strong>场景二</strong>: 使用真实数据，测试 request throughput</p>\\n<p>测试配置：LLaMA-7B, NVIDIA A100(80G)</p>\\n<p>TurboMind 的 output token throughput 超过 2000 token/s, 整体比 DeepSpeed 提升约 5% - 15%，比 huggingface transformers 提升 2.3 倍\\n在 request throughput 指标上，TurboMind 的效率比 vLLM 高 30%。</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fceaa4f6da59436f83a13a30428f8ecf~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4494&amp;h=1320&amp;s=201272&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h2>结语</h2>\\n<p>总而言之，大模型推理框架的核心目标都是为了降低延迟；同时，尽可能地提升吞吐量；从上面的框架中可以看到，每个框架各有优缺点，但是目前来看，还没有一个LLM推理框架有一统天下的态势，大家都在加速迭代。</p>', 'author': '吃果冻不吐果冻皮', 'reading_time': 19, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 70, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'c957199e3c5587f0d8aa1c419ec1b2d10be2b566515d05a9671874f39896de03', 'title': '【技术干货】Attacking SOAP API', 'url': 'https://juejin.cn/post/7286508785104846882', 'published_at': '2023-10-07T03:42:09Z', 'created_at': '2023-10-07T09:03:25.142314Z', 'changed_at': '2023-10-07T09:03:25.142314Z', 'content': '<h2>SOAP API 介绍</h2>\\n<p>SOAP（Simple Object Access Protocol）简单对象访问协议是交换数据的一种协议规范，是一种轻量的、简单的、基于XML（标准通用标记语言下的一个子集）的协议，它被设计成在WEB上交换结构化的和固化的信息。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/464b42ddc9924827a2e12a9b966901f2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=186&amp;s=50901&amp;e=jpg&amp;b=fdfcfc\" alt=\"1696648979_6520cf130d016a7295e69.png!small?1696648980635\" loading=\"lazy\"/></p>\\n<p>SOAP API是采用SOAP协议的应用程序接口，和REST API不同的是，SOAP 是一种协议，而 REST 是一种架构风格，这对 SOAP API 和 REST API 的行为造成了显著差异，比如：</p>\\n<ul>\\n<li>SOAP API 仅允许在应用程序之间进行 XML 消息收发</li>\\n<li>SOAP 消息更大、更复杂，这使得其传输和处理速度变慢</li>\\n<li>SOAP 协议要求应用程序存储请求之间的状态，提高了带宽和内存要求，更难以扩展。</li>\\n<li>SOAP 内置了错误处理逻辑，因此相比于REST API更加可靠。</li>\\n</ul>\\n<p><strong>SOAP的组成</strong>：</p>\\n<p>在SOAP API的消息中存在了四个不同的元素：</p>\\n<ul>\\n<li><strong>Envelope</strong>: 是将文档标识为 SOAP 消息而不是任何其他类型的 XML 文档的基本元素。消息以信封的标签开始和结束。（必须元素）</li>\\n<li><strong>Header</strong>: 是一个可选元素，可以使用 SOAP 模块添加新特性和功能。一个Envelope中可以包含多个标题。（可选元素）</li>\\n<li><strong>body</strong>: 正文包含实际消息：请求或响应。（必须元素）</li>\\n<li><strong>Fault</strong>: 如果在处理过程中出现问题，则用于错误消息和状态信息。（可选元素）</li>\\n</ul>\\n<h2>SOAP API 发现</h2>\\n<ul>\\n<li>消息体格式判断，一般来说SOAP API请求数据包采用xml格式进行消息传输</li>\\n<li>WSDL文件，可以通过Googlehacking的语法找到对应的WSDL文件，例如：</li>\\n</ul>\\n<pre><code></code></pre>\\n<p><code>inurl:jws?wsdl inurl:asmx?wsdl inurl:aspx?wsdl inurl:ascx?wsdl inurl:ashx?wsdl inurl:dll?wsdl inurl:exe?wsdl inurl:php?wsdl inurl:pl?wsdl inurl:?wsdl filetype:jws filetype:asmx filetype:ascx ......</code></p>\\n<p>``</p>\\n<ul>\\n<li>通过流量被动识别，例如通过burpsuite插件或者其他工具进行被动特征识别。</li>\\n</ul>\\n<h2>SOAP API 攻击面</h2>\\n<h3>SOAP 注入</h3>\\n<p>SOAP 注入是指在SOAP 协议中，连接服务端和客户端API处发生的注入，通过向SOAP API发送消息的参数添加注入语句来实现注入效果，通常包括SQL注入、XML注入、代码注入。</p>\\n<p><strong>漏洞案例</strong>：</p>\\n<p>在识别到SOAP API之后，通过对消息体中的参数进行简单的sql注入判断，发现数据库报错信息</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8b30aa92152343c5804f6d7efcfcc203~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=565&amp;s=220522&amp;e=jpg&amp;b=fcfcfc\" alt=\"1696649003_6520cf2b30b429ed10048.png!small?1696649004528\" loading=\"lazy\"/></p>\\n<p>使用条件语句继续进行判断</p>\\n<pre><code></code></pre>\\n<p><code>&#39; or 1=1 ---qqq ’ or 1=2 ---qqq</code></p>\\n<p>``</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/137e5585a1e34b5080f0f44ef35988d4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=343&amp;s=156190&amp;e=jpg&amp;b=fdfcfc\" alt=\"1696649015_6520cf37cb0fcc6d8a0f2.png!small?1696649017252\" loading=\"lazy\"/></p>\\n<p>从返回的结果判断该参数部分存在sql注入</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/24cf3b5b099749d4abe1bd5a99d4469c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=352&amp;s=156722&amp;e=jpg&amp;b=fdfcfc\" alt=\"1696649026_6520cf42c931bd07e86a3.png!small?1696649028724\" loading=\"lazy\"/></p>\\n<p>后续使用sqlmap工具进一步利用</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/70da0eb15a65427d83282a7428c8e152~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=422&amp;s=161167&amp;e=jpg&amp;b=2e2f29\" alt=\"1696649037_6520cf4d3e84144667bd4.png!small?1696649039070\" loading=\"lazy\"/></p>\\n<p>成功注入并获取数据库名。</p>\\n<h3>XXE</h3>\\n<p>在SOAP API中，当允许引用外部实体时，通过构造恶意内容，可能导致任意文件读取、系统命令执行、内网端口探测等风险危害。</p>\\n<p><strong>漏洞案例</strong>：</p>\\n<p>通过Wsdler插件找到SOAP API</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/82ce156c907845a7a9adeac7b56c1739~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=317&amp;s=97677&amp;e=jpg&amp;b=fbfbfb\" alt=\"1696649047_6520cf57840fcbfceb629.png!small?1696649048816\" loading=\"lazy\"/></p>\\n<p>尝试进行外部实体注入，执行任意文件读取</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d6d409a99db24496a1157a885c5b9b87~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=390&amp;s=188926&amp;e=jpg&amp;b=fcfbfb\" alt=\"1696649058_6520cf622bf333eef7910.png!small?1696649059802\" loading=\"lazy\"/></p>\\n<p>成功利用并回显。</p>\\n<h3>SSRF</h3>\\n<p>在SOAP API中，如果允许本地主机或者其他服务器不限制输入，就可能遭受服务端请求伪造攻击风险。</p>\\n<p><strong>漏洞案例</strong>：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/06b9ea15aaa84ede8afbcce528ad7923~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=429&amp;s=96679&amp;e=jpg&amp;b=fefefe\" alt=\"1696649069_6520cf6d480f204212b87.png!small?1696649070866\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/39ed5a3bdd9347e9a072268ffc2f57a0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=295&amp;s=65387&amp;e=jpg&amp;b=ffffff\" alt=\"1696649079_6520cf7735fa3e65fd5c7.png!small?1696649080864\" loading=\"lazy\"/></p>\\n<h3>XSS</h3>\\n<p>在SOAP API请求消息中，同样容易遭受XSS漏洞攻击</p>\\n<p><strong>漏洞案例</strong>：</p>\\n<p>同样的，在相关参数部分插入xss payload，值得注意的是payload中的尖括号要进行HTML编码，不然会造成SOAP标签解析错误从而报错。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/408126c2d3214d3ab8be11e481f6bb11~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=423&amp;s=209977&amp;e=jpg&amp;b=fcfbfb\" alt=\"1696649088_6520cf80a2b2594442174.png!small?1696649089986\" loading=\"lazy\"/></p>\\n<p>成功解析并触发弹框</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ebc9a7483d5648a083cd84ca52f76cc1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=373&amp;s=84827&amp;e=jpg&amp;b=000000\" alt=\"1696649098_6520cf8a19510a1467937.png!small?1696649099305\" loading=\"lazy\"/></p>\\n<h3>用户名枚举</h3>\\n<p>在SOAP API中，同样由于相关业务设计的疏忽，可能也会存在一些业务性漏洞，比如：用户名枚举、验证码爆破等等</p>\\n<p><strong>漏洞案例</strong>：</p>\\n<p>在SOAP API请求中对username参数进行枚举</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2cc8b3aac82449f29a2c3eb420cfc67f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=311&amp;s=152921&amp;e=jpg&amp;b=fdfcfc\" alt=\"1696649108_6520cf945cc2f5ac9abaf.png!small?1696649109680\" loading=\"lazy\"/></p>\\n<p>当存在用户时返回：User Exists</p>\\n<p>当不存在用户时返回：User Not Found</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ce7516d0c1ba40169c5d168b908166dc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=323&amp;s=145470&amp;e=jpg&amp;b=fdfcfc\" alt=\"1696649132_6520cfac454a07c85acf4.png!small?1696649133613\" loading=\"lazy\"/></p>\\n<h3>命令执行</h3>\\n<p>SOAP API的命令执行漏洞其实也输入SOAP注入的一种，通过对相关参数部分植入命令执行payload，由于相关验证机制不全或者被绕过，可能导致恶意命令被执行。</p>\\n<p><strong>漏洞案例</strong>：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b7e3270886fb4e8ba3d6b23917bd7937~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=389&amp;s=180795&amp;e=jpg&amp;b=fcfbfb\" alt=\"1696649141_6520cfb5378b6133309aa.png!small?1696649142525\" loading=\"lazy\"/></p>\\n<h3>DOS</h3>\\n<p>在SOAP API中采用的一般都是XML数据格式，请求中的XML数据会由服务端的XML解析器进行解析和处理，在这个过程中，通过对相关元素、属性进行操作可以实行DOS攻击，除此之外利用XXE漏洞也可以进行DOS攻击。下面将列举相关DOS攻击示例：</p>\\n<p>针对元素名称的DoS攻击的示例</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/911fd576ecb54c88911a8cd4bd1f174a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=514&amp;s=185293&amp;e=jpg&amp;b=fdfdfd\" alt=\"1696649150_6520cfbe06b1679641c2b.png!small?1696649151392\" loading=\"lazy\"/></p>\\n<p>针对元素属性的DoS攻击的示例</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/968565f567104c47b2de7a91dc866bd5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=551&amp;s=187120&amp;e=jpg&amp;b=fdfcfc\" alt=\"1696649159_6520cfc7b44dd4a3c438f.png!small?1696649161213\" loading=\"lazy\"/></p>\\n<p>针对元素个数的DoS攻击的示例（也可以通过重复某个特定元素达到同样效果）</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8f1f225781174540a9fc200e7cbfd819~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=595&amp;s=233769&amp;e=jpg&amp;b=fdfcfc\" alt=\"1696649170_6520cfd2c6d498970c124.png!small?1696649172246\" loading=\"lazy\"/></p>\\n<p>利用XXE漏洞造成DoS示例</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/465eac41804b4094ae4bfed52a80110b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=690&amp;h=573&amp;s=206291&amp;e=jpg&amp;b=fdfdfd\" alt=\"1696649180_6520cfdc88b26a429e8a1.png!small?1696649182106\" loading=\"lazy\"/></p>\\n<h2>渗透测试工具</h2>\\n<p>SOAP API的渗透测试和对常规API渗透测试是一样的、只是，可以使用安全工具来辅助进行，部分工具如下：</p>\\n<ul>\\n<li>SoapUI，地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.soapui.org%2F\" title=\"https://www.soapui.org/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.soapui.org/</a></li>\\n<li>SOApSonar，地址：<a href=\"https://link.juejin.cn?target=http%3A%2F%2Fwww.crosschecknet.com%2Fproducts%2Fsoapsonar%2F\" title=\"http://www.crosschecknet.com/products/soapsonar/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.crosschecknet.com/products/so…</a></li>\\n<li>SoapUI NG Pro，地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fsmartbear.com%2Fproduct%2Fready-api%2Fsoapui-ng%2Ffree-trial%2F\" title=\"https://smartbear.com/product/ready-api/soapui-ng/free-trial/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">smartbear.com/product/rea…</a></li>\\n<li>WSSAT，地址：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FYalcinYolalan%2FWSSAT\" title=\"https://github.com/YalcinYolalan/WSSAT\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/YalcinYolal…</a></li>\\n<li>Wsdler，Burp应用商店下载。</li>\\n</ul>\\n<p>工具的介绍和使用在这里不做过多演示，感兴趣的小伙伴可以利用靶场等环境自行测试。</p>', 'author': '星阑科技', 'reading_time': 5, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 69, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '183903b95e1d5177580d3bbd889e2096202080467116de5ccff9f9116545d2c5', 'title': '自动驾驶传感器技术', 'url': 'https://juejin.cn/post/7286508785104781346', 'published_at': '2023-10-07T03:32:21Z', 'created_at': '2023-10-07T09:03:25.137436Z', 'changed_at': '2023-10-07T09:03:25.137436Z', 'content': '<p>自动驾驶传感器技术是自动驾驶系统的关键组成部分，它使车辆能够感知并理解周围环境。本文将深入探讨自动驾驶传感器技术，包括常见类型、工作原理以及它们在自动驾驶中的作用。</p>\\n<h2>1. 摄像头</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0e9af5c22aff4c8eab52e99c96b26872~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=558&amp;h=488&amp;s=144698&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>摄像头的工作原理</h3>\\n<p>摄像头是基于光学原理的传感器，其工作原理基于以下过程：</p>\\n<ul>\\n<li><strong>光学透镜：</strong> 摄像头的前部有一个透镜，它负责捕捉光线并将其聚焦在摄像头传感器上。</li>\\n<li><strong>图像传感器：</strong> 图像传感器是摄像头的核心组成部分，通常是一块芯片。它包含许多小的光敏元件，称为像素。每个像素负责测量特定区域的光强度。</li>\\n<li><strong>光线捕捉：</strong> 当光线穿过透镜并照射到图像传感器上时，像素将测量光线的强度。不同强度的光线创建了图像中不同部分的亮度和颜色。</li>\\n<li><strong>数据处理：</strong> 通过测量每个像素的光强度，图像传感器将光学信息转换为数字图像数据。这些数据可以表示为像素阵列，每个像素都有一个与其亮度和颜色相关的数值。</li>\\n</ul>\\n<h3>摄像头的种类</h3>\\n<p>在自动驾驶领域，有几种不同类型的摄像头，用于不同的用途：</p>\\n<ul>\\n<li><strong>单目摄像头：</strong> 单目摄像头只有一个透镜和一个图像传感器。它们通常用于捕捉前方的道路图像，但缺乏深度感知能力。</li>\\n<li><strong>立体摄像头：</strong> 立体摄像头由两个摄像头组成，模拟人类的双眼视觉。这种摄像头能够提供深度信息，有助于识别距离和三维结构。</li>\\n<li><strong>鱼眼摄像头：</strong> 鱼眼摄像头使用广角透镜捕捉大范围的图像，适用于全景感知和环境监测。</li>\\n<li><strong>红外摄像头：</strong> 红外摄像头能够捕捉红外光谱范围的图像，用于夜间和低光环境下的感知。</li>\\n</ul>\\n<h3>摄像头在自动驾驶中的作用</h3>\\n<p>摄像头在自动驾驶系统中扮演了关键的角色，具体作用如下：</p>\\n<ul>\\n<li><strong>视觉感知：</strong> 摄像头用于捕捉车辆周围的视觉信息，包括道路、车辆、行人、道路标志和交通信号等。这些信息对于实时感知和决策至关重要。</li>\\n<li><strong>对象识别：</strong> 通过计算机视觉技术，摄像头可以帮助自动驾驶系统识别和分类各种对象，如其他车辆、行人和障碍物。这有助于规划安全的驾驶路径。</li>\\n<li><strong>车道保持：</strong> 摄像头可以监测车辆在道路上的位置，帮助自动驾驶系统维持车辆在正确的车道上行驶。</li>\\n<li><strong>交通信号识别：</strong> 摄像头可以检测和识别交通信号、标志和路牌，以确保车辆遵循交通规则。</li>\\n<li><strong>目标跟踪：</strong> 摄像头可以跟踪其他车辆和行人的运动，以预测它们的行为并采取适当的驾驶决策。</li>\\n</ul>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1788f9c1e2ba454fb6bca977ffdbb311~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=592&amp;h=351&amp;s=18798&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<p>总之，摄像头在自动驾驶中起到了关键的感知作用，使车辆能够了解周围环境并做出智能决策。通过不断改进图像处理算法和传感器技术，摄像头在自动驾驶系统中的作用将变得越来越重要。</p>\\n<h2>2. 雷达</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9e9371d173c24ee19af9a3b2f8f9a3e8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=540&amp;s=1083888&amp;e=png&amp;b=083734\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>雷达的工作原理</h3>\\n<p>雷达（Radar，即Radio Detection and Ranging）是一种使用电磁波进行远距离感知和定位的技术。其工作原理基于以下基本原理：</p>\\n<ul>\\n<li><strong>发射信号：</strong> 雷达系统会发射一束电磁波（通常是微波或毫米波），这些波束传播到周围的环境中。</li>\\n<li><strong>接收反射信号：</strong> 一旦电磁波遇到物体，它们会被反射回雷达系统。</li>\\n<li><strong>测量时间延迟：</strong> 雷达系统会测量发射信号和反射信号之间的时间延迟，也称为往返时间。</li>\\n<li><strong>计算距离：</strong> 通过知道电磁波的传播速度（等于光速），雷达可以计算出物体到雷达系统的距离，通过以下公式计算：距离 = (速度 x 时间延迟) / 2。</li>\\n<li><strong>分析反射强度：</strong> 雷达还可以测量反射信号的强度，这有助于确定物体的大小和形状。</li>\\n</ul>\\n<h3>雷达的种类</h3>\\n<p>在自动驾驶领域，存在不同类型的雷达，用于不同的应用和距离范围：</p>\\n<ul>\\n<li><strong>毫米波雷达：</strong> 毫米波雷达使用毫米波段的电磁波，通常在76 GHz到81 GHz的频率范围内。它们适用于中短距离感知，如障碍物检测和自适应巡航控制。</li>\\n<li><strong>长距离雷达：</strong> 长距离雷达通常工作在更低的频率范围，如24 GHz到40 GHz，以便在远距离上探测目标。它们用于检测远处的车辆和物体，帮助车辆规划安全路径。</li>\\n<li><strong>立体雷达：</strong> 立体雷达由多个雷达传感器组成，通常位于车辆的前部和后部。它们用于创建环绕车辆的三维感知，帮助车辆感知和跟踪其他车辆的位置和速度。</li>\\n</ul>\\n<h3>雷达在自动驾驶中的作用</h3>\\n<p>雷达在自动驾驶系统中扮演着关键的角色，具体作用如下：</p>\\n<ul>\\n<li><strong>远距离感知：</strong> 雷达可以检测远距离上的车辆、行人和障碍物，有助于车辆及时采取避障措施。</li>\\n<li><strong>速度测量：</strong> 通过测量目标的速度，雷达可以帮助自动驾驶系统预测其他车辆的行为，从而更好地规划驾驶路径。</li>\\n<li><strong>障碍物检测：</strong> 雷达可以检测前方的障碍物，包括静态物体和运动物体，以避免碰撞。</li>\\n<li><strong>夜间和恶劣天气下的感知：</strong> 与视觉传感器不同，雷达不受光线条件的限制，因此在夜间和恶劣天气条件下仍能工作良好。</li>\\n<li><strong>数据融合：</strong> 雷达数据通常与其他传感器数据（如摄像头和激光雷达）结合使用，以提供全面的环境感知。</li>\\n</ul>\\n<p>总之，雷达是自动驾驶系统中不可或缺的一部分，它提供了远距离感知和可靠的物体检测能力，有助于确保车辆在复杂的交通环境中安全行驶。通过不断改进雷达技术和数据处理算法，自动驾驶车辆的感知和决策能力将得到进一步提升。</p>\\n<h2>3. 激光雷达 (LiDAR)</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/df16b1f9e7fe4ae4911174469f67a553~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=650&amp;h=434&amp;s=536800&amp;e=png&amp;b=1c303c\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>激光雷达的工作原理</h3>\\n<p>激光雷达的工作原理基于光的传播和反射原理：</p>\\n<ul>\\n<li><strong>激光发射：</strong> 激光雷达通过发射非可见激光束（通常是红外激光）来测量物体。这束激光光束以非常高的速度传播。</li>\\n<li><strong>光线反射：</strong> 激光束遇到物体后会被反射回激光雷达。</li>\\n<li><strong>测量时间延迟：</strong> 激光雷达测量发射激光束和反射激光束之间的时间延迟（往返时间）。</li>\\n<li><strong>计算距离：</strong> 通过知道光的速度（光速）以及往返时间，激光雷达可以准确计算物体到雷达的距离。</li>\\n<li><strong>激光束扫描：</strong> 激光雷达通常会以旋转或者以其他方式扫描激光束，从而生成物体的三维点云图，包含了物体的位置和形状。</li>\\n</ul>\\n<h3>激光雷达的类型</h3>\\n<p>有不同类型的激光雷达，它们具有不同的工作原理和性能特点：</p>\\n<ul>\\n<li><strong>机械式激光雷达：</strong> 机械式激光雷达通过机械装置旋转激光传感器来扫描周围环境。这种类型通常拥有较高的分辨率和测量精度。</li>\\n<li><strong>固态激光雷达：</strong> 固态激光雷达采用固定的传感器头，它们使用电子控制来改变激光束的方向。它们通常更紧凑，更耐用，但分辨率可能较低。</li>\\n<li><strong>多光束激光雷达：</strong> 这种类型的激光雷达同时发射多个激光束，以加快扫描速度和提高点云密度。</li>\\n<li><strong>固态光学雷达：</strong> 这种新兴的技术不使用机械部件，而是依靠微光学元件来扫描激光束，具有更小的体积和更快的响应速度。</li>\\n</ul>\\n<h3>激光雷达在自动驾驶中的作用</h3>\\n<p>激光雷达在自动驾驶系统中具有关键作用，包括：</p>\\n<ul>\\n<li><strong>高精度的距离测量：</strong> 激光雷达提供高精度的距离测量，使车辆能够准确地感知周围环境中的障碍物、其他车辆和道路结构。</li>\\n<li><strong>三维环境感知：</strong> 通过生成三维点云地图，激光雷达使车辆能够了解周围环境的三维结构，有助于识别物体并进行路径规划。</li>\\n<li><strong>低光和恶劣天气下的稳定性：</strong> 激光雷达不受光照条件的限制，因此在夜间、雨雪和浓雾等恶劣天气下表现出色。</li>\\n<li><strong>障碍物检测和避免：</strong> 激光雷达能够检测和跟踪障碍物的位置和运动，帮助车辆规划避障路径。</li>\\n<li><strong>数据融合：</strong> 激光雷达数据通常与其他传感器数据（如摄像头和雷达）结合使用，以提供全面的环境感知。</li>\\n</ul>\\n<p>总之，激光雷达在自动驾驶中扮演了关键的角色，提供了高精度、可靠的环境感知数据，有助于确保车辆在各种复杂的道路和天气条件下安全行驶。随着激光雷达技术的不断发展和改进，自动驾驶系统将变得更加智能和可靠。</p>\\n<h2>4. 超声波传感器</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5fd0d476761b41f5b914ca00725be581~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=559&amp;h=387&amp;s=219134&amp;e=png&amp;b=fbf3ef\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>超声波传感器的工作原理</h3>\\n<p>超声波传感器的工作原理基于声波的传播和回声原理：</p>\\n<ul>\\n<li><strong>声波发射：</strong> 超声波传感器会发射高频率声波（通常在20 kHz到65 kHz之间），这些声波传播到周围的环境中。</li>\\n<li><strong>声波反射：</strong> 当声波遇到物体时，它们会被反射回传感器。</li>\\n<li><strong>测量时间延迟：</strong> 超声波传感器测量发射声波和反射声波之间的时间延迟（往返时间）。</li>\\n<li><strong>计算距离：</strong> 通过知道声波在空气中的传播速度（约为343米/秒），传感器可以准确计算物体到传感器的距离，使用以下公式：距离 = (速度 x 时间延迟) / 2。</li>\\n</ul>\\n<h3>超声波传感器的种类</h3>\\n<p>有几种不同类型的超声波传感器，用于不同的应用：</p>\\n<ul>\\n<li><strong>单发射单接收（1S1R）超声波传感器：</strong> 这种传感器只有一个发射器和一个接收器，主要用于测量距离。</li>\\n<li><strong>单发射多接收（1S多R）超声波传感器：</strong> 这种传感器包含一个发射器和多个接收器，可以提高距离测量的准确性和稳定性。</li>\\n<li><strong>多发射多接收（多S多R）超声波传感器：</strong> 这种传感器包括多个发射器和多个接收器，可用于检测多个方向的障碍物。</li>\\n</ul>\\n<h3>超声波传感器在自动驾驶中的作用</h3>\\n<p>超声波传感器在自动驾驶系统中具有多种作用，包括：</p>\\n<ul>\\n<li><strong>近距离感知：</strong> 超声波传感器通常用于近距离感知，如停车、泊车和低速驾驶。它们可以检测车辆周围的障碍物，以帮助避免碰撞。</li>\\n<li><strong>停车辅助：</strong> 在停车或泊车时，超声波传感器可以提供准确的距离测量，以确保车辆停在正确的位置。</li>\\n<li><strong>低速驾驶辅助：</strong> 超声波传感器还可用于在低速驾驶时检测前方的障碍物，帮助车辆避免碰撞。</li>\\n<li><strong>辅助自动驾驶：</strong> 在某些自动驾驶系统中，超声波传感器可以与其他传感器（如摄像头和雷达）结合使用，以提供全面的环境感知。</li>\\n<li><strong>停车和起步：</strong> 超声波传感器还可以用于帮助车辆进行平稳的停车和起步，尤其是在拥挤的交通情况下。</li>\\n</ul>\\n<p>虽然超声波传感器主要用于近距离感知，但它们在低速驾驶和精确定位方面具有重要作用。通过将超声波传感器与其他传感器技术整合，自动驾驶系统能够在各种交通和停车场景中更加智能地操作。</p>\\n<h2>5. GPS和IMU</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4717c3a463614629972bf75cdc026bb8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1440&amp;h=1440&amp;s=468116&amp;e=png&amp;b=e9e7ea\" alt=\"\" loading=\"lazy\"/></p>\\n<p>全球定位系统（GPS）和惯性测量单元（IMU）是自动驾驶系统中用于定位和导航的重要传感器和技术。</p>\\n<h3>GPS（全球定位系统）</h3>\\n<p>GPS是一种卫星导航系统，由美国政府维护和控制，用于确定地球上任何点的精确三维位置。GPS工作原理如下：</p>\\n<ul>\\n<li><strong>卫星定位：</strong> GPS系统由一组卫星组成，它们在地球轨道上运行。这些卫星发射连续的无线电信号，包含卫星的精确位置和时间信息。</li>\\n<li><strong>接收GPS信号：</strong> GPS接收器（通常是车辆上的GPS天线和接收器）接收来自多颗卫星的信号。接收器可以同时接收多颗卫星的信号。</li>\\n<li><strong>三角测量：</strong> GPS接收器根据来自不同卫星的信号和每颗卫星的已知位置，通过三角测量法计算出接收器的精确位置（纬度、经度和海拔高度）。</li>\\n<li><strong>数据融合：</strong> 为了提高精度和可靠性，GPS通常与其他传感器数据（如IMU、激光雷达和地图数据）一起使用。数据融合技术可帮助处理GPS信号中的误差和不确定性，以提供更准确的定位信息。</li>\\n</ul>\\n<p>GPS的优势包括全球范围内的覆盖和相对较高的精度。然而，它可能在高楼大厦、山脉、森林覆盖等情况下受到信号遮挡，因此需要与其他传感器一起使用，以获得更可靠的定位信息。</p>\\n<h3>IMU（惯性测量单元）</h3>\\n<p>IMU是一种传感器组合，通常包括加速度计和陀螺仪，用于测量车辆的运动状态和方向。IMU的工作原理如下：</p>\\n<ul>\\n<li><strong>加速度计：</strong> 加速度计测量车辆的加速度，即速度的变化率。通过积分加速度数据，可以计算车辆的速度和位移。</li>\\n<li><strong>陀螺仪：</strong> 陀螺仪测量车辆的旋转速度和方向。通过积分陀螺仪数据，可以计算车辆的方向和角度。</li>\\n<li><strong>数据融合：</strong> IMU通常与其他传感器数据（如GPS和激光雷达）一起使用，以提供全面的车辆定位和导航信息。数据融合算法可将不同传感器的数据整合在一起，以减少误差和提高精度。</li>\\n</ul>\\n<p>IMU的优势包括实时性强、对信号遮挡不敏感以及在短时间内提供高频率的运动信息。然而，IMU也存在累积误差的问题，因此需要定期校准和数据融合来提高精度。</p>\\n<h3>GPS和IMU在自动驾驶中的作用</h3>\\n<p>GPS和IMU在自动驾驶系统中扮演了关键的角色，具体作用如下：</p>\\n<ul>\\n<li><strong>全球定位和导航：</strong> GPS提供了车辆的全球位置信息，而IMU提供了车辆的运动状态信息。结合使用，它们支持车辆的全球定位和导航。</li>\\n<li><strong>车辆定位：</strong> GPS和IMU一起用于确定车辆的精确位置、方向和速度，以便进行路径规划和控制。</li>\\n<li><strong>地图匹配：</strong> 通过将GPS和IMU数据与高精度地图进行匹配，自动驾驶系统可以更准确地定位车辆，并在复杂城市环境中进行导航。</li>\\n<li><strong>姿态估计：</strong> IMU数据用于估计车辆的姿态（即倾斜和旋转角度），这对于控制车辆的稳定性和安全性至关重要。</li>\\n</ul>\\n<p>总之，GPS和IMU是自动驾驶系统中不可或缺的传感器技术，它们提供了车辆的定位、导航和运动状态信息，有助于实现安全、高效和精确的自动驾驶。通过数据融合和算法改进，这些传感器可以提供更可靠的车辆定位和导航性能。</p>\\n<h2>总结</h2>\\n<p>自动驾驶传感器技术的整合和数据融合是自动驾驶系统的核心挑战之一。通过将不同类型的传感器数据整合在一起，车辆可以建立对周围环境的全面理解，并做出相应的决策和控制操作。这些传感器技术的不断创新和改进将为实现更安全、高效和可靠的自动驾驶交通做出贡献。</p>', 'author': '探索者日记', 'reading_time': 11, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 58, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '81f5b391e346f1671c9030fa98281bf15946cb90b9ec94ce5da15da27187534b', 'title': '要体验 AI 编程助手吗？', 'url': 'https://juejin.cn/post/7286669548740034617', 'published_at': '2023-10-07T03:25:29Z', 'created_at': '2023-10-07T07:03:23.141362Z', 'changed_at': '2023-10-07T07:03:23.141362Z', 'content': '<p>能不能用 AI 编程辅助写代码？</p>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DSF\" title=\"https://dev.amazoncloud.cn/?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=SF\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">亚马逊云科技开发者社区</a>为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，点<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpassport.amazoncloud.cn%2Fsignin%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DSF\" title=\"https://passport.amazoncloud.cn/signin?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=SF\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">这里</a>让它成为你的技术宝库！</p>\\n</blockquote>\\n<p>答案是能！</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c6d9551f2517425c81cd287ed9c85134~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=2137&amp;s=845142&amp;e=png&amp;b=e9f0fb\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>文章来源：\\n<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F65137c50659184378dd23871%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/column/article/65137c50659184378dd23871?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">dev.amazoncloud.cn/column/arti…</a></p>', 'author': '亚马逊云开发者', 'reading_time': 1, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 68, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '8651a69c987b566d02511582893a74b6c4fe95fd9749d555db8b5a922753887d', 'title': '抖音与美团展开攻防战，谁能在竞争中占据主导地位？/ 人工智能技术突破，实现与鲸鱼对话，拯救海洋生命的新希望｜魔法半周报', 'url': 'https://juejin.cn/post/7286670786562916410', 'published_at': '2023-10-07T03:18:16Z', 'created_at': '2023-10-07T09:03:25.133911Z', 'changed_at': '2023-10-07T09:03:25.133911Z', 'content': '<p>我有魔法✨为你劈开信息大海❗</p>\\n<p>高效获取AIGC的热门事件🔥，更新AIGC的最新动态，生成相应的魔法简报，节省阅读时间👻</p>\\n\\n<h3>🔥资讯预览</h3>\\n<ul>\\n<li>小鹏G6智能驾驶技术大获成功，成为自动驾驶行业翘楚</li>\\n<li>百度发布国内首个“产业级”医疗大模型，灵医大模型技术评分第一</li>\\n<li>中国新富人群财富信心持续增长，月收入高的群体更满意</li>\\n<li>抖音与美团展开攻防战，谁能在竞争中占据主导地位？</li>\\n<li>Adobe Firefly利用GAN技术帮助用户轻松进行图像编辑和创作</li>\\n<li>微软宣布：Windows 11 23H2大更新将带来超过150项新功能，包括AI助手Copilot和原生RAR支持！</li>\\n<li>人工智能技术突破，实现与鲸鱼对话，拯救海洋生命的新希望</li>\\n<li>埃斯顿酷卓发布通用型协作机器人整机产品，共创人机共融美好未来</li>\\n<li>金山办公推出智能办公助手WPS AI，实现文档创作、PPT自动生成等多种功能的一键满足</li>\\n<li>上海人工智能实验室与商汤科技联合推出了超强大的中文大模型InternLM-20B</li>\\n</ul>\\n\\n<h3>🪄魔法简报</h3>\\n<h3>小鹏G6智能驾驶技术大获成功，成为自动驾驶行业翘楚</h3>\\n<p>小鹏仅用了一年时间 XNGP就从鸡肋变成了杀招。小鹏G6上市后取得了巨大成功，不仅吸引了更多消费者前往体验，也为小鹏品牌带来了更大的影响力。G6的智驾能力得到用户的高度认可，70%的用户选择了智能驾驶能力更强的Max版本。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d3f022c7cee40b187ae31d90a32abb0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=600&amp;h=375&amp;s=205270&amp;e=png&amp;b=f7f6f6\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>小鹏拥有中国最大的自动驾驶智算中心“扶摇”，通过免费标配Max版本，小鹏能够以较低的成本吸引更多用户，获取更多的数据量。小鹏的技术力在市场竞争中得到广泛认可，被其他车企所羡慕。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5fceae5ac6d94023901e61e1cbf634c1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=986&amp;h=579&amp;s=435536&amp;e=png&amp;b=aac3d4\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>小鹏在数据闭环方面也取得了突破，拥有更多优质有效的数据量，训练速度大幅提升。尽管华为在自动驾驶方面也有一定的积累和能力，但中国的智能辅助驾驶市场仍然广阔，小鹏有望在竞争中继续领先。</p>\\n<h3>百度发布国内首个“产业级”医疗大模型，灵医大模型技术评分第一</h3>\\n<p>百度日前发布了国内首个“产业级”医疗大模型——灵医大模型。该模型已与多家医疗机构达成合作，并定向向公立医院、药械企业、互联网医院平台、连锁药房等200多家机构开放体验。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1bb9d184de434a1db6f7ce25229730c5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=554&amp;h=312&amp;s=313406&amp;e=png&amp;b=0e4388\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>灵医大模型推出了Lite版、旗舰版、定制版三个版本，合作伙伴可通过API方式调用大模型能力，或以插件形式将其嵌入到已有产品系统中。该模型能够结合自由文本生成结构化病历，并支持多篇中英文文献同时解析，实现智能问答等功能。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/57ccb502431e4ca4a44303507b2e8722~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=391&amp;s=201826&amp;e=png&amp;b=091a3b\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>根据IDC发布的报告，《AI大模型技术能力评估报告，2023》，灵医大模型的技术底座表现优秀，评分第一。</p>\\n<h3>中国新富人群财富信心持续增长，月收入高的群体更满意</h3>\\n<p>根据《SAIF：2023年中国新富人群财富健康指数》的调研结果显示，中国新富人群的财富信心持续增长。近60%的受访者表示他们在过去一年里财富信心进一步提升，对自身财务状况的总体满意度也上升至8.06分（满分为10分）。然而，不同收入群体的财富信心存在较大差异，高收入群体对自身财务状况的满意度更高。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3347c05374d7425fabdc16711ad7f177~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=320&amp;s=16297&amp;e=jpg&amp;b=f2e9da\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>月收入在1.5-3万元和3万元以上的受访者的满意度有所提高，而收入在1-1.5万元之间的受访者的满意度则有所降低。新富人群对未来五年财富健康的预期也呈现类似分布，收入越高的群体对未来更为乐观。经济持续增长是新富人群最主要的财富信心来源，但有41.5%的受访者对未来五年的经济增长表示担忧。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/99b839351fbc4ae2a62ae65194d338c9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=366&amp;s=56091&amp;e=jpg&amp;b=e0ddc1\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>另外，个人理财能力被41.8%的受访者视作财富信心的来源，其中45.1%表示自己擅长财务规划。总体而言，中国新富人群对财富的信心较高，但仍需关注收入差异和经济变化对财富信心的影响。</p>\\n<h3>抖音与美团展开攻防战，谁能在竞争中占据主导地位？</h3>\\n<p>抖音和美团之间的竞争愈发激烈，双方正展开一场攻防战。这两个互联网巨头都有着强大的用户基础和资金实力，但面临的挑战也不容小觑。抖音在短视频领域占据主导地位，但美团凭借其强大的外卖业务和本地生活服务平台也有一席之地。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/441de114fd0d43acbd39a74fe3fe8323~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=605&amp;s=49018&amp;e=jpg&amp;b=25221f\" alt=\"图片\" loading=\"lazy\"/>双方都在不断扩大自己的业务范围，争夺更多用户和市场份额。这场竞争对于用户而言，可能意味着更多的选择和更好的服务质量。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/78c9802ea1ae42febbac11af7177db10~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=565&amp;s=781947&amp;e=png&amp;b=f2c739\" alt=\"图片\" loading=\"lazy\"/>然而，对于抖音和美团来说，胜利并非易事，需要不断创新和优化自己的业务模式，以应对竞争对手的挑战。谁能在这场攻防战中占据主导地位，将取决于双方的战略决策和执行能力。</p>\\n<h3>Adobe Firefly利用GAN技术帮助用户轻松进行图像编辑和创作</h3>\\n<p>Adobe Firefly是一种生成式AI工具，利用生成式对抗网络（GAN）技术，可以生成新的图像内容，包括合成、修复和风格迁移等任务。它可以帮助用户在Photoshop等应用程序中进行图像编辑，如抠图修补、元素生成、基于单图的扩展补充和基于多图的融合补充等。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d7012047142045c8929866442e5e458d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=681&amp;s=626590&amp;e=png&amp;b=adc9de\" alt=\"图片\" loading=\"lazy\"/>用户可以通过订阅Adobe的Creative Cloud套餐或使用网页版的Adobe Firefly来体验这些功能。AI功能在使用过程中需要良好的网络环境，并支持多种语言指令。尽管生成的图像效果可能不完美，但对于非专业设计师来说，它是一个高效、方便的创作工具。</p>\\n<h3>微软宣布：Windows 11 23H2大更新将带来超过150项新功能，包括AI助手Copilot和原生RAR支持！</h3>\\n<p>微软在秋季发布会上宣布，将于9月26日发布Windows 11的下一个大更新23H2。这次系统更新将会有超过150项新功能，其中包括人工智能驱动的Windows Copilot功能、重新设计的文件资源管理器、针对手写笔用户的新Ink Anywhere功能、对画图应用的重大改进、原生RAR和7-zip文件支持、新的音量混合器等。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bb8c1b21176b49eca783514a496aff84~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=600&amp;h=337&amp;s=10023&amp;e=jpg&amp;b=c0cdec\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>Copilot将集成在整个Windows操作系统中，用户可以通过Copilot控制电脑设置、启动应用程序或提问问题，还能使系统自带软件得到AI加持。此次更新还添加了原生RAR和7-zip支持，无需下载第三方软件即可打开tar、7-zip、rar、gz等文件。另外，微软还计划在明年能够支持直接创建这些文件。更新后的文件资源管理器界面也得到了优化，包括现代主页界面和轮播界面，更加适配Windows 11的整体设计风格。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4388aac4a66c43b1886660b68a0e5012~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=611&amp;h=387&amp;s=95978&amp;e=png&amp;b=bcd3e4\" alt=\"图片\" loading=\"lazy\"/>总的来说，Windows 11 23H2的更新版本带来了许多新功能，用户可以期待在9月26日体验这些改进。</p>\\n<h3>人工智能技术突破，实现与鲸鱼对话，拯救海洋生命的新希望</h3>\\n<p>人工智能技术的不断发展使得与动物进行对话成为可能，这对于拯救地球上的生命来说具有重要意义。鲸鱼作为海洋生态系统的重要组成部分，其保护显得尤为重要，目前，研究人员利用人工智能技术，通过声音识别和模式分析等手段，实现了与鲸鱼进行对话的突破。通过人工智能技术，研究人员可以对鲸鱼的声音进行识别和解读，从而理解它们的需求和情感。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5c5f91225cbf4de98bc8a790c00217b9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=812&amp;s=147586&amp;e=jpg&amp;b=d1d641\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>这项技术的应用使得科学家能够更好地了解鲸鱼的行为习惯、繁殖情况以及受到的威胁，为制定相应的保护策略提供了重要依据。与鲸鱼进行对话不仅有助于科学研究，也对于保护海洋生态系统具有积极影响。通过与鲸鱼的交流，科学家可以及时发现鲸鱼所面临的威胁和危险，从而采取相应的救援行动，保护它们的生命安全。然而，人工智能与鲸鱼对话技术还存在一些挑战。鲸鱼的声音非常复杂多变，识别和解读这些声音需要更加高效和准确的算法。</p>\\n<p>\\xa0总之，人工智能技术为与鲸鱼对话提供了新的可能性，这对于拯救地球的生命具有重要意义。通过与鲸鱼的交流，科学家能够更好地了解它们的需求和情感，制定相应的保护策略，保护鲸鱼的生命安全，从而推动海洋生态系统的可持续发展。</p>\\n<h3>埃斯顿酷卓发布通用型协作机器人整机产品，共创人机共融美好未来</h3>\\n<p>埃斯顿酷卓在2023年9月19日举办的发布会上推出了具身智能机器人平台，并发布了通用型协作机器人整机产品。该平台名为CoDroid EIP，专注于基于手臂部分的机器人应用。埃斯顿酷卓的创始人和总经理表示，机器人形态并非关键，关键是提升机器人的智能化程度。埃斯顿酷卓在感知、认知、推理、决策和执行等关键要素方面进行了工作，实现了机器人的智能化。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7b3e206e1cc84fc5a72d1f82767ed411~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=338&amp;s=34896&amp;e=jpg&amp;b=0cabab\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>此外，埃斯顿酷卓还开发了具备刚柔并济特点的三个仿生类关节模组，以及面向灵巧操作的通用形态产品。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6a79e15faa0b4c66b4841d1643853121~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=720&amp;s=84000&amp;e=jpg&amp;b=272d31\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>埃斯顿酷卓将“共创人机共融美好未来”定为公司使命，并与微软人工智能和物联网实验室合作，共同打造具身智能创新应用场景。未来，埃斯顿酷卓希望通过标准化、模块化和大规模量产推动通用智能机器人产业革命，加速推动行业的发展。</p>\\n<h3>金山办公推出智能办公助手WPS AI，实现文档创作、PPT自动生成等多种功能的一键满足</h3>\\n<p>WPS AI是金山办公推出的一款智能办公助手，通过使用大模型技术，将各种复杂的WPS软件功能整合在一起。该助手可以帮助用户进行文档创作、自动生成PPT、阅读资料、解答问题等多种工作。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/edc3686fb875420da3e96c89d8abdbf5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=604&amp;s=64858&amp;e=jpg&amp;b=fffefe\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>用户只需用自然语言描述需求，助手就能快速连接到正确的功能上，并提供满足用户需求的结果。WPS AI的优势在于将大模型的能力应用于办公软件，为用户提供更高效、便捷的办公体验。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6a84a7755eb1403ba7cb70b156c52f5c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=557&amp;s=69155&amp;e=jpg&amp;b=fdfcfc\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>金山办公选择了与多家业内领先的大模型合作，将它们的能力统一管理，以满足用户的各种需求。通过与大模型的结合，WPS AI能够帮助用户节省时间和精力，提升工作效率，让用户能够更专注于创造性的工作。随着WPS AI的全组件功能正式上线，它将为用户带来更多变革。</p>\\n<h3>上海人工智能实验室与商汤科技联合推出了超强大的中文大模型InternLM-20B</h3>\\n<p>上海人工智能实验室与商汤科技联合香港中文大学和复旦大学推出了书生・浦语大模型（InternLM）200亿参数版本InternLM-20B，并在阿里云魔搭社区开源首发。该模型具备强大的综合能力，在复杂推理和反思能力上尤为突出，能在单卡上进行推理，因而在实际应用中更为便捷。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c5b9e372249849eb979fd38b30aa0a06~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=536&amp;s=215261&amp;e=png&amp;b=062751\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>InternLM-20B是基于2.3T token预训练语料从头训练的中量级语言大模型，训练语料经过了更高水平的多层次清洗和补充，因此在理解能力、推理能力、数学能力和编程能力等方面有显著提升。InternLM-20B在多个评测中领先于相近量级的开源模型，平均成绩明显超越Llama-33B和Llama2-70B。该模型还具备强大的工具调用能力，支持数十类插件和上万个API功能，并在评测中取得了最佳结果。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe8bd9632ff945b5a8cf731bb789bbb8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=304&amp;s=163180&amp;e=png&amp;b=edf1fd\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>此外，InternLM-20B还在价值对齐和对话能力方面有所增强，能识别出用户的偏见并给出正确的价值引导。上海人工智能实验室还开源了覆盖数据、预训练、微调、部署和评测的全链条工具体系，共同为学术界和产业界提供全链条的研发与应用支持。</p>\\n\\n<p>如果对AIGC感兴趣，请关注我们的微信公众号“<strong>我有魔法WYMF</strong>”，我们会<strong>定期分享AIGC最新资讯和经典论文精读分享</strong>，让我们一起交流学习！！</p>', 'author': '我有魔法WYMF', 'reading_time': 9, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 53, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '2046ba4c4057339b6435c1258b71ea29cc20778ffa22f4e0b0baf66892948a5b', 'title': 'Embedding技术与应用 (2) ：神经网络的发展及现代Embedding方法简介', 'url': 'https://juejin.cn/post/7286642599325810751', 'published_at': '2023-10-07T02:38:38Z', 'created_at': '2023-10-07T04:03:22.88328Z', 'changed_at': '2023-10-07T04:03:22.88328Z', 'content': '<blockquote>\\n<p>编者按：IDP开启Embedding系列专栏，详细介绍Embedding的发展史、主要技术和应用。</p>\\n<p><strong>本文是《Embedding技术与应用系列》的第二篇，重点介绍神经网络的发展历程及其技术架构，剖析了嵌入技术与这些神经网络（Transformer、BERT和GPT等）的关系。</strong></p>\\n<p>正如OpenAI去年年底推出的ChatGPT在对话领域的重要地位，嵌入技术正在成为人工智能进步的重要基石。本文作者认为,嵌入技术与生成式方法以及基于人类反馈的强化学习相结合，将支持人工智能在自然语言理解和内容生成方面取得更大突破。</p>\\n<p>通过介绍神经网络技术发展历程中的关键点,作者全面梳理了嵌入技术同神经网络共同演变的脉络,为读者呈现了嵌入技术在神经网络发展过程中所起到的作用，值得我们深入学习和思考。相信随着相关技术的不断进步，嵌入技术必将在推动AI语言能力的持续进步。</p>\\n<p>以下是译文，enjoy！</p>\\n</blockquote>\\n<p><strong>🚢🚢🚢欢迎小伙伴们加入<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23\" title=\"https://baihai-idp.yuque.com/mwvla8/doc?#\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">AI技术软件及技术交流群</a>，追踪前沿热点，共探技术难题~</strong></p>\\n<p><strong>作者 | Vicki Boykis</strong></p>\\n<p><strong>编译\\xa0|\\xa0岳扬</strong></p>\\n<p>Word2Vec是最早采用嵌入（embedding）概念生成固定特征词汇（fixed feature vocabular）的神经网络架构之一。然而，神经网络在自然语言建模（natural language modeling）方面变得愈加流行，这主要归功于几个关键要素。</p>\\n<p><strong>首先，在20世纪80年代，研究人员在使用反向传播算法（backpropagation）进行神经网络训练方面取得了一定的进展。</strong> 反向传播算法是一种通过计算损失函数（loss function）相对于神经网络权重的梯度，利用微积分中的链式法则（the chain rule）来学习收敛的方法[1]。这种机制使模型能够理解何时达到loss值的全局最小值（global minimum），并通过梯度下降（gradient descent）为模型参数找到正确的权重（correct weights），从而训练模型。早期的方法中（如感知机学习规则（perceptron learning rule）），也尝试过这样做，但有一定的限制，如仅适用于简单的模型架构、收敛时间长以及遭遇梯度消失（vanishing gradients），这使得有效地更新模型的权重十分困难。</p>\\n<p>这些进步推动了第一类多层神经网络-前馈神经网络（feed-forward neural networks）的出现。<strong>1998年，一篇论文使用反向传播算法在多层感知机（multilayer perceptrons）上成功完成了识别手写数字图像的任务</strong>[2]，展示了开发者和研究人员真正可以应用的实际用例。使用这个MNIST数据集现在是深度学习中经典的“Hello World”入门案例之一。</p>\\n<p>其次，<strong>在21世纪初，出现的PB级数据增长使我们能够从互联网上采集到大量多模态输入数据，进而出现了一些大型数据集。</strong> 这导致研究人员能够进行大量实验来证明神经网络在大量级数据上仍然能够正常工作。例如，斯坦福大学的研究人员推出了ImageNet，他们想创建一个神经网络输入数据的黄金标准数据集来提高模型性能。FeiFei Li组织了一支由学生以及Amazon Turk的众包工作者组成的团队，标记了从互联网上抓取的320万张图像，并根据WordNet（一个20世纪70年代出现的分类学）将其按类别进行组织和分类[3]。</p>\\n<p>研究人员看到了标准数据集的优势。\\xa0<strong>2015年，Alex Krizhevsky与现在在OpenAI工作的Ilya Sutskever合作，向ImageNet竞赛提交了一份名为AlexNet的作品。它是一个性能优于很多其他方法的卷积神经网络。</strong></p>\\n<p>AlexNet有两个特别之处。第一处是它有八层堆叠的权重层和偏置层（eight stacked layers of weights and biases），这在当时是极不寻常的。如今，12层的神经网络（如BERT和其他transformers）都是完全正常的，但在当时，神经网络超过两层就是变革性的。第二处是AlexNet在GPU上运行，这在当时是一个新的架构概念，因为当时GPU主要用于游戏的图形渲染。另外，Ilya Sutskever现在是GPT系列模型背后的主要研究人员之一，GPT系列模型是当前生成式人工智能浪潮的基础。</p>\\n<p><strong>神经网络结构（如循环神经网络（RNN）和后来的长短期记忆网络（LSTM））作为生成词汇表征（representations of vocabularies）的方法开始流行起来。</strong> 这些神经网络结构也成为了处理文本数据的方法，用于自然语言处理、计算机视觉等各种机器学习任务。神经网络是对传统机器学习模型的扩展，但它们具有一些独特的特征。在传统的机器学习方法中，只有一组或一层可学习的参数以及一个模型。如果输入的数据没有复杂的相互作用（interactions），传统机器学习模型可以相当容易地学习特征空间（feature space）并进行准确的预测。</p>\\n<p>然而，传统机器学习模型带来的问题是，当我们开始处理非常大的、潜在的（implicit）特征空间（比如存在于文本、音频或视频中），我们会无法推导出具体的特征。如果我们人工创建这些特征，这些特征将会不明显。通过将神经元堆叠在一起（译者注：这是构建神经网络的一种方法，通过堆叠神经元，多个层可以相互连接，从而构建出具有更强表达能力的、更复杂的深度神经网络模型），每个神经元代表模型的某一个方面，神经网络可以提取这些隐层表征（latent representations，译者注：在机器学习和深度学习中，数据通常会被转换为一个向量或者矩阵的形式。这个转换过程叫做“表征学习”。其中，将输入数据转换为一组隐式的表征，又被称为空间中的“隐式表征”或“隐式变量”）。神经网络非常善于学习数据的表征（representation），神经网络的每一层都会将前一层学到的表示转换成更高层次的表示，直到我们获得清晰的数据图像[4]。</p>\\n<h1>01 <strong>深度学习架构</strong></h1>\\n<p>\\xa0 \\xa0 \\xa0 我们已经前面已经提到了神经网络Word2Vec，这个神经网络试图去理解文本中单词之间的关系，而这些单词本身无法告知我们。在这个领域，有以下几种主流的神经网络架构：</p>\\n<ul>\\n<li><strong>前馈网络：</strong> 从固定长度的输入中提取含义。其输出结果不会反馈给模型进行迭代。</li>\\n<li><strong>卷积神经网络(CNNs)：</strong> 其主要用于图像处理，拥有一个由滤波器组成的卷积层，滤波器在图像上不断移动来检查特征表示（feature representations），然后通过进行点积与滤波器相乘来输出特定的特征。</li>\\n<li><strong>循环神经网络(RNNs)：</strong> 它们接受一连串的词语项（items），并产生一个概括该句子的向量。</li>\\n</ul>\\n<p>\\xa0 \\xa0 \\xa0 RNN和CNN主要用于特征提取（feature extraction），一般不代表整个模型构建流程，但在这之后将被送入前馈模型中，然后再完成分类、摘要等工作。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a783c3e4cd6a42eb919e16902deda42c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=828&amp;h=713&amp;s=32781&amp;e=png\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>图1 神经网络的种类</p>\\n<p>\\xa0 \\xa0 \\xa0 由于某些原因，神经网络的构建和管理都变得非常复杂。首先，这个过程需要大量的干净、标注清晰的数据来进行优化。其次，这个流程需要特殊GPU架构来进行处理，并且它们有自己的元数据管理（metadata management）和网络延迟注意事项（latency considerations）。最后，在神经网络本身，我们需要使用大量的训练数据对模型对象（the model object）进行处理，以使其收敛。我们需要用于运算的特征矩阵数量以及必须在模型生命周期内保留于内存中的数据量一直在不断累积，这就需要大量的性能调优（performance tuning）。</p>\\n<p>\\xa0 \\xa0 \\xa0 <strong>这些特性使得开发和运行神经网络的成本高得令人望而却步，直到最近的15年左右才实现突破。</strong> 首先，随着实体计算服务和云计算服务的规模不断扩大，市场上的存储空间呈指数级增长，这意味着我们现在有能力存储大量训练数据，而互联网数据的爆炸也为谷歌等公司提供了大量的训练数据。其次，GPU的使用也得到了普及，GPU是一种支持并行计算的硬件——适用于容易将任务分离成可以并行执行的任务，例如单词数量计算。在神经网络中，我们通常可以以任何给定的并行数量的方式并行计算，包括在单个神经元的层面上。</p>\\n<p>\\xa0 \\xa0 <strong>尽管GPU最初是用于处理计算机图形的，但在21世纪初</strong> <strong>[5]</strong> <strong>，研究人员发现了将其用于通用计算的潜力，Nvidia通过引入CUDA（即GPU之上的API层）入场“押宝”GPU用于通用计算。反过来又推动了主流高级深度学习框架（如PyTorch和Tensorflow）的出现和发展</strong> <strong>。</strong></p>\\n<p>\\xa0 \\xa0 \\xa0 现在神经网络可以被大规模地训练和实验。与我们之前介绍的方法相比，当我们计算TF-IDF时，我们需要循环遍历每个单词，在整个数据集上按顺序执行计算，以达到与其他单词成比例的分数，其时间复杂度将为O(ND)\\xa0[6]。</p>\\n<p>\\xa0 \\xa0 \\xa0 然而，使用神经网络，我们可以将模型训练分布在不同的GPU上，这个过程被称为模型并行（model parallelism），或者计算batches——将训练数据的大小送入模型中，在训练循环中与其他数据一起并行更新超参数（hyperparameters），并在每个minibatch结束时进行更新，这被称为数据并行\\xa0[7]。</p>\\n<h1>02 <strong>Transformers</strong></h1>\\n<p>\\xa0 \\xa0 \\xa0 Word2Vec是一种前馈网络（feed-forward network），其模型权重和信息仅从编码层（the encoding state）流向隐藏嵌入层（译者注：the hidden embedding layer，是Word2Vec模型中的一个隐藏层，它的作用是将输入的单词转换为一个向量表征，也称为嵌入向量。），再流向输出概率层（译者注：the output probability layer，是Word2Vec模型中的最后一层，它的作用是将隐藏层的嵌入向量映射为每个单词的概率分布。）。第二层和第三层之间没有反馈，这意味着每一层都对其后各层的状态一无所知。因此，它无法提供比上下文窗口大小更长的推理建议。但是，对于那些只需要使用单一、静态词汇的机器学习问题，这种模型非常有效。</p>\\n<p>\\xa0 \\xa0 \\xa0 然而，<strong>对于需要结合语境理解单词的长篇文章，Word2Vec就表现不佳了。</strong> 举个例子，假如在一次对话中，我们说了这样一句话：“我读了Langston Hughes的那句话，我喜欢它，但并没有真正读过他后来的作品。”我们可以理解，“它”指的是那句话，需要结合前一句的语境，而“他”指的是两句前提到的“Langston Hughes”。</p>\\n<p>\\xa0 \\xa0 \\xa0 另外，<strong>Word2Vec无法处理词汇表之外的单词，即模型尚未训练过的，需要泛化的单词。</strong> 这意味着，如果用户搜索一个新产生的热门词汇，或者我们想要推荐一个在模型训练之后才产生的内容，用户将无法从该模型的结果中看到任何相关内容[8]，除非经常重新训练模型。</p>\\n<p>\\xa0 \\xa0\\xa0 <strong>Word2Vec 面临的另一个问题是多义词导致的语境崩溃</strong>\\xa0—— 同一词语可能含有多种不同的含义：例如，如果在同一句子中有 “jail cell” 和 “cell phone” ，Word2Vec可能无法区分这两个词在不同语境下的不同含义。因此，基于深度学习的 NLP 的大部分工作都是为了保留和理解上下文，以便通过模型传播并提取语义。</p>\\n<p>\\xa0 \\xa0 \\xa0\\xa0<strong>为了克服Word2Vec的这些限制条件，人们提出了多种不同的方法，有研究人员尝试使用循环神经网络（RNN）。</strong> RNN基于传统的前馈网络，不同之处在于模型的各层会向前层提供反馈。这使得模型可以记住句子中单词周围的语境，从而更好地理解多义词在此处的含义。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0f969466f81c4a6489edebaaef14bf09~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=587&amp;h=392&amp;s=28682&amp;e=png&amp;b=fdfdfd\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>图2 前馈神经网络和循环神经网络架构图</p>\\n<p>\\xa0 \\xa0\\xa0 <strong>传统的RNN存在这样一种问题，在反向传播过程中，权重必须传递到上一层神经元，因此会出现梯度消失的问题。</strong> 当我们连续求导，使得反向传播过程中链式法则（chain rule）中使用的偏导数趋近于零时，就会出现这种情况。一旦趋近于零，神经网络就会认为它已经达到了局部最优，并在收敛前停止训练。</p>\\n<p>\\xa0 \\xa0 <strong>为了解决这个问题，出现了一种非常流行的RNN变体，即长短期记忆网络（LSTM）</strong> ，该网络最初由Schmidhuber开发，后来在语音识别（text applications speech recognition）和图像描述（image captioning）[9]等领域得到广泛应用。相比之前的模型每次只能输入一个向量，RNN可以使用门控循环单元（GRU）处理向量序列，这使得 LSTM 可以控制输入分析的信息量。虽然 LSTM 的效果相当不错，但它们也有一定的局限性。由于其架构比较复杂，无法进行并行训练，因此训练时间更长，计算成本更高。</p>\\n<h3><strong>2.1</strong> <strong>Encoders/Decoders and Attention 编码器/解码器架构和注意力机制</strong></h3>\\n<p>\\xa0 \\xa0 \\xa0 与之前的 RNN 和 Word2Vec 相比，有两个概念使研究人员能够克服在更大的上下文窗口中记忆长向量所带来的计算昂贵问题：编码器/解码器架构和注意力机制。</p>\\n<p>\\xa0 \\xa0 \\xa0 编码器/解码器架构是一种由两个神经网络组成的神经网络架构，编码器从数据中获取输入向量（input vectors）并创建固定长度的嵌入，另一个解码器（也是一个神经网络）将编码后的嵌入（embeddings encoded）作为输入，并生成一组静态输出，如文本翻译或进行文本摘要。在这两类层之间的是注意力机制，这是一种通过不断执行加权矩阵乘法来保持整个输入状态的方法，这种乘法可以突出词汇表中特定术语之间的相关性。我们可以将注意力（attention）看作一个非常庞大且复杂的哈希表，它可以跟踪文本中的单词，以及这些单词如何映射到输入和输出中的不同表述。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e12ec34afda84228ae27fd28554a0591~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=658&amp;h=267&amp;s=25729&amp;e=png&amp;b=ffffff\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>图3 编码器/解码器架构图</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1ff15a9ec8f7469b8f6f559b67772c3c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=681&amp;h=804&amp;s=128263&amp;e=png&amp;b=ffffff\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>图4 典型的编码器/解码器结构，来自《the Annotated Transformer》[10]</p>\\n<p>\\xa0 \\xa0 \\xa0 2017年发布的《Attention is All You Need》[11]将这两个概念结合到了一个单一的神经网络架构中。这篇论文立刻获得了巨大成功，如今Transformer已成为自然语言处理任务中使用的标准模型之一。</p>\\n<p>\\xa0 \\xa0 \\xa0 <strong>基于原始模型的成功，Transformer 架构的大量变体相继发布。2018年发布了 GPT 和 BERT ，2019 年发布了 BERT 的更小更紧凑版本 Distilbert，2020 年发布了 GPT-3。</strong></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dd9b44f02d254cd9b6a9aba41ea807b9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=442&amp;h=305&amp;s=20393&amp;e=png&amp;b=ffffff\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>图5 Transformer模型大事记</p>\\n<p>\\xa0 \\xa0 \\xa0 Transformer 架构本身并不新鲜，但其包含了我们到目前为止讨论过的所有概念：向量（vectors）、编码（encodings）和哈希映射（hash maps）。</p>\\n<p>\\xa0 \\xa0 \\xa0 Transformer 模型的目标是获取一段多模态内容，并通过在输入语料中创建单词组的多个视图（即多个上下文窗口）来学习其中的潜在关系。在Transformer论文中，自注意力机制是以缩放点积注意力（scaled dot-product attention）的形式实现的，它通过六层编码器层和六层解码器层多次创建数据的不同上下文窗口。输出的是特定机器学习任务的结果，例如翻译句子或总结段落，而倒数第二层是模型的嵌入，我们可以将其用于下游任务。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/08b2c80fb21743d680adc3a6b260712e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=646&amp;h=325&amp;s=52664&amp;e=png&amp;b=ffffff\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>图6 基于多个来源（包括这张图[12]）的启发，提供这张Transformer的层视图</p>\\n<p>\\xa0 \\xa0 \\xa0 论文中所描述的Transformer模型以文本语料库作为输入。我们首先通过分词（tokenizing）将文本转换为token嵌入，并将每一个单词或子单词映射为一个索引。这与 Word2Vec 的流程相同：我们只需将每个单词分配给矩阵中的一个元素即可。然而，仅凭这些并不能帮助我们理解上下文，因此，在此基础上，我们还要借助正弦函数或余弦函数来学习位置嵌入（positional embeddings），将词汇表中其他所有单词的位置映射并压缩到矩阵中。这一过程的最终输出是位置向量（positional vector）或单词编码（word encoding）。</p>\\n<p>\\xa0 \\xa0 \\xa0 接下来，这些位置向量将并行传递给模型。在Transformer论文中，模型由六层编码层和六层解码层组成。从编码器层开始介绍，该层由两个子层组成：自注意力层和前馈神经网络。自注意力层是关键部分，它通过缩放点积注意力（scaled dot-product attention）来学习每个术语与其他术语之间的关系。我们可以将自注意力看作是一个可微分的查找表（as a differentiable lookup table），或者是一个包含术语及其位置的大型查找字典（as a large lookup dictionary that contains both the terms and their positions），每个术语与其他术语之间的关系权重都是从前几层获得的。</p>\\n<p>\\xa0 \\xa0 \\xa0 缩放点积注意力涉及三个矩阵的计算：键矩阵（key）、查询矩阵（query）和值矩阵（value）。这些矩阵最初都是前几层输出的相同值。在模型的第一次学习时，它们被随机初始化并通过梯度下降在每一步都进行调整。对于每个嵌入（embedding），我们都会根据这些学习到的注意力权重生成一个加权平均值（weighted average value）。我们计算查询矩阵（query）和键矩阵（key）之间的点积，最后通过 softmax 对权重进行归一化处理。多头注意力机制（Multi-head attention）指的是并行执行多次缩放点积注意力的计算，并将结果串联成一个向量。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c82d1e95f61144439ea79bbed0b74ca4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=353&amp;h=74&amp;s=9415&amp;e=png&amp;b=fffefe\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>\\xa0 \\xa0 \\xa0\\xa0<strong>缩放点积注意力（以及编码器的所有层）的优点在于，它可以并行处理语料库中的所有tokens，不需要像 Word2Vec 那样等待一个token处理完成才能处理下一个token，因此无论语料词汇表有多大，输入步骤的数量都是一样的。</strong></p>\\n<p>\\xa0 \\xa0 \\xa0 解码器部分与编码器略有不同。首先便是输入数据集不同：在Transformer论文中，它（译者注：此处应当指解码器的输入数据集）使用是目标语言的数据集。例如，如果我们想要将英文翻译成意大利语，就需要在意大利语语料库上进行训练。否则，我们执行的所有操作都相同：创建索引嵌入（indexed embeddings），然后将其转换为位置嵌入（positional embeddings）。再然后，我们将目标文本的位置嵌入送入到一个包含三个部分的层中：带有掩码的多头注意力机制、多头注意力机制和前馈神经网络。带掩码的多头自注意力机制与编码器中的自注意力机制基本相同，不过多了一部分：由于解码器的输入词汇是我们需要的 &#34;答案&#34;，即翻译后的文本，因此在这一步中引入的掩码矩阵（mask matrix）起到了过滤器的作用，可以防止注意力头（attention head）查看未来的tokens。</p>\\n<p>\\xa0 \\xa0 \\xa0 来自带有掩码的多头注意力层（the masked multi-head self attention layer）的输出被传递到编码器-解码器-注意力部分（encoder-decoder attention portion），该部分接受来自最初六个编码器层的最终输入作为键矩阵（key）和值矩阵（value），并将前一个解码器层的输入作为查询矩阵（query），然后执行缩放点积（scaled dot-product）。最后，将每个输出馈送到前馈层（feed forward layer），生成最终的嵌入。</p>\\n<p>\\xa0 \\xa0 \\xa0 获得每个token的隐藏层状态（hidden state）后，就可以添加任务头（task head）了。对于我们正在处理的任务，即翻译任务，任务头的作用是预测下一个单词应该是什么。在这一过程的每个步骤中，解码器都会查看之前的步骤，并根据这些步骤生成一个完整的句子[13]。最后，我们就能得到预测的单词，就像在Word2Vec中一样。</p>\\n<p>\\xa0 \\xa0 \\xa0 Transformer之所以具有革命性意义，原因有很多，因为其解决了人们一直在努力解决的几个问题：</p>\\n<ul>\\n<li><strong>并行化（Parallelization）</strong> \\xa0- 模型中的每一步都可以并行处理，这意味着我们不需要等待知道一个单词的位置嵌入（positional embedding）后才能处理另一个单词。因为每个嵌入查找矩阵（embedding lookup matrix）都将注意力集中在一个特定的单词上，并带有一个与该单词相关的所有其他单词的查找表，每个单词的每个矩阵都包含整个输入文本的上下文窗口。</li>\\n<li><strong>梯度消失（Vanishing gradients）</strong> \\xa0- 以往的模型（如RNN）可能会出现梯度消失或梯度爆炸的问题，这意味着模型在完全训练好之前就达到了局部最小值，从而使得捕捉长期依赖问题（long-term dependencies）变得具有挑战性。Transformer允许序列中任意两个位置之间的直接连接，从而缓解了这一问题，使信息在前向和反向传播过程中都能更有效地流动。</li>\\n<li><strong>自注意力机制</strong>\\xa0- 注意力机制允许我们学习整个文本的上下文，而不仅仅是一个2或3个单词的滑动上下文窗口。这使我们能够学习不同上下文中的不同单词，并更准确地预测答案。</li>\\n</ul>\\n<h1>03 <strong>BERT</strong></h1>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b1133a020b44495b9b3130071a7581aa~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=380&amp;s=78218&amp;e=png&amp;b=ffffff\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>图7 Encoder-only架构</p>\\n<p>\\xa0 \\xa0 \\xa0 在《Attention is All you Need》获得巨大成功之后，各种各样的Transformer架构应运而生，这种架构的研究和实现在深度学习领域呈现爆炸式增长。下一个被认为是重大进步的Transformer架构是Google于2018年发布的BERT。BERT 是 Bi-Directional Encoder 的缩写，发布于 2018 年[14]，其基于Google的一篇论文，是解决常见的自然语言处理任务（如情感分析、问答和文本摘要）的一种方法。BERT是一种Transformer模型，也基于注意力机制，但其架构仅包括编码器部分。BERT最突出的应用是在谷歌搜索中，使得Google搜索能够更准确地理解用户的查询意图，从而提供更有用的搜索结果。<strong>在2019年Google发布的一篇博客文章中，强调了已将BERT添加到搜索排名算法中，Google特别讨论了将语境（亦常译作“上下文”）添加到查询中以替代基于关键字的方法的原因。</strong></p>\\n<p>\\xa0 \\xa0 \\xa0 BERT是一种掩码语言模型（masked language model）。我们在实现 Word2Vec 时，就是通过删除单词和建立上下文窗口来实现掩码的。当我们使用Word2Vec创建表示（representations）时，我们只关注向前移动的滑动窗口。BERT中的“B”代表双向，这说明它通过缩放点积注意力（scaled dot-product attention）双向关注单词。BERT有12个Transformer层。它首先使用WordPiece算法将单词分割为子单词，并将其转换为token。训练 BERT 的目的是让它能够根据上下文预测token。</p>\\n<p>\\xa0 \\xa0 \\xa0BERT的输出是单词及其上下文的隐层表示（latent representations） —— 一组嵌入。<strong>BERT本质上是一个大规模并行的Word2Vec，可以记住更长的上下文窗口。</strong> BERT十分灵活性，可以用于多种任务，从语言翻译到文本摘要，再到搜索引擎的自动填充（autocomplete）。<strong>由于它没有解码器组件，因此无法生成文本，这就为 GPT 模型接替 BERT 的工作铺平了道路。</strong></p>\\n<h1>04 <strong>GPT</strong></h1>\\n<p>在BERT开发的同时，OpenAI也在开发另一种Transformer架构——GPT系列。<strong>GPT与BERT的不同之处在于，其不仅能对嵌入文本进行编码，也能对其进行解码，因此可以用于概率推理（probabilistic inference）。</strong></p>\\n<p>\\xa0 \\xa0 \\xa0 最初的GPT模型是基于Book Corpus的数据训练完成的，是一个12层、12头的Transformer，只有解码器部分。后续版本在此基础上不断进行改进，试图提高上下文理解能力。最大的突破出现在GPT-4中，它是通过基于人类反馈的强化学习（reinforcement learning from Human Feedback）进行训练的，这一训练特性使它能够从文本中做出推理，令其结果感觉上更接近人类所写的内容。</p>\\n<p>\\xa0 \\xa0 \\xa0 现在，我们已经到达了使用嵌入技术的最前沿。随着生成式方法（generative methods）和基于人类反馈的强化学习方法（Reinforcement Learning with Human Feedback）的兴起，例如OpenAI的ChatGPT以及新兴的开源模型Llama、Alpaca等，本文中所述的任何内容在发表时都已经过时了。</p>\\n\\n<p><strong>Embedding*技术在生产环境中的应用效果到底如何？下一期，我们将分享Embedding技术的多个应用案例，敬请期待。</strong></p>\\n<p><strong>END</strong></p>\\n<h1><strong>参考资料</strong></h1>\\n<p>[1]David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986.</p>\\n<p>[2]Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998</p>\\n<p>[3]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bern-stein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015.</p>\\n<p>[4]Yann LeCun, Yoshua Bengio, Geoffrey Hinton, et al. Deep learning. nature, 521 (7553), 436-444. Google Scholar Google Scholar Cross Ref Cross Ref, page 25, 2015.</p>\\n<p>[5]Bogdan Oancea, Tudorel Andrei, and Raluca Mariana Dragoescu. Gpgpu computing. arXiv preprint arXiv:1408.6923, 2014.</p>\\n<p>[6]Yingnan Cong, Yao-ban Chan, and Mark A Ragan. A novel alignment-free method for detection of lateral genetic transfer based on tf-idf. Scientifi reports, 6(1):1–13, 2016.</p>\\n<p>[7]Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E Dahl. Measuring the effects of data parallelism on neural network training. arXiv preprint arXiv:1811.03600, 2018.</p>\\n<p>[8]Giovanni Di Gennaro, Amedeo Buonanno, and Francesco AN Palmieri. Considerations about learning word2vec. The Journal of Supercomputing, pages 1–16, 2021.</p>\\n<p>[9]Andrej Karpathy. The unreasonable effectiveness of recurrent neural networks, May 2015. URL <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fkarpathy.github.io%2F2015%2F05%2F21%2Frnn-effectiveness%2F\" title=\"https://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">karpathy.github.io/2015/05/21/…</a>.</p>\\n<p>[10]<a href=\"https://link.juejin.cn?target=http%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html\" title=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">nlp.seas.harvard.edu/2018/04/03/…</a></p>\\n<p>[11]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>\\n<p>[12]<a href=\"https://link.juejin.cn?target=https%3A%2F%2Flilianweng.github.io%2Fposts%2F2018-06-24-attention%2F\" title=\"https://lilianweng.github.io/posts/2018-06-24-attention/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">lilianweng.github.io/posts/2018-…</a></p>\\n<p>[13]Alexander M Rush. The annotated transformer. In Proceedings of workshop for NLP open source software (NLP-OSS), pages 52–60, 2018.</p>\\n<p>[14]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018</p>\\n<p><strong>本文经原作者授权，由Baihai IDP编译。如需转载译文，请联系获取授权。</strong></p>\\n<p><strong>原文链接</strong>：</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fvickiboykis.com%2Fwhat_are_embeddings%2Findex.html\" title=\"https://vickiboykis.com/what_are_embeddings/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">vickiboykis.com/what_are_em…</a></p>\\n<p><strong>🚢🚢🚢欢迎小伙伴们加入<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23\" title=\"https://baihai-idp.yuque.com/mwvla8/doc?#\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">AI技术软件及技术交流群</a>，追踪前沿热点，共探技术难题~</strong></p>', 'author': 'Baihai_IDP', 'reading_time': 26, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 57, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'b78e796bf4949b3c79ac9c42b4c1468a23a5c81db82f89a34081d93b6dc195a5', 'title': '进化算法中的人工免疫系统算法（Artificial Immune System）', 'url': 'https://juejin.cn/post/7286400576085852194', 'published_at': '2023-10-07T01:22:25Z', 'created_at': '2023-10-07T07:03:23.138439Z', 'changed_at': '2023-10-07T07:03:23.138439Z', 'content': '<h2>引言</h2>\\n<p>人工免疫系统算法（Artificial Immune System，AIS）是一种受到自然免疫系统启发的进化算法。它模拟了生物体免疫系统的特性和行为，通过学习和进化来解决优化和机器学习问题。本文将介绍人工免疫系统算法的基本原理、应用领域以及优势和局限性。</p>\\n<h2>人工免疫系统算法的原理</h2>\\n<p>人工免疫系统算法基于生物体免疫系统的工作原理，其中包括了免疫记忆、免疫选择和免疫进化等关键过程。</p>\\n<h3>1. 免疫记忆</h3>\\n<p>人工免疫系统算法通过构建一个免疫记忆库来存储已经学习到的抗体信息。抗体是一个向量，表示问题的解空间中的一个候选解。通过记忆库中的抗体，算法可以快速检索和匹配已经学习到的知识，从而加速搜索过程。</p>\\n<h3>2. 免疫选择</h3>\\n<p>在免疫选择过程中，算法通过一定的选择策略来选择适应度高的抗体，即具有较好解决问题能力的候选解。这些被选择的抗体将会参与到后续的免疫进化过程中。</p>\\n<h3>3. 免疫进化</h3>\\n<p>免疫进化是人工免疫系统算法的核心部分。在免疫进化过程中，通过模拟生物体免疫系统中抗体的变异和重组操作，生成新的抗体，并与已有的抗体进行交互和竞争。这样可以通过不断的进化和学习，提高算法的搜索能力和解决问题的效果。</p>\\n<p>以下是一个简单的示例代码，展示了人工免疫系统算法在解决优化问题（函数优化）中的应用。</p>\\n<pre><code>pythonCopy codeimport numpy as np\\n\\ndef rosenbrock(x):\\n    return sum(100 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\\n\\ndef artificial_immune_system(max_iter, pop_size, num_features, mutation_rate):\\n    \\n    population = np.random.uniform(low=-2, high=2, size=(pop_size, num_features))\\n    fitness = np.zeros(pop_size)\\n    \\n    for iter in range(max_iter):\\n        \\n        for i in range(pop_size):\\n            fitness[i] = rosenbrock(population[i])\\n        \\n        tournament_size = 2\\n        selected_indices = np.zeros(pop_size, dtype=int)\\n        for i in range(pop_size):\\n            tournament_indices = np.random.choice(pop_size, size=tournament_size, replace=False)\\n            selected_indices[i] = tournament_indices[np.argmin(fitness[tournament_indices])]\\n        \\n        for i in range(pop_size):\\n            mutant = population[selected_indices[i]].copy()\\n            for j in range(num_features):\\n                if np.random.rand() &lt; mutation_rate:\\n                    mutant[j] += np.random.uniform(-0.1, 0.1)\\n            \\n            if rosenbrock(mutant) &lt; fitness[selected_indices[i]]:\\n                population[selected_indices[i]] = mutant\\n    \\n    best_index = np.argmin(fitness)\\n    best_solution = population[best_index]\\n    best_fitness = fitness[best_index]\\n    return best_solution, best_fitness\\n\\nmax_iter = 100\\npop_size = 50\\nnum_features = 10\\nmutation_rate = 0.1\\nbest_solution, best_fitness = artificial_immune_system(max_iter, pop_size, num_features, mutation_rate)\\nprint(&#34;最优解：&#34;, best_solution)\\nprint(&#34;最优适应度：&#34;, best_fitness)\\n</code></pre>\\n<p>这段代码使用人工免疫系统算法来求解Rosenbrock函数的最优解。通过迭代优化过程中的选择和免疫进化操作，算法逐渐寻找最优解。最终，输出找到的最优解以及对应的适应度值。 请注意，这只是一个简单的示例代码，实际应用中可能需要根据具体问题进行参数调整和算法改进。同时，人工免疫系统算法还可以应用于其他优化问题、模式识别和数据挖掘等领域，需要根据具体应用场景进行适当的修改和扩展。</p>\\n<h2>人工免疫系统算法的应用领域</h2>\\n<p>人工免疫系统算法在许多领域都得到了广泛的应用，包括优化问题、模式识别、数据挖掘等。</p>\\n<h3>1. 优化问题</h3>\\n<p>人工免疫系统算法可以用于解决各种优化问题，例如组合优化、函数优化等。通过免疫进化过程，算法可以在解空间中搜索最优解，实现对复杂问题的优化。</p>\\n<h3>2. 模式识别</h3>\\n<p>人工免疫系统算法可以用于模式识别问题，例如图像识别、信号处理等。通过免疫选择和进化过程，算法可以学习并适应不同的模式，提高识别的准确性和鲁棒性。</p>\\n<h3>3. 数据挖掘</h3>\\n<p>人工免疫系统算法可以用于数据挖掘任务，例如聚类、分类等。通过免疫记忆和选择过程，算法可以发现并提取数据中的重要特征和模式，帮助分析和理解数据。</p>\\n<h2>人工免疫系统算法的优势和局限性</h2>\\n<p>人工免疫系统算法具有以下优势：</p>\\n<ul>\\n<li>受到生物体免疫系统的启发，具有一定的生物学意义。</li>\\n<li>可以处理复杂的优化和学习问题，适用于多种应用领域。</li>\\n<li>具有较好的鲁棒性和适应性，能够应对问题的变化和噪声的干扰。 然而，人工免疫系统算法也存在一些局限性：</li>\\n<li>参数的选择较为困难，需要根据具体问题进行调整和优化。</li>\\n<li>算法的收敛性和性能与问题的特性相关，不同问题可能需要不同的算法设计和改进。</li>\\n<li>在处理大规模问题时，算法的计算复杂度较高，需要较长的执行时间。</li>\\n</ul>\\n<p>以下是一个简单的示例代码，展示了人工免疫系统算法在信号处理中的应用。</p>\\n<pre><code>pythonCopy codeimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nt = np.linspace(0, 2*np.pi, 1000)\\nsignal = np.sin(2*t) + np.sin(3*t)\\n\\ndef artificial_immune_system(signal, max_iter, pop_size, num_antibodies, mutation_rate):\\n    \\n    antibodies = np.random.uniform(low=-1, high=1, size=(num_antibodies, len(signal)))\\n    \\n    for iter in range(max_iter):\\n        \\n        affinity = np.zeros(num_antibodies)\\n        for i in range(num_antibodies):\\n            affinity[i] = np.sum(np.abs(signal - antibodies[i]))\\n        \\n        tournament_size = 2\\n        selected_indices = np.zeros(pop_size, dtype=int)\\n        for i in range(pop_size):\\n            tournament_indices = np.random.choice(num_antibodies, size=tournament_size, replace=False)\\n            selected_indices[i] = tournament_indices[np.argmin(affinity[tournament_indices])]\\n        \\n        for i in range(pop_size):\\n            mutant = antibodies[selected_indices[i]].copy()\\n            for j in range(len(signal)):\\n                if np.random.rand() &lt; mutation_rate:\\n                    mutant[j] += np.random.uniform(-0.1, 0.1)\\n            \\n            if np.sum(np.abs(signal - mutant)) &lt; affinity[selected_indices[i]]:\\n                antibodies[selected_indices[i]] = mutant\\n    \\n    best_index = np.argmin(affinity)\\n    best_signal = antibodies[best_index]\\n    return best_signal\\n\\nmax_iter = 1000\\npop_size = 50\\nnum_antibodies = 100\\nmutation_rate = 0.1\\nreconstructed_signal = artificial_immune_system(signal, max_iter, pop_size, num_antibodies, mutation_rate)\\n\\nplt.plot(t, signal, label=&#39;Original Signal&#39;)\\nplt.plot(t, reconstructed_signal, label=&#39;Reconstructed Signal&#39;)\\nplt.xlabel(&#39;Time&#39;)\\nplt.ylabel(&#39;Amplitude&#39;)\\nplt.legend()\\nplt.show()\\n</code></pre>\\n<p>这段代码使用人工免疫系统算法来重建信号。首先，生成一个包含两个正弦波的信号数据。然后，通过迭代优化过程中的选择和免疫进化操作，算法逐渐寻找与原始信号相似度最高的抗体，从而实现信号的重建。最终，绘制原始信号和重建信号的对比图。 请注意，这只是一个简单的示例代码，实际应用中可能需要根据具体问题进行参数调整和算法改进。同时，人工免疫系统算法还可以应用于其他信号处理问题，如信号滤波、信号识别和信号分析等领域，需要根据具体应用场景进行适当的修改和扩展。</p>\\n<h2>结论</h2>\\n<p>人工免疫系统算法是一种受到自然免疫系统启发的进化算法，具有广泛的应用前景和研究价值。通过模拟生物体免疫系统的特性和行为，该算法能够解决优化和学习问题，适用于多个领域。然而，人工免疫系统算法仍然需要进一步的研究和改进，以提高算法的性能和实用性。</p>', 'author': '皮牙子抓饭', 'reading_time': 10, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 56, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '4e7ac184efe66c9e6c3d753b831b61ffc59d27630ea3a190b921f1c426382904', 'title': '进化算法中的基因表达式编程（Gene Expression Programming）', 'url': 'https://juejin.cn/post/7286508785103536162', 'published_at': '2023-10-07T01:19:38Z', 'created_at': '2023-10-07T07:03:23.13312Z', 'changed_at': '2023-10-07T07:03:23.13312Z', 'content': '<h2>引言：</h2>\\n<p>进化算法是一类受到自然界进化过程启发的优化算法，能够应用于各种优化问题。其中，基因表达式编程（Gene Expression Programming，简称GEP）是一种基于进化算法的符号回归方法，通过演化生成可以解决复杂问题的数学模型。本文将介绍基因表达式编程的基本原理、操作步骤以及应用领域。</p>\\n<h2>基因表达式编程的基本原理</h2>\\n<p>基因表达式编程是由Cândida Ferreira于1992年提出的一种进化算法。它基于基因表达式的概念，将问题的解表示为一个基因表达式树。每个基因表达式树由一组基因组成，而每个基因则表示树中的一个节点。</p>\\n<h2>基因表达式编程的操作步骤</h2>\\n<p>（1）初始化种群：随机生成一组初始基因表达式树作为种群的初始解。</p>\\n<p>（2）适应度评估：根据问题的特性和目标函数，计算每个基因表达式树的适应度值。</p>\\n<p>（3）选择操作：使用选择算子从种群中选择适应度较高的个体作为父代。</p>\\n<p>（4）交叉操作：通过交叉算子将父代个体的基因进行交叉，生成新的子代个体。</p>\\n<p>（5）变异操作：使用变异算子对子代个体的基因进行变异，引入新的基因，增加种群的多样性。</p>\\n<p>（6）更新种群：将父代和子代个体合并，更新种群。</p>\\n<p>（7）终止条件判断：根据预设的终止条件（如达到最大迭代次数或满足特定的停止准则）判断是否终止算法。</p>\\n<p>（8）返回最优解：返回适应度值最高的个体作为最优解。</p>\\n<p>以下是一个基于基因表达式编程的函数逼近的示例代码：</p>\\n<pre><code>pythonCopy codeimport numpy as np\\n\\ndef evaluate_fitness(expression_tree, data):\\n    X = data[:, :-1]  \\n    y_true = data[:, -1]  \\n    y_pred = expression_tree.evaluate(X)  \\n    error = np.mean((y_pred - y_true) ** 2)  \\n    return 1 / (1 + error)  \\n\\nclass Node:\\n    def __init__(self, value, left=None, right=None):\\n        self.value = value\\n        self.left = left\\n        self.right = right\\n    def evaluate(self, X):\\n        if self.value == &#39;+&#39;:\\n            return self.left.evaluate(X) + self.right.evaluate(X)\\n        elif self.value == &#39;-&#39;:\\n            return self.left.evaluate(X) - self.right.evaluate(X)\\n        elif self.value == &#39;*&#39;:\\n            return self.left.evaluate(X) * self.right.evaluate(X)\\n        elif self.value == &#39;/&#39;:\\n            return self.left.evaluate(X) / self.right.evaluate(X)\\n        else:\\n            return X[:, int(self.value)]\\n\\nclass ExpressionTree:\\n    def __init__(self, max_depth):\\n        self.max_depth = max_depth\\n        self.root = None\\n    def generate(self, depth, terminals, functions):\\n        if depth &lt; self.max_depth:\\n            if np.random.rand() &lt; 0.5 and len(functions) &gt; 0:\\n                value = np.random.choice(functions)\\n                left = self.generate(depth + 1, terminals, functions)\\n                right = self.generate(depth + 1, terminals, functions)\\n                return Node(value, left, right)\\n            else:\\n                value = np.random.choice(terminals)\\n                return Node(value)\\n        else:\\n            value = np.random.choice(terminals)\\n            return Node(value)\\n    def evaluate(self, X):\\n        return self.root.evaluate(X)\\n    def crossover(self, other):\\n        new_tree = ExpressionTree(self.max_depth)\\n        new_tree.root = self._crossover(self.root, other.root)\\n        return new_tree\\n    def _crossover(self, node1, node2):\\n        if np.random.rand() &lt; 0.5:\\n            return node1\\n        else:\\n            new_node = Node(node2.value, node2.left, node2.right)\\n            if node2.left:\\n                new_node.left = self._crossover(node1, node2.left)\\n            if node2.right:\\n                new_node.right = self._crossover(node1, node2.right)\\n            return new_node\\n    def mutation(self, terminals, functions):\\n        self._mutation(self.root, terminals, functions)\\n    def _mutation(self, node, terminals, functions):\\n        if node.left:\\n            self._mutation(node.left, terminals, functions)\\n        if node.right:\\n            self._mutation(node.right, terminals, functions)\\n        if np.random.rand() &lt; 0.1:  \\n            if node.left or node.right:  \\n                node.value = np.random.choice(terminals + functions)\\n            else:  \\n                new_node = self.generate(0, terminals, functions)\\n                node.value = new_node.value\\n                node.left = new_node.left\\n                node.right = new_node.right\\n\\nclass GEP:\\n    def __init__(self, pop_size, max_depth, num_generations, terminals, functions, data):\\n        self.pop_size = pop_size\\n        self.max_depth = max_depth\\n        self.num_generations = num_generations\\n        self.terminals = terminals\\n        self.functions = functions\\n        self.data = data\\n    def run(self):\\n        pop = []\\n        for _ in range(self.pop_size):\\n            tree = ExpressionTree(self.max_depth)\\n            tree.root = tree.generate(0, self.terminals, self.functions)\\n            pop.append(tree)\\n        for gen in range(self.num_generations):\\n            \\n            fitness = [evaluate_fitness(tree, self.data) for tree in pop]\\n            \\n            selected_indices = np.random.choice(range(self.pop_size), size=self.pop_size, p=fitness/np.sum(fitness))\\n            selected_pop = [pop[i] for i in selected_indices]\\n            \\n            new_pop = []\\n            for i in range(0, self.pop_size, 2):\\n                parent1 = selected_pop[i]\\n                parent2 = selected_pop[i+1]\\n                child = parent1.crossover(parent2)\\n                new_pop.append(child)\\n            \\n            for tree in new_pop:\\n                tree.mutation(self.terminals, self.functions)\\n            \\n            pop = new_pop\\n        \\n        best_tree = max(pop, key=lambda tree: evaluate_fitness(tree, self.data))\\n        return best_tree\\n\\ndata = np.array([[1, 3, 4], [2, 5, 7], [3, 7, 10], [4, 9, 13]])\\n\\npop_size = 50\\nmax_depth = 3\\nnum_generations = 100\\nterminals = [&#39;0&#39;, &#39;1&#39;, &#39;2&#39;]\\nfunctions = [&#39;+&#39;, &#39;-&#39;, &#39;*&#39;, &#39;/&#39;]\\n\\ngep = GEP(pop_size, max_depth, num_generations, terminals, functions, data)\\nbest_tree = gep.run()\\n\\nprint(&#34;Best Expression Tree:&#34;)\\nprint(best_tree.root.value)\\n</code></pre>\\n<p>以上示例代码演示了一个基于基因表达式编程的函数逼近问题。代码中使用基因表达式树来表示数学模型，并通过基因表达式编程算法进行演化，最终得到一个适应度较高的最优解。在这个示例中，我们使用一个简单的数据集进行演示，通过基因表达式编程来逼近数据中的数学关系。最后，输出最优解的表达式树的根节点值，即为最优的数学模型。</p>\\n<h2>基因表达式编程的应用领域</h2>\\n<p>基因表达式编程在许多领域都有广泛的应用，以下列举几个典型的应用领域：</p>\\n<p>（1）函数逼近：基因表达式编程可以用于生成数学模型，逼近给定的数据集。它可以自动发现适应于数据集的数学函数表达式，从而实现函数逼近的目标。</p>\\n<p>（2）符号回归：基因表达式编程可以用于发现复杂的符号回归模型。通过演化生成的基因表达式树可以表示复杂的数学关系，如非线性函数、多变量函数等。</p>\\n<p>（3）特征选择：在机器学习和数据挖掘中，特征选择是一个重要的问题。基因表达式编程可以用于自动选择最佳特征子集，从而提高模型的性能和泛化能力。</p>\\n<p>（4）图像处理：基因表达式编程可以应用于图像处理领域，如图像分类、图像降噪等。它可以生成适应于特定图像处理任务的图像处理算法。</p>\\n<p>以下是一个使用Python和OpenCV库进行图像处理的示例代码：</p>\\n<pre><code>pythonCopy codeimport cv2\\n\\nimage = cv2.imread(&#39;image.jpg&#39;)\\n\\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\nblurred = cv2.GaussianBlur(gray, (5, 5), 0)\\n\\nedges = cv2.Canny(blurred, 50, 150)\\n\\ncv2.imshow(&#39;Original Image&#39;, image)\\ncv2.imshow(&#39;Gray Image&#39;, gray)\\ncv2.imshow(&#39;Blurred Image&#39;, blurred)\\ncv2.imshow(&#39;Edges Image&#39;, edges)\\n\\ncv2.waitKey(0)\\n\\ncv2.destroyAllWindows()\\n</code></pre>\\n<p>这段代码首先加载了一张图像，然后将其转换为灰度图，并对灰度图进行高斯模糊处理。接下来，使用Canny边缘检测算法提取图像的边缘。最后，将原始图像、灰度图、模糊图和边缘图显示在窗口中，并等待用户按下任意按键后关闭窗口。</p>\\n<h2>结论：</h2>\\n<p>基因表达式编程是一种基于进化算法的符号回归方法，通过演化生成可以解决复杂问题的数学模型。它具有灵活性和自适应性，适用于各种优化问题。基因表达式编程在函数逼近、符号回归、特征选择和图像处理等领域都有广泛的应用。随着研究的不断深入，基因表达式编程有望在更多领域发挥重要作用，并为问题求解提供更好的解决方案。</p>', 'author': '皮牙子抓饭', 'reading_time': 13, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 55, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'd8001a46e96b393e545d5623c0cd6f90575fc1fac71155f1428ca49f41c84d32', 'title': '进化算法中的人工鱼群算法（Artificial Fish Swarm Algorithm）', 'url': 'https://juejin.cn/post/7286266688194084864', 'published_at': '2023-10-07T01:16:02Z', 'created_at': '2023-10-07T07:03:23.128835Z', 'changed_at': '2023-10-07T07:03:23.128835Z', 'content': '<h3>1. 引言</h3>\\n<p>进化算法是一类模拟自然进化过程的优化算法，通过模拟生物进化过程中的选择、交叉和变异等机制，来优化问题的解。人工鱼群算法（Artificial Fish Swarm Algorithm，简称AFSA）是一种基于鱼群行为模拟的进化算法，它模拟了鱼群觅食和逃避掠食的行为，用于解决优化问题。</p>\\n<h3>2. 算法原理</h3>\\n<p>AFSA算法基于鱼群行为模型，通过模拟鱼群的觅食行为来求解优化问题。算法中的每个个体被称为“鱼”，它们在搜索空间中以某种方式移动，并根据周围环境的变化来调整移动方向和速度。 AFSA算法主要包括以下几个步骤：</p>\\n<ol>\\n<li>初始化鱼群的位置和速度。</li>\\n<li>计算每个鱼的适应度值，适应度值反映了个体在当前位置的优劣程度。</li>\\n<li>根据适应度值，更新鱼的位置和速度。适应度较高的鱼更有可能吸引其他鱼，而适应度较低的鱼则更有可能被其他鱼吸引。</li>\\n<li>根据一定的规则，进行个体间的交互和信息传递。这些规则可以是某种形式的邻域关系，用于决定鱼群中个体之间的互动行为。</li>\\n<li>重复步骤2至4，直到满足停止条件（如达到最大迭代次数或找到满意的解）。</li>\\n</ol>\\n<h3>3. 算法优势</h3>\\n<p>AFSA算法具有以下几个优势：</p>\\n<ul>\\n<li>并行性：AFSA算法中的个体之间可以同时进行移动和互动，因此具有较强的并行性，适用于并行计算环境。</li>\\n<li>自适应性：AFSA算法中的个体根据周围环境的变化来调整移动方向和速度，具有一定的自适应性，能够适应环境的变化。</li>\\n<li>全局搜索能力：AFSA算法通过模拟鱼群觅食行为，具有较强的全局搜索能力，能够在搜索空间中找到较好的解。</li>\\n<li>鲁棒性：AFSA算法对问题的约束条件和搜索空间的限制较少，具有较好的鲁棒性，适用于多种类型的优化问题。</li>\\n</ul>\\n<p>以下是一个简单的示例代码，展示了如何使用遗传算法（Genetic Algorithm）来解决一个物流路径规划问题。</p>\\n<pre><code>pythonCopy codeimport numpy as np\\n\\ndef objective_function(route, distance_matrix):\\n    total_distance = 0\\n    for i in range(len(route) - 1):\\n        total_distance += distance_matrix[route[i]][route[i+1]]\\n    return total_distance\\n\\ndef genetic_algorithm(num_cities, population_size, max_iter, distance_matrix):\\n    \\n    population = np.zeros((population_size, num_cities))\\n    for i in range(population_size):\\n        population[i] = np.random.permutation(range(num_cities))\\n    \\n    for iter in range(max_iter):\\n        \\n        fitness = np.zeros(population_size)\\n        for i in range(population_size):\\n            fitness[i] = 1 / objective_function(population[i], distance_matrix)\\n        \\n        parents = np.random.choice(range(population_size), size=population_size//2, replace=False, p=fitness/np.sum(fitness))\\n        \\n        offspring = np.zeros((population_size, num_cities))\\n        for i in range(0, population_size, 2):\\n            parent1 = population[parents[i]]\\n            parent2 = population[parents[i+1]]\\n            crossover_point = np.random.randint(1, num_cities)\\n            offspring[i] = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\\n            offspring[i+1] = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\\n        \\n        for i in range(population_size):\\n            if np.random.rand() &lt; 0.1:\\n                mutation_point1 = np.random.randint(num_cities)\\n                mutation_point2 = np.random.randint(num_cities)\\n                offspring[i][mutation_point1], offspring[i][mutation_point2] = offspring[i][mutation_point2], offspring[i][mutation_point1]\\n        \\n        population = offspring.copy()\\n    \\n    best_route = population[np.argmax(fitness)]\\n    return best_route\\n\\nif __name__ == &#34;__main__&#34;:\\n    \\n    num_cities = 10\\n    population_size = 50\\n    max_iter = 100\\n    \\n    distance_matrix = np.random.randint(1, 10, size=(num_cities, num_cities))\\n    np.fill_diagonal(distance_matrix, 0)\\n    \\n    best_route = genetic_algorithm(num_cities, population_size, max_iter, distance_matrix)\\n    \\n    print(&#34;Best route:&#34;, best_route)\\n    print(&#34;Total distance:&#34;, objective_function(best_route, distance_matrix))\\n</code></pre>\\n<p>以上代码使用Python语言实现了遗传算法（Genetic Algorithm）来解决一个物流路径规划问题。你可以根据需要修改问题的参数，如城市数量、种群大小、最大迭代次数和距离矩阵等，以适应不同的问题。请注意，此示例代码仅为了展示遗传算法的基本思想和实现方式，具体的应用场景中需要根据问题的特点进行相应的调整和优化。</p>\\n<h3>4. 应用案例</h3>\\n<p>AFSA算法在实际应用中已经取得了一些成功。以下是几个典型的应用案例：</p>\\n<ul>\\n<li>\\n<p>电力系统优化：AFSA算法被用于电力系统中的优化问题，如电力网络的潮流计算、电力系统调度和能量管理等。</p>\\n</li>\\n<li>\\n<p>机器学习：AFSA算法可以应用于机器学习领域中的参数优化问题，如支持向量机（SVM）的参数优化、神经网络的权重调整等。</p>\\n</li>\\n<li>\\n<p>物流路径规划：AFSA算法可以应用于物流路径规划问题，如配送车辆路径规划、货物装载优化等。</p>\\n</li>\\n<li>\\n<p>调度问题：AFSA算法可以应用于调度问题，如工厂生产调度、任务分配等。</p>\\n</li>\\n</ul>\\n<p>以下是一个简单的示例代码，展示了如何使用人工鱼群算法（AFSA）来解决一个简单的函数优化问题。</p>\\n<pre><code>pythonCopy codeimport numpy as np\\n\\ndef rastrigin_function(x):\\n    return 10 * len(x) + sum([(xi**2 - 10 * np.cos(2 * np.pi * xi)) for xi in x])\\n\\ndef artificial_fish_swarm_algorithm(dim, population_size, max_iter, lb, ub):\\n    \\n    population = np.random.uniform(lb, ub, (population_size, dim))\\n    velocity = np.random.uniform(lb, ub, (population_size, dim))\\n    \\n    best_solution = None\\n    best_fitness = float(&#39;inf&#39;)\\n    \\n    for iter in range(max_iter):\\n        for i in range(population_size):\\n            \\n            population[i] += velocity[i]\\n            velocity[i] += np.random.uniform(-1, 1, dim)\\n            \\n            population[i] = np.clip(population[i], lb, ub)\\n            \\n            fitness = rastrigin_function(population[i])\\n            \\n            if fitness &lt; best_fitness:\\n                best_solution = population[i]\\n                best_fitness = fitness\\n        \\n    return best_solution, best_fitness\\n\\nif __name__ == &#34;__main__&#34;:\\n    \\n    dim = 10\\n    population_size = 50\\n    max_iter = 100\\n    \\n    lb = -5.12\\n    ub = 5.12\\n    \\n    best_solution, best_fitness = artificial_fish_swarm_algorithm(dim, population_size, max_iter, lb, ub)\\n    \\n    print(&#34;Best solution:&#34;, best_solution)\\n    print(&#34;Best fitness:&#34;, best_fitness)\\n</code></pre>\\n<p>以上代码使用Python语言实现了人工鱼群算法（AFSA）来优化一个简单的函数（Rastrigin函数）。你可以根据需要修改问题的维度、种群大小、最大迭代次数和搜索空间的上下界等参数，以适应不同的问题。请注意，此示例代码仅为了展示AFSA算法的基本思想和实现方式，具体的应用场景中需要根据问题的特点进行相应的调整和优化。</p>\\n<h3>5. 结论</h3>\\n<p>人工鱼群算法（AFSA）作为一种基于鱼群行为模拟的进化算法，具有较好的全局搜索能力和鲁棒性。它已经在多个领域取得了一些成功的应用，并且在实际问题中展现出了一定的优势。然而，AFSA算法仍然存在一些挑战和改进的空间，如算法的收敛性和收敛速度等。未来的研究可以进一步探索和改进AFSA算法，以提高其在实际问题中的应用效果。</p>', 'author': '皮牙子抓饭', 'reading_time': 11, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 54, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'b51a29e7a723592d0ec4ef1f252c3b70c46a5fcf104670be4974178a1a7558c4', 'title': 'LangChain大模型应用开发指南-AI大模型衍生的新能力', 'url': 'https://juejin.cn/post/7286482043139784762', 'published_at': '2023-10-07T01:06:13Z', 'created_at': '2023-10-07T07:03:23.08877Z', 'changed_at': '2023-10-07T07:03:23.08877Z', 'content': '<h1>LangChain大模型应用开发指南-AI大模型衍生的新能力</h1>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ce76ea008e194d998f955aa0e37f51b2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=540&amp;s=1337942&amp;e=png&amp;b=0e3286\" alt=\"\" loading=\"lazy\"/></p>\\n<p>上节课，我以传统应用编程设计模式和思维为入口和对比对象，介绍了LangChain中的Chain、Agent、Callback三大核心概念，并整理了LangChain为众多开发者内置的能力与工具。没有看过的小伙伴可以点击链接查看：\\n<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fh5.clewm.net%2F%3Furl%3Dqr61.cn%252Fo5bXLk%252FqHMutbX%26hasredirect%3D1\" title=\"https://h5.clewm.net/?url=qr61.cn%2Fo5bXLk%2FqHMutbX&amp;hasredirect=1\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">AI课程合集</a></p>\\n<blockquote>\\n<p>今天我将为大家介绍LangChain基于AI大模型衍生的新能力，Model I/O、Retrieval、和Memory。</p>\\n</blockquote>\\n<h2>组件总览</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e2d6f8dd4be94128aff34071f2b924f0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4963&amp;h=2880&amp;s=1014279&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<p>大家看完今天的文章后，可以回头再来看下LangChain的组件与架构图，就能从全局的角度基本理清LangChain的组件与抽象层以及它们之间的相关联系。</p>\\n<p>本篇文章主要介绍LangChain基于AI大模型衍生的新能力，本次课程整体提纲如下:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a842ddc0a07441e1a0b7e6a5983c3ff2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=991&amp;h=506&amp;s=67968&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>Model I/O -交互而非IO</h3>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6a39de9156d945009e033d845ad3af40~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4000&amp;h=1536&amp;s=368471&amp;e=png&amp;b=eefcf1\" alt=\"\" loading=\"lazy\"/></p>\\n<p>传统的应用开发通常需要定义好输入输出（IO）的格式和规范，例如文本、图像、音频、视频等。这样做的好处是可以保证数据的一致性和可解释性，但也带来了一些限制和不便，例如需要对数据进行预处理和后处理，需要适配不同的设备和平台，需要考虑用户的习惯和偏好等。</p>\\n<p>LangChain则提供了一种新的方式：Model I/O，即直接与模型进行交互，而不需要关心IO的细节。你可以把它想象成我们日常交流时使用的嘴巴和耳朵。如上图所示，Model I/O有三种类型：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>类型</th><th>作用</th></tr></thead><tbody><tr><td>Prompts</td><td>用于向语言模型提供输入的模板，它们可以定义输入变量，输出格式，部分变量等，以便生成符合用户需求的文本</td></tr><tr><td>Language models</td><td>用于人工智能模型的参数自定义，它们可以根据不同的参数（如温度，最大长度，前缀等）产生不同风格和内容的文本</td></tr><tr><td>Output parsers</td><td>用于将语言模型的输出解析为更结构化的信息的类如JSON、XML</td></tr></tbody></table>\\n<p>Model I/O的核心思想是利用AI大模型强大的自然语言理解和生成能力，将任何形式的输入转换为自然语言，然后将自然语言输入到模型中，得到自然语言的输出，再将自然语言转换为任何形式的输出。这样做的好处是可以简化应用开发流程，提高用户体验，增加应用场景和功能。</p>\\n<p>如下是一个提供了多个角色对话内容的Model I/O交互示例：</p>\\n<pre><code>from langchain.prompts import ChatPromptTemplate\\n\\ntemplate = ChatPromptTemplate.from_messages([\\n    (&#34;system&#34;, &#34;You are a helpful AI bot. Your name is {name}.&#34;),\\n    (&#34;human&#34;, &#34;Hello, how are you doing?&#34;),\\n    (&#34;ai&#34;, &#34;I&#39;m doing well, thanks!&#34;),\\n    (&#34;human&#34;, &#34;{user_input}&#34;),\\n])\\n\\nmessages = template.format_messages(\\n    name=&#34;Bob&#34;,\\n    user_input=&#34;What is your name?&#34;\\n)\\n</code></pre>\\n<h3>Retrieval -检索而非查询</h3>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/52f643c8e0bd4d0db175fa7d4ed7990d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4256&amp;h=1472&amp;s=574769&amp;e=png&amp;b=e6fbeb\" alt=\"\" loading=\"lazy\"/></p>\\n<p>传统的应用开发通常需要定义好查询（Query）的格式和规范，例如关键词、标签、分类等。这样做的好处是可以保证查询的准确性和有效性，但也带来了一些限制和不便，例如需要对查询进行规范化和优化，需要适配不同的数据源和接口，需要考虑用户的意图和需求等。</p>\\n<p>LangChain则提供了一种新的方式：Retrieval，即定义好数据源以及加载方式后，直接从模型中检索（Retrieve）所需的信息，而不需要关心查询的细节。你可以把它想象成语言模型使用搜索引擎来查找相关内容。如上图所示，我们可以将Retrieval的能力划分为以下几个部分：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>能力</th><th>作用</th></tr></thead><tbody><tr><td>Document loaders</td><td>从不同的来源加载文档，支持多种文档类型和位置</td></tr><tr><td>Document transformers</td><td>对文档进行变换，提取相关的部分，分割成小块</td></tr><tr><td>Text embedding models</td><td>为文档创建语义嵌入，实现快速有效的相似度搜索</td></tr><tr><td>Vector stores</td><td>存储和检索嵌入向量，支持多种数据库类型和接口</td></tr><tr><td>Retrievers</td><td>从数据库中检索数据，支持多种检索算法和优化方法</td></tr></tbody></table>\\n<p>Retrieval是LangChain中最重要的组件之一，它可以让语言模型拥有更丰富和更准确的上下文。Retrieval通过利用AI大模型强大的知识库和语义理解能力，能够将任何形式的查询转换为自然语言，然后将自然语言输入到模型中，得到自然语言的答案，再将自然语言转换为任何形式的输出。</p>\\n<p>如下是一个MultiQueryRetriever的使用示例：</p>\\n<pre><code>\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\n\\nloader = WebBaseLoader(&#34;https://lilianweng.github.io/posts/2023-06-23-agent/&#34;)\\ndata = loader.load()\\n\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\\nsplits = text_splitter.split_documents(data)\\n\\n\\nembedding = OpenAIEmbeddings()\\nvectordb = Chroma.from_documents(documents=splits, embedding=embedding)\\n</code></pre>\\n<h3>Memory -记忆而非存储</h3>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0bf0b839899c4483950516f9df0a7911~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2880&amp;h=1472&amp;s=161942&amp;e=png&amp;b=f2fcf5\" alt=\"\" loading=\"lazy\"/></p>\\n<p>传统的应用开发通常需要定义好存储（Storage）的格式和规范，例如数据库、文件、缓存等。这样做的好处是可以保证数据的安全性和可靠性，但也带来了一些限制和不便，例如需要对数据进行备份和恢复，需要适配不同的存储系统和协议，需要考虑用户的隐私和权限等。</p>\\n<p>LangChain则提供了一种新的方式：Memory，即直接利用模型的记忆（Memory）能力。Memory是LangChain中用于存储和更新上下文的组件，它可以让语言模型记住之前的信息和状态。你可以把它想象成语言模型的大脑，它可以存储短期记忆和长期记忆。根据不同的使用场景，LangChain内部定义的Memory有以下类型：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>类型</th><th>作用</th></tr></thead><tbody><tr><td>Conversation Buffer</td><td>用于存储对话中的所有消息和元数据的内存，它可以返回一个列表，包含对话中的每个消息和其相关的信息，如发送者，接收者，时间戳等</td></tr><tr><td>Conversation Buffer Window Entity</td><td>用于从对话缓冲区中提取特定窗口大小内的实体的内存，它可以返回一个字典，包含窗口内出现的实体及其频率，类型和位置</td></tr><tr><td>Conversation Knowledge Graph</td><td>用于构建和更新对话中涉及的实体和关系的知识图谱的内存，它可以返回一个图结构，包含节点（实体）和边（关系），以及一些统计信息，如图的大小，密度，聚类系数等</td></tr><tr><td>Conversation Summary</td><td>用于生成对话的摘要的内存，它可以返回一个字符串，包含对话的主要内容和目标</td></tr><tr><td>Conversation Summary Buffer</td><td>用于存储对话摘要的内存，它可以返回一个列表，包含对话摘要的每个句子和其相关的信息，如生成时间，置信度等</td></tr><tr><td>Conversation Token Buffer</td><td>用于存储对话中的所有单词和标点符号的内存，它可以返回一个列表，包含对话中的每个单词或标点符号及其相关的信息，如词性，命名实体类型等</td></tr><tr><td>Vector Store</td><td>用于将对话中的文本或实体转换为向量表示，并进行相似度计算或聚类分析的内存，它可以返回一个矩阵，包含对话中每个文本或实体的向量表示，以及一些度量值，如余弦相似度，欧氏距离等</td></tr></tbody></table>\\n<p>Memory的核心思想是利用AI大模型强大的参数和数据容量，将任何形式的数据转换为自然语言，并将其作为模型的输入或输出。这样做的好处是可以简化应用开发流程，提高数据处理速度，增加数据来源和质量。</p>\\n<p>如下是在链中中使用memory的示例：</p>\\n<pre><code>from langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain.memory import ConversationBufferMemory\\n\\n\\nllm = OpenAI(temperature=0)\\n\\ntemplate = &#34;&#34;&#34;You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\n{chat_history}\\n\\nNew human question: {question}\\nResponse:&#34;&#34;&#34;\\nprompt = PromptTemplate.from_template(template)\\n\\nmemory = ConversationBufferMemory(memory_key=&#34;chat_history&#34;)\\nconversation = LLMChain(\\n    llm=llm,\\n    prompt=prompt,\\n    verbose=True,\\n    memory=memory\\n)\\n</code></pre>\\n<h2>总结</h2>\\n<p>本文以传统应用编程设计模式和思维为对比对象，介绍了LangChain基于AI大模型衍生出的三种新的能力：Model I/O、Retrieval和Memory，它们分别解决了传统AI应用开发中遇到的IO、Query和Storage方面的问题和挑战。通过利用这些能力，应用开发者可以开发出更简单、更高效、更创新的AI应用。</p>\\n<p>通过本文的指导，读者可以迭代在传统应用编程中累积的思维方式和经验，充分利用了AI大模型衍生的新能力的创新应用。</p>\\n<p><strong>下一节课我们将整合前两节课所讲述的LangChain六大核心概念和组件，提供一个贯穿所有环节的实际应用示例，请大家持续关注。</strong></p>', 'author': 'AI小智', 'reading_time': 9, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 50, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '1680ef0227697301abb95c2dfa045595127c94a5410460c0792c47308e6c9b84', 'title': '开发人员的首选：CodeWhisperer', 'url': 'https://juejin.cn/post/7286327753112027136', 'published_at': '2023-10-07T00:20:41Z', 'created_at': '2023-10-07T03:03:22.062497Z', 'changed_at': '2023-10-07T03:03:22.062497Z', 'content': '<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">亚马逊云科技开发者社区</a>为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，点<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpassport.amazoncloud.cn%2Fsignin%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://passport.amazoncloud.cn/signin?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">这里</a>让它成为你的技术宝库！</p>\\n</blockquote>\\n<p>CodeWhisperer 是亚⻢逊出品的一款针对开发人员代码检查的 AI 编程助手，可以通过在 idea 里集成 CodeWhisperer 插件来达到使用，通过体验，目前发现以下有点：</p>\\n<ol>\\n<li>在开发人员编写代码时，会自动的针对现有的代码生成建议</li>\\n<li>可以绕过我们不熟悉或者第一次见的 api，从而达到迅速构建一个项目</li>\\n<li>有代码检查功能，通过扫描代码发现开发人员经验不足或者难以发现的代码漏洞，从而获得代码建议，让开发人员根据建议立即修复，保证代码的安全。</li>\\n</ol>\\n<p>文章来源：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F64f8765bd75d3d261ac2a1de%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/column/article/64f8765bd75d3d261ac2a1de?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">dev.amazoncloud.cn/column/arti…</a></p>', 'author': '亚马逊云开发者', 'reading_time': 1, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 49, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '6eeb8fdbf87a5e09c28340ed834b697b24de50f753e037fc3324e197e5d9df6a', 'title': 'OpenAI官方ChatGPT提示工程 - 最佳实践11篇（大合集）', 'url': 'https://juejin.cn/post/7286310628352606269', 'published_at': '2023-10-06T13:02:13Z', 'created_at': '2023-10-06T17:28:03.324383Z', 'changed_at': '2023-10-06T17:28:03.324383Z', 'content': '<p>今年，在OpenAI开放了ChatGPT不久后，网络上出现了许多关于如何编写Prompt的教程。这些课程和相关的Prompt模版，在一定程度上弥补了这块新鲜领域的空白，使得大家对于如何编写Prompt有了一定的认知。</p>\\n<p>在这不久之后，OpenAI和吴恩达合作开发了一套针对ChatGPT提示工程的最佳实践课程，课程名为《ChatGPT Prompt Engineering for Developers》。</p>\\n<p>随后，OpenAI官方又正式的发布了一份&#34;GPT 最佳实践&#34;指南，可以说是期盼已久。官方出品，必是精品。想快速了解，可以先看看这篇文章：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F_nB3bpyGRf2411KnBPouJg\" title=\"https://mp.weixin.qq.com/s/_nB3bpyGRf2411KnBPouJg\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">GPT最佳实践 - 提升Prompt效果的六个策略</a>。</p>\\n<p><strong>本文汇总了这些官方权威教程的所有文章，内容包括：</strong></p>\\n<ul>\\n<li>OpenAI官方GPT最佳实践的六个策略，共6篇文章</li>\\n<li>OpenAI与吴恩达合作开发的ChatGPT提示工程课程，共5篇文章</li>\\n<li>扩展内容\\n<ul>\\n<li>OpenAI关于AGI通用人工智能及未来技术的规划</li>\\n<li>比尔·盖茨关于“人工智能的风险是真实存在的”的思考</li>\\n</ul>\\n</li>\\n</ul>\\n<h2>一、OpenAI官方GPT 最佳实践指南</h2>\\n<p>本指南分享了提高GPT的效果的策略和方法，这些方法有时可以结合使用以获得更好的效果。同时鼓励多尝试试验，找到最适合自己的方法。</p>\\n<p>以下是提高Prompt效果的六大关键策略：</p>\\n<h3>1.编写清晰的提示</h3>\\n<p>如果GPT输出的内容过长，可以要求模型进行简短的回复；如果输出过于简单，可以要求模型使用专业的写作水准输出内容。如果你对输出的格式不满意，可以提供自己想要的格式。越是明确表达自己的需求，越有可能得到满意的答案。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-uxXLEv13ymyhFPbBoufeA\" title=\"https://mp.weixin.qq.com/s/-uxXLEv13ymyhFPbBoufeA\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">提升GPT Prompt效果最佳实践 - 编写清晰的提示</a></p>\\n<h3>2.提供参考文本</h3>\\n<p>GPT模型可以自信地编造虚假答案，尤其是在涉及深奥主题或引用和URL时。就像学生在考试时可以查看笔记来帮助自己更好地回答问题一样，向GPT模型提供参考文本可以帮助其减少编造虚假答案的情况。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FrtdTnlrZHuHjB1paUNTssQ\" title=\"https://mp.weixin.qq.com/s/rtdTnlrZHuHjB1paUNTssQ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">提升GPT Prompt效果最佳实践 - 提供参考文本</a></p>\\n<h3>3.将复杂的任务拆分为更简单的子任务</h3>\\n<p>就像在软件工程中将复杂系统分解为一组模块化组件一样，在提交给GPT模型的任务中也是如此。复杂任务往往比简单任务出错率更高。此外，复杂任务通常可以重新定义为一系列简单任务的工作流程，其中前置任务的输出作为后续任务的输入。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fo6iM59TfD2kf_Z8MSRibzw\" title=\"https://mp.weixin.qq.com/s/o6iM59TfD2kf_Z8MSRibzw\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">提升GPT Prompt效果最佳实践 - 拆解复杂任务</a></p>\\n<h3>4.给 GPT 时间思考</h3>\\n<p>如果让你计算17乘以28，你可能不会立即知道答案，但是却可以花时间计算出来。类似地，当GPT试图立即回答问题时，它会犯更多的推理错误，而不是花时间计算出答案。在回答问题之前，要求模型给出一系列的推理过程可以帮助GPT更可靠地推理正确的答案。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FfB6n-NgG6UKe78QvVMc9Yg\" title=\"https://mp.weixin.qq.com/s/fB6n-NgG6UKe78QvVMc9Yg\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">提升GPT Prompt效果最佳实践 - 给 GPT 时间思考</a></p>\\n<h3>5.使用外部工具</h3>\\n<p>通过使用其他工具的输出来弥补GPT的不足。例如，使用文本检索系统来告诉GPT相关文档的信息，或者使用代码执行引擎来帮助GPT进行数学计算和代码运行。如果有其他工具可以更可靠或更有效地完成某个任务，就应该使用这些工具，以获得最佳效果。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUNJL3BugIw4tHWk6VUx0xw\" title=\"https://mp.weixin.qq.com/s/UNJL3BugIw4tHWk6VUx0xw\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">提升GPT Prompt效果最佳实践 - 使用外部工具</a></p>\\n<h3>6.系统地测试变更</h3>\\n<p>如果能够进行测量，那么提高效果就会更容易。在某些情况下，对提示的修改会在几个孤立的示例上实现更好的效果，但会导致在一组更具代表性的示例上整体表现变差。因此，为了确保更改对效果能够产生积极的影响，可能有必要定义一个全面的测试套件（也称为“评估(eval)”）。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FmZyawqAW66DK1C3lypIRQQ\" title=\"https://mp.weixin.qq.com/s/mZyawqAW66DK1C3lypIRQQ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">提升GPT Prompt效果最佳实践 - 系统的测试变更</a></p>\\n<h2>二、OpenAI与吴恩达合作的ChatGPT提示工程课程</h2>\\n<p>这门课程时长为1个小时，内容简单易懂，还提供了实践的环境。讲师是吴恩达（Andrew Ng，DeepLearning.AI创始人）和伊莎·富尔福德（Isa Fulford，OpenAI的技术人员），含金量非常高。</p>\\n<p>以下是该课程的5篇文章：</p>\\n<h3>1.编写Prompt的两个关键原则</h3>\\n<ul>\\n<li>原则一：编写清晰、具体的说明</li>\\n<li>原则二：给予模型思考的时间</li>\\n</ul>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FLzwRLNlHpVd-sEY0RNub9Q\" title=\"https://mp.weixin.qq.com/s/LzwRLNlHpVd-sEY0RNub9Q\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程的两个关键原则</a></p>\\n<h3>2.文本总结</h3>\\n<p>这篇主要介绍了如何对内容进行总结，可以让模型来总结/提取重点内容，限制结果的长度。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FDUMutcPkLXRl_3u2Eel_LQ\" title=\"https://mp.weixin.qq.com/s/DUMutcPkLXRl_3u2Eel_LQ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 总结</a></p>\\n<h3>3.文本推理</h3>\\n<p>可以让模型来识别一段内容的情绪，或者提取指定的内容。</p>\\n<p>也可以一次性执行多个任务，从而节省多次请求的整体耗时和成本。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FAHNbS5USVWcRaCIAue2Hxg\" title=\"https://mp.weixin.qq.com/s/AHNbS5USVWcRaCIAue2Hxg\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 推理</a></p>\\n<h3>4.文本转换</h3>\\n<p>在这篇文章中，介绍了如何使用大型语言模型来进行文本转换工作，如语言翻译、语调调整和格式转换。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FiAaFSRJxOt0iZwZL7l0vMg\" title=\"https://mp.weixin.qq.com/s/iAaFSRJxOt0iZwZL7l0vMg\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 转换</a></p>\\n<h3>5.邮件回复与营销文案</h3>\\n<p>在这篇文章中，介绍了如何使用大型语言模型来进行邮件自动回复、生成营销文案。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F7qmfASBuq6M12f-3PaKvTA\" title=\"https://mp.weixin.qq.com/s/7qmfASBuq6M12f-3PaKvTA\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 邮件回复、营销文案</a></p>\\n<h2>三、规划与风险</h2>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fa4l5NkUy54gD-72U-mCzHQ\" title=\"https://mp.weixin.qq.com/s/a4l5NkUy54gD-72U-mCzHQ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">OpenAI关于AGI通用人工智能及未来技术的规划（全文译文）</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FdTp4ETiF8SR2xflZ1TSLew\" title=\"https://mp.weixin.qq.com/s/dTp4ETiF8SR2xflZ1TSLew\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">人工智能的风险是真实存在的 - 比尔·盖茨</a></p>\\n<h2>最后</h2>\\n<p>如果大家感兴趣的话，也建议去官网学习一下。然后结合我总结的文章，以达到更好的学习效果。</p>\\n<p>无论如何，这些提示工程的最佳实践课程是OpenAI官方出品，还有吴恩达的加持，属于行业顶尖的权威教程。请大家务必收藏好，以便随时进行查阅和复习。</p>\\n<p>希望这些文章能够助你在接下来的AI之路，赢在起跑线上。</p>\\n<h2>参考</h2>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Fgpt-best-practices\" title=\"https://platform.openai.com/docs/guides/gpt-best-practices\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">platform.openai.com/docs/guides…</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.deeplearning.ai%2Fshort-courses%2Fchatgpt-prompt-engineering-for-developers%2F\" title=\"https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.deeplearning.ai/short-cours…</a></p>', 'author': 'jsam', 'reading_time': 5, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 48, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '41b2e239070abc933783ba30d8316dc91c3c60a478d30ce93f879c0292d01378', 'title': '重磅！ChatGPT 可以进行 AI 绘图了，来看教程吧', 'url': 'https://juejin.cn/post/7286127842420752420', 'published_at': '2023-10-06T12:22:01Z', 'created_at': '2023-10-06T17:28:03.321099Z', 'changed_at': '2023-10-06T17:28:03.321099Z', 'content': '<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ad910a8a57343b08a766d6bdddfafa3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1792&amp;h=1024&amp;s=4085927&amp;e=png&amp;b=817a72\" alt=\"\" loading=\"lazy\"/>\\n前几个月，OpenAI 上线了代码解释器，从此人们无需代码，只要通过自然语言，就可以进行大数据分析，从此人人都是数据分析师。就在最近，又上线了 DALL·E 3， 通过自然语言，在 ChatGPT 就可以进行 AI 绘画，从此人人都是数字艺术家。</p>\\n<p>比如说，我在推特上 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2FLearnAI_MJ\" title=\"https://twitter.com/LearnAI_MJ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">@LearnAI_MJ</a> 看到的最有创意的提示词就是 “A painter painting a canvas, where the canvas shows the same painter painting a canvas, and so on (一个画家在画布上作画，画布上显示的是同一个画家在画布上作画，以此类推)”。把这段文本输入 DALL·E 3，ChatGPT 就为我生成了下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/089494374e064458a83c2b7546f16112~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1200&amp;h=686&amp;s=132208&amp;e=jpg&amp;b=7d7771\" alt=\"a painter painting by dalle 3\" loading=\"lazy\"/></p>\\n<p>从此在数字艺术领域，比拼的不是技术，而是创意。</p>\\n<p>那么如何在 ChatGPT 中使用 DALL·E 3 进行 AI 绘画呢？在下面的文章中，我将从基本的方法开始，一步步过渡到 DALL·E 3 的高级用法。</p>\\n<h2>1. 如何激活 DALL·E 3</h2>\\n<p>目前 DALL·E 3 仅对部分 ChatGPT Plus 用户开放，名额由 OpenAI 自行决定。它的入口和联网功能，代码解释器，插件功能一样，都位于 GPT-4 按钮菜单中。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/497747baa0f541f9843e05a777b34ec7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1196&amp;h=882&amp;s=63458&amp;e=jpg&amp;b=fefefe\" alt=\"dalle 3 entrance\" loading=\"lazy\"/></p>\\n<p>选中以后，在聊天框里输入提示词就可以作画了。</p>\\n<h2>2. 如何写提示词</h2>\\n<p>和 ChatGPT 的用法一样，你只需要把 DALL·E 3 当作一个助理，告诉你对图片的要求即可，比如说 “A serene mountain landscape during autumn. (秋季宁静的山景)”。</p>\\n<p>DALL·E 3 甚至还可以作为你的提示词助理，在你没有 Idea 的时候帮你写提示词。大家都知道 ChatGPT 的目前的训练时间截至 2022 年 1 月，所以它不知道 DALL·E 3，更加不知道 Midjourney ，所以它不知道如何写 AI 绘画的提示词。但是 DALL·E 3 却知道！来看看我们如何把它作为提示词助理，比如我输入如下提示词：</p>\\n<blockquote>\\n<p>Prompt: Providing me 4 image prompts that will be universally appreciated for their usefulness and humor. (为我提供 4 个详细的图片提示，它们的实用性和幽默感将得到普遍赞赏。)</p>\\n</blockquote>\\n<p>于是乎 DALL·E 3 就为我生成了 4 个非常详细的提示词。</p>\\n<blockquote>\\n<p>当然！以下是四则图片提示，旨在体现实用性和幽默感：</p>\\n<ol>\\n<li><strong>图片：</strong> 一只猫戴着安全护目镜和建筑帽，站在一个有玩具推土机和起重机的微型建筑工地旁。这只猫似乎在视察工地，表情严肃，旁边一只穿着西装的老鼠拿着一张小蓝图。背景是一个标牌，上面写着 &#34;Paws Construction Co.&#34; （爪子建筑公司）。</li>\\n<li><strong>插图：</strong> 健身房里的一组不同蔬菜。西兰花在举重，胡萝卜在跑步机上，西红柿在做瑜伽。这个场景描绘了他们挥汗如雨、挑战极限的场景。在健身房入口上方，写着 &#34;蔬菜健身俱乐部&#34;。</li>\\n<li><strong>漫画：</strong> 一条一脸疑惑的鱼在一个装满水的鱼缸里阅读一本名为 &#34;如何游泳 &#34;的书。鱼缸外，一只狗和一只鸟在咯咯地笑着，互相窃窃私语，努力忍住笑声。</li>\\n<li><strong>水彩画：</strong> 一只年迈的乌龟坐在电脑桌前，戴着眼镜，表情沮丧。它正试图点击屏幕上的一个弹出窗口，上面写着：&#34;点击这里，加快您的电脑速度！&#34; 旁边的一只小野兔用爪子捂住嘴巴，努力忍住不笑。</li>\\n</ol>\\n<p>您想把这些变成图片吗？</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/681560e38a2f425e8215d899b9a316c2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=910&amp;s=154362&amp;e=jpg&amp;b=f8f7f7\" alt=\"use dalle 3 to generate image prompts\" loading=\"lazy\"/></p>\\n<p>DALL·E 3 默认会生成 4 张图片。而我刚好让它生成了 4 个提示词，于是它就问我是否要把这些提示词变成图片。我回答了 “Yes”，然后 DALL·E 3 就为我生成了 4 张图片。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/226b24b8f78b4c43b9f5a3206d3e5e04~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1111&amp;h=1243&amp;s=248891&amp;e=jpg&amp;b=f3ecea\" alt=\"images generated by dalle 3-1\" loading=\"lazy\"/></p>\\n<h2>3. 如何下载图片</h2>\\n<p>当你把鼠标悬停在图片上时，就会出现下载按钮，点击按钮即可完成下载。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6e3de91029cf451a93ac34d197fdcd61~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=590&amp;h=616&amp;s=46467&amp;e=jpg&amp;b=e9e2e0\" alt=\"dalle 3 download button\" loading=\"lazy\"/></p>\\n<p>还有一种方式是点击图片，然后在打开的页面上也会出现下载按钮，除此之外，你还可以看到用于生成该图片的提示词。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3ee3b49507244865973619c15fcaa9b0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1553&amp;h=830&amp;s=107952&amp;e=jpg&amp;b=171615\" alt=\"dalle 3 image preview page\" loading=\"lazy\"/></p>\\n<h2>4. 如何生成不同宽高比的图片</h2>\\n<p>DALL·E 3 支持 3 种分辨率：</p>\\n<ul>\\n<li>正方形（1024x1024）： 这是默认分辨率，适合大多数普通图像。</li>\\n<li>宽屏（1792x1024）： 适合风景、全景或任何需要水平方向的图像。</li>\\n<li>高（1024x1792）： 最适合全身肖像、高大结构或任何需要垂直方向的图像。</li>\\n</ul>\\n<p>DALL·E 3 会根据提示词的内容自动选择分辨率，你也可以指定一个分辨率。举个例子来说，比如我输入下面的提示词：</p>\\n<blockquote>\\n<p>Prompt: Full body portrait of a cat wearing safety goggles and a construction hat, inspecting the site with a serious expression. In the background, there&#39;s a sign that reads, &#34;Paws Construction Co.&#34; (一只猫的全身像，它戴着安全护目镜和施工帽，表情严肃地检查工地。背景中，有一个写着 &#34;Paws Construction Co.&#34; 的标志。)</p>\\n</blockquote>\\n<p>根据提示词中 full body portrait (全身照) 这个词，DALL·E 3 会选择 1024x1792 这个分辨率，如下图所示。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f301441affed48e482317bc0a980574c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1003&amp;h=1731&amp;s=220722&amp;e=jpg&amp;b=ddd9d2\" alt=\"full body portrait of cat by dalle 3\" loading=\"lazy\"/></p>\\n<p>你还可以使用 “vertical images (竖向图)” 这个词输出 1024x1792 的图片，或者直接在提示词中指定这个分辨率。</p>\\n<p>如果要生成 1792x1024 的图片，我可以这样调整提示词：</p>\\n<blockquote>\\n<p>Prompt: A panoramic view of a cat wearing safety goggles and a construction hat, standing next to a miniature construction site with toy bulldozers and cranes. The cat appears to be inspecting the site with a serious expression, while a mouse in a suit holds a tiny blueprint next to it. In the background, there&#39;s a sign that reads, &#34;Paws Construction Co.&#34; (一只戴着安全护目镜和建筑帽的猫站在一个有玩具推土机和起重机的微型建筑工地旁的全景。这只猫似乎在视察工地，表情严肃，旁边一只穿着西装的老鼠拿着一张小蓝图。背景中，有一个写着 &#34;Paws Construction Co.&#34; 的标志。)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/885bc658c97c4eceb6f6a6798bebef82~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=904&amp;h=554&amp;s=93454&amp;e=jpg&amp;b=e8e0dc\" alt=\"wide image of cat by dalle 3\" loading=\"lazy\"/></p>\\n<p>你还可以用 “wide images (宽幅图像) ” 这个词输出 1792x1024 的图片，或者直接指定这个分辨率。</p>\\n<h2>5. 如何修改图片</h2>\\n<p>你只需要指定要修改的图片以及怎么修改即可，比如我要把上面生成的四张图中的第一张中的场景由室内变成室外，可以通过下面这个提示词进行修改：</p>\\n<blockquote>\\n<p>Prompt: Could you maintain the appearance of the characters, including their clothing and accessories, as shown in the first image above, but change the setting from indoors to outdoors? (能否保留人物的外观，包括服装和配饰，如上图所示，但将场景从室内改为室外？)</p>\\n</blockquote>\\n<p>DALL·E 3 修改后的图片如图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a6de941d66a4da5907e63fd382d5900~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1792&amp;h=1024&amp;s=249857&amp;e=jpg&amp;b=efece1\" alt=\"panoramic outdoor scene with a cat adorned in safety goggles and a construction hat by dalle 3\" loading=\"lazy\"/></p>\\n<p>DALL·E 3 其实是把提示词修改了一下然后重新生成了另一张图像，并没有再原有的基础上进行修改。所以尽管我要求保持人物一致，但实际上 DALL·E 3 生成的是另一只猫和老鼠。</p>\\n<h2>总结</h2>\\n<p>DALL·E 3 又降低了 AI 绘画的门槛，基本上会用 ChatGPT 的人都会很快掌握 DALL·E 3。当然，目前的艺术效果和对图像精确的控制和 Midjourney 是有差距的。不过对于大多数用户来说，功能已足够好了。你怎么看呢？欢迎发消息进行探讨。</p>\\n<blockquote>\\n<p>分享完毕，感谢阅读🤗 欢迎点赞，收藏，评论</p>\\n<p>更多免费原创教程，🚀关注公众号：我的AI力量</p>\\n</blockquote>', 'author': '我的AI力量', 'reading_time': 7, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 52, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '2c3c4ca12d81eaeda7935e7e836250a5127a01709fb7a62329a2ef816b718dfe', 'title': '自动驾驶技术的基础知识', 'url': 'https://juejin.cn/post/7286264172097749044', 'published_at': '2023-10-06T11:03:57Z', 'created_at': '2023-10-07T04:03:22.875449Z', 'changed_at': '2023-10-07T04:03:22.875449Z', 'content': '<p>自动驾驶技术是现代汽车工业中的一项革命性发展，它正在改变着我们对交通和出行的理解。本文将介绍自动驾驶技术的基础知识，包括其概念、历史发展、分类以及关键技术要素。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/66cd8a1e3a964fc2a1d9c9c54bda6b13~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=600&amp;h=326&amp;s=482512&amp;e=png&amp;b=63737d\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>1. 自动驾驶概念</h2>\\n<p>自动驾驶是一种先进的交通技术，它允许汽车在没有人类干预的情况下完成驾驶任务。这意味着车辆可以自动执行加速、制动、转向以及遵循交通规则等任务，而无需驾驶员的操控。自动驾驶技术的最终目标是实现完全自动化，让车辆能够在各种道路和天气条件下安全地行驶。</p>\\n<h2>2. 自动驾驶技术的历史发展</h2>\\n<p>自动驾驶技术的发展可以追溯到几十年前。早期的自动驾驶系统主要是基于机械和电子技术，用于辅助驾驶员的操控，例如巡航控制和自动泊车系统。然而，随着计算机科学和人工智能的发展，自动驾驶技术取得了巨大进步。</p>\\n<p>2004年，美国国防高级研究计划局（DARPA）举办了第一个“无人驾驶挑战赛”，这是一个里程碑式的事件，激发了自动驾驶技术的研究和发展。在随后的几年里，许多科技公司和汽车制造商开始投入大量资金和资源，致力于推动自动驾驶技术的商业化。</p>\\n<h2>3. 自动驾驶的分类</h2>\\n<p>自动驾驶技术根据其自主性和能力水平可以分为不同级别，<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzk0ODUwNjUxNQ%3D%3D%26mid%3D2247484328%26idx%3D1%26sn%3Dfea16e0c7756a619c350365fd55c8817%26chksm%3Dc367d954f410504206f4fb9c41720fe3738bce855cebac4869077904583dd266b84095a7e54f%26token%3D200487957%26lang%3Dzh_CN%23rd\" title=\"https://mp.weixin.qq.com/s?__biz=Mzk0ODUwNjUxNQ==&amp;mid=2247484328&amp;idx=1&amp;sn=fea16e0c7756a619c350365fd55c8817&amp;chksm=c367d954f410504206f4fb9c41720fe3738bce855cebac4869077904583dd266b84095a7e54f&amp;token=200487957&amp;lang=zh_CN#rd\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">前边的文章我们对这一块有详细的讲解</a>，通常采用L0到L5的分类系统：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8a5230544d3846b3a81b05ee719a7904~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1920&amp;h=1200&amp;s=1572241&amp;e=png&amp;b=01000d\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>L0级是完全的手动驾驶</strong>, 驾驶员要完成汽车的全部操控操作。L0级别并不意味着车辆完全没有智能化辅助,依然可以配备一些辅助驾驶的保护系统,如刹车辅助、盲区监测等。但这些功能对驾驶过程影响很小,驾驶员始终要全程参与驾驶。目前市面上的大多数车型仍属于L0级。</p>\\n<p><strong>L1级是辅助驾驶级别</strong>,可以协助驾驶员完成一些简单和重复的驾驶操作。典型的L1级功能有自适应巡航系统,可以控制车辆的速度;还有车道保持系统,可以辅助控制方向。这些功能可以减轻驾驶员的部分工作量,但驾驶员仍需全程参与驾驶,随时做好接管准备。</p>\\n<p><strong>L2级是部分自动驾驶</strong>,此时车辆可以同时控制车速和方向,完成一些基本的驾驶任务,但驾驶员仍需监控周围环境,随时接管车辆。典型的L2功能有自适应巡航搭配车道保持、自动泊车和自动变道等。目前,多数品牌推出的所谓自动驾驶功能,实际上大多数处于L2级水平。</p>\\n<p><strong>L3级是有条件的自动驾驶</strong>,它可以在特定条件下实现自动驾驶,如高速公路或专用道路上。在这种条件下,L3级自动驾驶可以完成所有驾驶任务和周边环境监控,驾驶员可以暂时不参与驾驶。但一旦自动系统发出接管请求,驾驶员就必须立即接手车辆。目前,还没有量产车型达到真正的L3级水平。</p>\\n<p><strong>L4级是高度自动驾驶</strong>,它可以在特定区域或环境内实现完全的自动驾驶,无需人工接管。与L3级不同,L4级系统在预定操作域内可以处理突发情况,不需要驾驶员参与。但L4也存在环境限制,无法达到随时随地的完全自动驾驶。目前也还没有车型实现真正的L4级自动驾驶。</p>\\n<p><strong>L5级代表完全的自动驾驶</strong>,整个行车过程全由车辆自己完成,无需任何人工干预。理论上L5级自动驾驶汽车不再需要方向盘和 pedal,可以在任何时间任何地点以任何速度进行自动驾驶,完全实现无人驾驶。但由于技术和法规限制,真正做到L5级自动驾驶还需时日。</p>\\n<p>目前，大多数商用自动驾驶系统还处于L2和L3级别，而L4和L5级别的自动驾驶仍然在研发和测试阶段。</p>\\n<h2>4. 自动驾驶的关键技术要素</h2>\\n<p>自动驾驶技术的实现依赖于多个关键技术要素，包括：</p>\\n<h3>4.1 传感器技术</h3>\\n<p>传感器是自动驾驶系统的眼睛和耳朵，它们用于感知和理解车辆周围的环境。常见的自动驾驶传感器包括：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f824d0ed294b44b3ac8df22b8670ff6d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1000&amp;h=659&amp;s=387908&amp;e=png&amp;b=e4e1e1\" alt=\"\" loading=\"lazy\"/></p>\\n<ul>\\n<li><strong>摄像头：</strong> 用于捕捉道路上的图像，帮助识别其他车辆、行人、道路标志和交通信号等。</li>\\n<li><strong>雷达：</strong> 通过发射无线电波并测量其反射来检测周围物体的位置和速度。不同类型的雷达可以提供不同范围和分辨率的数据。</li>\\n<li><strong>激光雷达 (LiDAR)：</strong> 使用激光束来创建高分辨率的三维地图，以获取关于周围环境的精确深度信息。</li>\\n<li><strong>超声波传感器：</strong> 用于检测车辆周围的近距离障碍物，如停车时的距离感知。</li>\\n</ul>\\n<p>这些传感器共同工作，为自动驾驶系统提供丰富的环境感知数据，以便系统能够了解周围世界。</p>\\n<h3>4.2 软件和算法</h3>\\n<p>软件和算法是自动驾驶技术的核心。它们负责处理传感器数据、感知环境、做出决策和控制车辆。以下是软件和算法的主要任务：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b54d98a2d1484bb4bb21a9123c4fd1c0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1378&amp;h=525&amp;s=64010&amp;e=png&amp;b=fbfbfb\" alt=\"\" loading=\"lazy\"/></p>\\n<ul>\\n<li><strong>感知：</strong> 通过图像处理、目标检测和物体跟踪等技术，将传感器数据转化为对车辆周围环境的理解。这包括检测其他车辆、行人、道路标志、车道线等。</li>\\n<li><strong>定位：</strong> 确定车辆在地图上的精确位置，通常使用全球定位系统（GPS）和惯性测量单元（IMU）等传感器数据。</li>\\n<li><strong>路径规划：</strong> 基于环境感知和目标，规划车辆的行驶路径，确保安全和有效的驾驶。</li>\\n<li><strong>控制：</strong> 执行加速、制动、转向等控制命令，以实现规划的驾驶路径。</li>\\n<li><strong>决策制定：</strong> 根据环境条件和车辆状态，做出决策，如超车、让路、停车等。</li>\\n</ul>\\n<p>这些软件和算法必须能够在不同的道路和天气条件下可靠运行，并且考虑到各种潜在情况，包括紧急情况和其他交通参与者的行为。</p>\\n<h3>4.3 控制系统</h3>\\n<p>控制系统负责将决策转化为车辆的物理动作。它涉及到控制车辆的加速、制动、转向和维持车辆的稳定性。自动驾驶车辆通常配备了电子稳定控制系统（ESC）等技术，以确保在各种情况下保持车辆的稳定性和安全性。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8d0bfe579a9041a5a701762ecdc99937~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=528&amp;s=182985&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>4.4 高精度地图</h3>\\n<p>高精度地图是自动驾驶系统的重要组成部分。这些地图不仅包含基本的地理信息，还包括车道标记、交通信号、道路坡度等详细信息。自动驾驶车辆使用这些地图来定位自己在道路上的位置，并与传感器数据进行校准。这些地图通常由专业测绘公司创建，并需要定期更新以反映道路的变化。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1e7711cc65ee426280fbfa5aecfa0a77~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=346&amp;s=421299&amp;e=png&amp;b=1b2c3f\" alt=\"\" loading=\"lazy\"/></p>\\n<p>综合来说，自动驾驶技术的关键技术要素密切合作，使车辆能够感知、理解和应对复杂的交通环境。这些技术要素的不断发展和改进将推动自动驾驶系统更加安全、可靠和普及化。然而，自动驾驶技术的成功也依赖于法规、伦理、安全标准和大规模测试等因素的综合考虑。</p>\\n<h2>总结</h2>\\n<p>自动驾驶技术的不断发展和改进将继续推动我们朝着实现更安全、高效和便捷的交通系统迈进。然而，同时也伴随着法律、伦理和安全等方面的挑战，需要不断解决和调整。在未来，我们可以期待看到自动驾驶技术在道路上的广泛应用，改变我们的出行方式和交通体验。路上的广泛应用，改变我们的出行方式和交通体验。</p>', 'author': '探索者日记', 'reading_time': 6, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 51, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '071173176d54d95a7e366ea33c2f8fa7b3a55f6d06e6ee703edc266638be4a1a', 'title': 'ChatGPT 解数学题：我被小学生狠狠地鄙视了！', 'url': 'https://juejin.cn/post/7285939168303185931', 'published_at': '2023-10-06T10:58:36Z', 'created_at': '2023-10-07T04:03:22.844362Z', 'changed_at': '2023-10-07T04:03:22.844362Z', 'content': '<h1>一、这道题小学生都会</h1>\\n<p>之前在《富足》这本书中看到一个例子。说是著名的物理学家张首晟教授，在一次报告中问了大家一个问题：</p>\\n<blockquote>\\n<p>什么数自己乘以自己等于 4？</p>\\n</blockquote>\\n<p>大家想想是什么数呢？</p>\\n<p>很多人可能都会不假思索地说，当然是「2」了，这么简单的问题还需要问吗？</p>\\n<p>但正确的答案，除了「2」之外，还有「-2」。</p>\\n<p>有人可能这道题有辱智商，但现实却是，很多人都会忽略「-2」。大家不妨也试着问下身边的朋友，看看有多少人还能说出有「-2」。</p>\\n<p>一道连小学生都会的数学题，而很多学过高等数学的大学生，居然做不对。是大脑的用进废退，还是思维惯性的问题。这个问题很值得人们反思。</p>\\n<h1>二、利用 ChatGPT 解数学题</h1>\\n<p>说到「用进废退」的问题，今天下午简直要被自己蠢哭了。</p>\\n<p>下午在解决一个问题的时候，需要解一个二次方程。我把问题简化一下，也就是最终要求 <code>x + x² = 200</code> 这样一个二次方程。</p>\\n<p>对，就是这个乍一看很简单、中学生都会解的方程，我居然没解出来。</p>\\n<p>初中老师知道了估计得哭晕在厕所。你们也别憋着了，就尽情地嘲笑我吧。</p>\\n<p>无奈之下，搬出了救兵——ChatGPT。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/84290e040303497da3c767a9127aacfe~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1576&amp;h=302&amp;s=48930&amp;e=png&amp;b=363742\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/38e56a046d0c4512bfe5df0a69d49272~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1310&amp;h=1706&amp;s=250675&amp;e=png&amp;b=393b46\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9bd1755e153344bbb0cff3fb9153991b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1274&amp;h=1542&amp;s=242364&amp;e=png&amp;b=3a3d48\" alt=\"\" loading=\"lazy\"/></p>\\n<p>原来如此！上学时的记忆，一点点被 ChatGPT 恢复过来。完美呀，还得是 ChatGPT！</p>\\n<h1>三、ChatGPT 在辅导学习上的应用</h1>\\n<p>上面只是举了个简单的例子。</p>\\n<p>很多家长抱怨，孩子学的课程难度越来越大，已经辅导不了了。看了上面那个简单的例子，是不是有所启发呢？</p>\\n<p>是的，完全可以使用 ChatGPT 来辅导孩子学习。</p>\\n<p>ChatGPT 多模态升级后，支持了图片和语音功能。我们可以利用它的图片功能，将数学题或其他科目的问题拍照，上传 ChatGPT 进行提问。这为学生以及为辅导学生有困难的家长，提供了很大的便利。</p>\\n<p>想了解更多关于 ChatGPT 多模态图片和语音功能的同学，可以看之前写的一篇文章：<a href=\"https://juejin.cn/post/7284608063913345043\" title=\"https://juejin.cn/post/7284608063913345043\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT多模态升级，支持图片和语音，体验如何？</a></p>\\n<h1>四、结束语</h1>\\n<p>很多人觉得 AI 离自己很远，但其实 AI 能解决生活中的很多问题，AI 也逐渐渗透到我们生活中的方方面面。后面如果有好玩的关于 AI 在生活领域的应用，也会陆续分享出来。</p>\\n<p>随着 AI 等各项技术的不断发展，我们的学习方式和资源也在发生着翻天覆地的变化。ChatGPT 等 AI 工具，不仅为我们提供了一个全新的学习工具，更重要的是，它为我们打开了一个全新的学习维度。</p>\\n<p>无论是复杂的数学难题，还是其他学科的挑战，我们都可以通过 AI 来寻找答案和解决方案。</p>\\n<p>但同时我们也应该清楚，<strong>AI 只是一个工具，AI 不能代替我们思考。</strong> 真正的学习和理解，还需要我们自己的努力。</p>', 'author': '程序员X小鹿', 'reading_time': 3, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 47, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '020c603949730ac74cd5c9321315fa55480d4c6bee41bdaf35ad6ae8ad2db074', 'title': '假期AI新闻热点：亚运会Al技术亮点；微软GPT-4V论文精读；Perplexity推出pplx-api；DALL-E 3多渠道测评 | ShowMeAI日报', 'url': 'https://juejin.cn/post/7285939168303022091', 'published_at': '2023-10-06T09:32:46Z', 'created_at': '2023-10-06T17:28:03.316157Z', 'changed_at': '2023-10-06T17:28:03.316157Z', 'content': '<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/745d59417c9a46799556573c4cd55e99~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2953&amp;h=886&amp;s=112976&amp;e=png&amp;a=1&amp;b=dbe9ee\" loading=\"lazy\"/>\\n<blockquote>\\n<p>👀<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fhomepage%3F__biz%3DMzg2OTYyMTcwMw%3D%3D%26hid%3D2%26sn%3D51f7bead52c41447cd0ecb3d57b884e7\" title=\"https://mp.weixin.qq.com/mp/homepage?__biz=Mzg2OTYyMTcwMw==&amp;hid=2&amp;sn=51f7bead52c41447cd0ecb3d57b884e7\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>日报&amp;周刊合集</strong></a>  |  🎡<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.showmeai.tech%2Ftutorials%2F85\" title=\"https://www.showmeai.tech/tutorials/85\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>生产力工具与行业应用大全</strong></a>  |  🧡 点赞关注评论拜托啦！</p>\\n</blockquote>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8ec6df203e0d4e9180c7c2c723c0465f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1890&amp;h=567&amp;s=15390&amp;e=png&amp;b=ffffff\" loading=\"lazy\"/>\\n<h2>🔥 科技感拉满，第19届杭州亚运会中的Al技术亮点</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a423c95b8b004203bd2652575f27abb7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1654&amp;h=827&amp;s=1008693&amp;e=png&amp;b=ededed\" loading=\"lazy\"/>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a150c7ceafc44fdda3437ffcf8cd2d89~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1654&amp;h=827&amp;s=888667&amp;e=png&amp;b=ededed\" loading=\"lazy\"/>\\n<p>八年筹备，杭州第19届亚运会开幕式于9月23日晚隆重举行，为全球观众带来了一场前所未有的数字科技盛宴。以下盘点了7项亚运会用到的AI技术，详解介绍可点击放大上方图片，非常期待10月8日的闭幕式呀：</p>\\n<blockquote>\\n<ol>\\n<li>\\n<p>数字人火炬手：开启全球首个数字点火仪式</p>\\n</li>\\n<li>\\n<p>无介质全息AI机器人：全球首款</p>\\n</li>\\n<li>\\n<p>互动3D双威亚:空中的视觉盛宴</p>\\n</li>\\n<li>\\n<p>杭州亚运会总指挥部</p>\\n</li>\\n<li>\\n<p>AI裁判评分</p>\\n</li>\\n<li>\\n<p>亚运元宇宙</p>\\n</li>\\n<li>\\n<p>真人级全息直播亭</p>\\n</li>\\n</ol>\\n</blockquote>\\n<h2>🔥 ARC浏览器发布重磅AI功能，彻底改变网页浏览体验</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/56a6eebf14f44aa0b27cfa19c43145d2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1654&amp;h=827&amp;s=1362656&amp;e=png&amp;b=3c43fc\" loading=\"lazy\"/>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Farc.net%2Fmax\" title=\"https://arc.net/max\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arc.net/max</a></p>\\n</blockquote>\\n<blockquote>\\n<p>Arc 是一款基于人工智能的浏览器，使用了 OpenAI GPT-3.5 和 Anthropic 的模型。<strong>Arc Max 是 Arc 浏览器最新发布的一个增强AI功能</strong>，免费，目前对 Mac 用户开放，Windows 用户可以申请加入 Waitlist。</p>\\n</blockquote>\\n<p>尾部超链接是一个中文使用教程视频，展示了本次更新的5项新功能。以下使用文字进行简单介绍，整体来说第1、2项功能很精彩，第4、5项功能直戳我心：</p>\\n<blockquote>\\n<ol>\\n<li>\\n<p><strong>Ask On Page：对当前页面内容进行搜索、询问、总结等操作</strong>。在任何页面使用 Command + F 键就可以唤起对话框，根据页面内容进行提问后 Arc 会进行回答，并提供原文位置避免幻觉</p>\\n</li>\\n<li>\\n<p><strong>5 Second Previews：快速预览链接内容要点</strong>。将鼠标移动到目标链接，按住 Shift 键，就会弹出链接内容的5条信息总结，可以减少不必要的打开和跳转</p>\\n</li>\\n<li>\\n<p><strong>Ask ChatGPT：搜索框的ChatGPT</strong>。使用 Arc Command Line 唤起此项功能，在搜索框输入问题后点击Tab键，就可以直接跳转 ChatGPT 对话界面，节省了寻找 ChatGPT 网页的时间</p>\\n</li>\\n<li>\\n<p><strong>Tidy Tab Titles：标签页标题优化</strong>。根据标签网页内容对标题进行优化，帮助用户快速分辨和整理标签网页</p>\\n</li>\\n<li>\\n<p><strong>Tidy Downloads：下载列表优化</strong>。根据下载内容对文件进行重命名，避免文件下载后的文件名杂乱不可阅读    <a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftwitter.com%2FCydiar404%2Fstatus%2F1709827557997392104\" title=\"https://twitter.com/Cydiar404/status/1709827557997392104\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>⋙ 中文教程 X@Cydiar404</strong></a></p>\\n</li>\\n</ol>\\n</blockquote>\\n<h2>🔥 微软166页GPT-4V论文 (中文精校版)：多模态的新时代</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/81c9029de7354ec98c1bb0da872a6a97~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1656&amp;h=829&amp;s=407086&amp;e=png&amp;b=fefdfd\" loading=\"lazy\"/>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2309.17421.pdf\" title=\"https://arxiv.org/pdf/2309.17421.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/pdf/2309.17…</a></p>\\n</blockquote>\\n<p>9月25日，OpenAI 发布具有视觉功能的 GPT-4V(ision)，能够分析用户提供的图像输入。9月29日，微软发布了166 页的 GPT-4V 研究报告「<strong>The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) (大型多模态的新时代：GPT-4V(ision)的初步探索)</strong>」。这也成为了我们了解 GPT-4V(ision) 的敲门砖。</p>\\n<p>从整体上来说，这篇论文依靠「试」对 GPT-4V 进行了研究，设计了涵盖多个领域的一系列输入，将它们喂给 GPT-4V 并观察记录 GPT-4V 的输出。研究员们对 GPT-4V 完成各类任务的能力进行评估，还给出了使用GPT-4V的新提示词技巧。</p>\\n<p>然后，这份说明书用了 150 多页来展示各种 Demo，详细剧透了 GPT-4V 在面对不同回答时展现出的能力。感兴趣可以阅读  <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fa8Y_yU5XYgJhQ2xMuTK13w\" title=\"https://mp.weixin.qq.com/s/a8Y_yU5XYgJhQ2xMuTK13w\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>⋙ 中文精校版本</strong></a></p>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/af65d8ea184342a192a390a1e8d97eff~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1890&amp;h=567&amp;s=16659&amp;e=png&amp;b=ffffff\" loading=\"lazy\"/>\\n<h2>🧰 Anthropic 全面开放 Claude API 访问权限，可供所有AWS客户使用</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eda3d79667d4487aa316e0ec8a03b0b5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1654&amp;h=827&amp;s=199392&amp;e=png&amp;b=ebdbbc\" loading=\"lazy\"/>\\n<p>9月28日，Anthropic 在官方博文中宣布与 Amazon 达成合作，所有 AWS 客户都可以在 Amazon Bedrock 上基于 Claude 来构建生成式AI应用。</p>\\n<p>Claude API 的官网申请审核通过率不高，如果你想基于 Claude API 进行开发，可以试试 AWS 的途径！  <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.anthropic.com%2Findex%2Famazon-bedrock-general-availability\" title=\"https://www.anthropic.com/index/amazon-bedrock-general-availability\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>⋙ 了解详细信息</strong></a></p>\\n<h2>🧰 Perplexity 推出 pplx-api：允许开发者将开源 LLM 集成到项目中</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4b9fe81439d541dbab4f6d9f434b1340~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1654&amp;h=827&amp;s=1806277&amp;e=png&amp;b=c7c1a9\" loading=\"lazy\"/>\\n<p>10月4日，Perplexity 在其官方博文宣布推出 pplx-api，为开发者提供了快速访问 <strong>Mistral 7B、Llama2 13B、Code Llama 34B、Llama2 70B</strong> 等开源大语言模型的方式，可以帮助其更轻松地将这些最先进的开源大模型集成到自己项目中。</p>\\n<p>pplx-api 目前处于公开测试阶段，Perplexity Pro订阅用户可以免费使用。据介绍，pplx-api具有以下几个优点：</p>\\n<blockquote>\\n<ul>\\n<li>\\n<p><strong>易于使用</strong>：开发人员可以集成现有最先进的开源模型，并通过熟悉的 REST API 在几分钟内开始使用</p>\\n</li>\\n<li>\\n<p><strong>高速推理</strong>：系统设计得非常高效，延迟时间只有 Replicate 和 Anyscale 的 1/3</p>\\n</li>\\n<li>\\n<p><strong>稳定可靠</strong>：经过了 Perplexity 自有产品的实践证明</p>\\n</li>\\n<li>\\n<p><strong>持续打造 LLM 商店</strong>：团队致力于打造一个开原模型一站式商店，并将添加更多新的开源模型   <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fblog.perplexity.ai%2Fblog%2Fintroducing-pplx-api\" title=\"https://blog.perplexity.ai/blog/introducing-pplx-api\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>⋙ 了解详细信息</strong></a></p>\\n</li>\\n</ul>\\n</blockquote>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/380cabce921d44b68f437c61b5aec2f2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1890&amp;h=567&amp;s=18659&amp;e=png&amp;b=ffffff\" loading=\"lazy\"/>\\n<h2>🧩 DALL-E 3 多渠道使用体验测评：超强语义理解能力名副其实</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/96ebf4025a3242a28edd0110b5a5b32f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1656&amp;h=829&amp;s=480115&amp;e=png&amp;b=f0edf4\" loading=\"lazy\"/>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.bing.com%2Fimages%2Fcreate\" title=\"https://www.bing.com/images/create\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.bing.com/images/crea…</a></p>\\n</blockquote>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.bing.com\" title=\"https://www.bing.com\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.bing.com</a></p>\\n</blockquote>\\n<p>假期社群里比较热闹的话题，应该就是 DALL-E 3的绘画尝试了！随着 bing 的实装和陆续开放 GPT Plus 用户的使用权限，这个绘画工具也逐步被小伙伴们「玩坏」了~</p>\\n<p>总结一下，当前部分 GPT Plus 用户还在等待开放权限，也有部分用户反馈生成图片速度较慢或只能生成两张，这是因为使用者过多导致的资源紧张。</p>\\n<p>上方两个链接也可以<strong>免费</strong>使用 DALL-E 3 进行绘图，只需要提示词中添加「绘图」的相关提示。</p>\\n<p>综合各方测评结果来看， DALL-E 3 的语义理解能力独步绝尘，其他绘画能力与 Midjourney、SDXL 相比算是各有千秋  <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F3sly2apT9UdcJ3Ds6gWD-g\" title=\"https://mp.weixin.qq.com/s/3sly2apT9UdcJ3Ds6gWD-g\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>⋙ bing 测评</strong></a>   |   <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FtHjx47mZSJy5A6yFPnULaA\" title=\"https://mp.weixin.qq.com/s/tHjx47mZSJy5A6yFPnULaA\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>GPT-4 测评</strong></a></p>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ddd00d13590f4616a87ec46b3ed87a06~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1890&amp;h=567&amp;s=15547&amp;e=png&amp;b=ffffff\" loading=\"lazy\"/>\\n<h2>👀 软银 CEO 孙正义：AI 智慧有望在 10 年内超过人类</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c882f97ad32f417397ed583d14468174~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1654&amp;h=827&amp;s=3725206&amp;e=png&amp;b=0c4fd6\" loading=\"lazy\"/>\\n<p>10月4日，软银集团创始人兼 CEO 孙正义在软银世界企业大会上发表演讲，<strong>认为 ChatGPT 等生成式人工智能正在快速发展，并相信未来 AGI (通用人工智能) 比所有人类智慧的总和还要聪明 10 倍</strong>。</p>\\n<p>从演讲内容了解到，孙正义是 ChatGPT 的重度用户，并相当看好AI的发展，软银也正开发日本版的 ChatGPT。</p>\\n<p>孙正义以几个精明的赌注而闻名，这使软银成为一家科技投资巨头，而这次他选择押宝 AGI  <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fsoftbank-ceo-masayoshi-son-says-artificial-general-intelligence-will-come-within-2023-10-04%2F\" title=\"https://www.reuters.com/technology/softbank-ceo-masayoshi-son-says-artificial-general-intelligence-will-come-within-2023-10-04/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>⋙ 新闻报道</strong></a></p>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4f0a0734e6b74586aa6b3a58e5c7a300~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1890&amp;h=567&amp;s=14167&amp;e=png&amp;b=ffffff\" loading=\"lazy\"/>\\n<h2>🚀 Anthropic 刚从 Amazon 拿到40 亿美金，又获得 Google 的20亿美金融资</h2>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa926187c0964e25b200572284aab6a2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1654&amp;h=827&amp;s=119805&amp;e=png&amp;b=cc765c\" loading=\"lazy\"/>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.crunchbase.com%2Forganization%2Fanthropic\" title=\"https://www.crunchbase.com/organization/anthropic\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.crunchbase.com/organizatio…</a></p>\\n</blockquote>\\n<blockquote>\\n<p>Anthropic 是一家位于美国加州旧金山的人工智能初创公司，创始团队曾参与过 GPT-2 和 GPT-3 模型的研发，但因为对 OpenAI 的商业化和安全性方面不满，于 2021 年离开并成立了 Anthropic，并推出了大语言模型 Claude。</p>\\n</blockquote>\\n<p>Anthropic 这家公司真的很绝！ShowMeAI 根据 CrunchBase 网站整理了Anthropic 融资路线图，可以看到截至上个月底已经完成了7轮融资，总金额超过55亿美元。</p>\\n<p>而最近，又有消息称 Anthropic 正在与 Google 及其它投资人谈判，预计将至少再融 20 亿美金。目前官方还没有披露相关信息，等一等靴子落地~  <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fslate.com%2Ftechnology%2F2023%2F09%2Fupstarts-like-anthropic-are-making-the-tech-giants-scramble.html\" title=\"https://slate.com/technology/2023/09/upstarts-like-anthropic-are-making-the-tech-giants-scramble.html\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>⋙ 新闻来源</strong></a></p>\\n<blockquote>\\n<p>感谢贡献一手资讯、资料与使用体验的 ShowMeAI 社区同学们！</p>\\n</blockquote>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f3cd1c357e354a4dbf63b8221c11727c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2953&amp;h=886&amp;s=421426&amp;e=png&amp;a=1&amp;b=dbe9ee\" loading=\"lazy\"/>\\n<blockquote>\\n<p>◉ 点击 👀<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fhomepage%3F__biz%3DMzg2OTYyMTcwMw%3D%3D%26hid%3D2%26sn%3D51f7bead52c41447cd0ecb3d57b884e7\" title=\"https://mp.weixin.qq.com/mp/homepage?__biz=Mzg2OTYyMTcwMw==&amp;hid=2&amp;sn=51f7bead52c41447cd0ecb3d57b884e7\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>日报&amp;周刊合集</strong></a>，订阅话题 <strong>#ShowMeAI日报</strong>，一览AI领域发展前沿，抓住最新发展机会！</p>\\n<p>◉ 点击 🎡<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.showmeai.tech%2Ftutorials%2F85\" title=\"https://www.showmeai.tech/tutorials/85\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><strong>生产力工具与行业应用大全</strong></a>，一起在信息浪潮里扑腾起来吧！</p>\\n</blockquote>', 'author': 'ShowMeAI', 'reading_time': 7, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 46, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '5d9e8948cb07bd221b1b68ea4d028fb6becea49aa43610d300eab787f5a1f3a4', 'title': 'LLVM之父Chris Lattner：我的AI基础设施软件构建理念', 'url': 'https://juejin.cn/post/7286035963519205432', 'published_at': '2023-10-06T08:51:36Z', 'created_at': '2023-10-06T17:28:03.282802Z', 'changed_at': '2023-10-06T17:28:03.282802Z', 'content': '<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/352e419c96ee45ac9bc3430213bc4f07~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=687&amp;s=1033268&amp;e=png&amp;b=2f3b20\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><strong>来源｜Latent Space</strong></p>\\n<p><strong>OneFlow编译</strong></p>\\n<p><strong>翻译 | 杨婷、宛子琳</strong></p>\\n<blockquote>\\n<p>如果 AI 如此重要，那么为什么软件如此糟糕？</p>\\n<p>创办 Modular 之前，这是让 LLVM 之父 Chris Lattner 困扰已久的问题。随后，他与 TensorFlow 的前产品同事 Tim Davis 合作研究模块化解决方案，以解决人工智能开发中庞大、单一、分散的平台问题。2022 年，他们以 3000 万美元的种子资金启动 Modular ，继今年 5 月成功推出 AI 引擎 Modular 与编程语言 Mojo，不久前又获得 1 亿美元的 A 轮融资。</p>\\n<p>尽管 Mojo 以其多线程编译的 Python 超集和出色的性能受到瞩目，但这只是一个副业项目，而 Modular 的 Python 推理引擎的愿景同样宏大。</p>\\n<p>Chris在编译器方面的成就卓著。他在博士期间开发了 LLVM，并因此获得了 2012 年 ACM 软件系统奖。随后，他加入苹果，创建了 Clang 和 Swift（取代 Objective-C 的 iPhone 编程语言）。之后，他在 Google 带领 TensorFlow 基础设施团队，并开发了 XLA 编译器以及 MLIR 编译器框架，这时，他的任务不是建立一套最适用于 AI 的编译器，而是为 TPU 构建最佳的编译器，以便所有 TensorFlow 用户都能在 Google Cloud 上获得良好的体验。</p>\\n<p>在他看来，Meta 的 PyTorch 团队也并非在改善所有领域的 AI 性能，而是主要服务于他们的推荐和广告系统。Chris 和 Tim 意识到，大型科技公司并没有将 AI 引擎和开发者体验作为优先考虑的产品，因此，他们认为，Modular 是交付未来的 AI 开发平台的最佳方式。</p>\\n<p>尽管 Chris 是业内公认的顶级编译器工程师，但他并没有将编译器方法简单地应用到 Python 上，相反，他选择了与此完全不同的方法。</p>\\n<p>Modular 的最初目标是构建“统一的 AI 引擎”，以加速 AI 的开发和推理。与 GPU 即是一切的 AI 世界不同，在那里只有“富有 GPU 资源”的人才能受益，而 Modular 将 AI 视为一个大规模、异构化、并行计算的问题。他们希望以一种更加广泛和包容的方式来运行 AI，不仅仅依赖于 GPU ，而是探索利用多种计算资源，以满足不同应用、不同环境和不同需求下的 AI 计算需求。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e11cddc812fe4581944de35704aee8cf~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=369&amp;s=44209&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>作为一个独立的 AI 引擎，Modular 可以在 CPU 上运行所有其他框架，相比原本的性能，它可以将这些框架的速度提升 10% 至 650%（ 即将支持 GPU）。</p>\\n<p>Mojo 是 Modular 的一部分，它是一种全新的 AI 编程语言，是 Python 的超集，它最初作为一种内部 DSL 来提高团队的工作效率而创建。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/61f68cd6541141bc8b26b50b3056f419~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=399&amp;s=74900&amp;e=png&amp;b=f7f7f7\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Mojo 采用了 Python 的语法，同时，它还支持所有现有的 PyPi 包，开发者从 Python 切换到 Mojo 不需要进行任何代码更改。在性能上，Mojo 相比 Python 的运行速度最高快约 68000 倍，而大多数 Python 代码库只需进行很小的调整，转换到 Mojo 后其运行速度就可以提高 10 到 100 倍。</p>\\n<p>Modular 团队正在进行一项艰巨的技术挑战。在 Latent Space 的最新一期播客中，Chris Lattner 解释了他为何开发 Modular 与 Mojo，并分享了他所看到的 AI 基础设施软件发展的挑战和未来。</p>\\n<p>（本文经授权后由OneFlow编译发布，转载请联系授权。原文：<br/>\\n<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.latent.space%2Fp%2Fmodular%23details%25EF%25BC%2589\" title=\"https://www.latent.space/p/modular#details%EF%BC%89\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.latent.space/p/modular#d…</a></p>\\n</blockquote>\\n<p><strong>1</strong></p>\\n<h1>统一的 AI 引擎平台</h1>\\n<p><strong>Alessio：Modular 的目标是构建一个统一的 AI 引擎，这意味着AI 引擎面临的情况是碎片化的。从源头出发，在 AI 研究和开发领域，需要解决问题有哪些？</strong></p>\\n<p><strong>Chris：</strong> 虽然 AI 发展的状况大家有目共睹，但回顾 2015年-2017 年，对于那些追随者来说，AI 领域的发展确实令人惊叹。那个时代的技术主要由 TensorFlow 和 PyTorch 驱动，虽然 PyTorch 稍晚一些出现，但它们在某些方面的设计有点相似。</p>\\n<p>其中的挑战在于，构建这些系统的人主要专注于人工智能、研究、微分方程、自动微分等各个方面，并没有考虑解决软硬件边界问题。因此，他们认为需要一种方法来让人们设置层，类似于 Keras 或 NNModule。</p>\\n<p>在底层，这些被称为算子（operator）的部件起到了作用，因此会得到不同类型的操作，如卷积、矩阵乘法、归约和逐元素操作，那么如何实现这些操作？可以使用 CUDA 和英特尔的数学库（如Intel MKL）进行构建。</p>\\n<p>起初看起来不错，即使只是推出一个新的 Intel CPU 型号，也只能得到很有限的算子。然而现在 TensorFlow 和 PyTorch 已经拥有数千个算子。结果就是每个算子都会有对应的 kernel，这通常由人工手动编写。因此，当引入一款新的硬件时，就需要复现成千上万个 kernel，这使得人们进入硬件领域变得十分困难。</p>\\n<p>另一方面，只有少数人知道这些 kernel 如何运行，而且要掌握这些技能与创新模型架构所需的技能截然不同，因此不便展开研究。</p>\\n<p>当我参与 Google TPU 的开发工作时，最初的目标主要是激励和推动大量的研究。在我加入之前，已经有人提出了一个新颖的思路，那就是使用编译器进行处理，而非手动编写成千上万个 kernel，并重新编写所有算子，试图做到英特尔或者 NVIDIA 那样。</p>\\n<p>相比之下，编译器具有更强的可扩展性和通用性，它能以不同的方式组合 kernel，这其中还有许多重要的优化技术，包括可以大幅减少内存流量的 kernel 融合。如果使用传统的手写 kernel，得到的是人们认为有趣的特定组合，而不是研究人员接下来想要研究的新事物，但很多研究都在尝试探索新的领域。</p>\\n<p>因此，我们押注在编译器上，开发了 XLA 系统，它是 Google 堆栈的一部分，它使得数十亿亿次规模的超级计算机成为可能，并且在此基础上展开了许多令人惊叹的工作。但最大的问题是，它最初是为了支持硬件如 Google TPU 而开发。</p>\\n<p>现在就需要雇佣编译器工程师，而熟悉机器学习和所有这些领域的编译器工程师更是凤毛麟角。从技术角度出发，如果你在 Google 工作并且可以访问相关硬件，就可以对其进行扩展，否则就会面临真正的挑战。</p>\\n<p>因此，我特别喜爱 NVIDIA 平台的一个原因是，虽然人们对 CUDA 抱有各种不满，但如果我们回顾 AI 起飞的那个时刻，如深度学习与 AlexNet 的结合，很多人将这个时刻归功于两个因素：数据（如 ImageNet）和计算能力（强大的 GPU）的结合，这才使得 AlexNet 的诞生成为可能。</p>\\n<p>然而，<strong>他们经常忽略了第三个因素，即可编程性。</strong> 正是因为 CUDA 使研究人员能够创造出之前不存在的卷积 kernel。当时还没有像 TensorFlow 这样的框架，也没有如今的丰富工具。实际上，正是数据、计算能力和可编程性三位一体，才开启了一种新的研究范式，形成了整个深度学习系统的浪潮。</p>\\n<p>因此，我们必须借鉴历史经验，采用模块化的思路，思考如何迈向下一个时代。如何获得人们在算法创新、思想、稀疏性等方面的惊人优势，以及那些研究边缘中可能相关的思想。我们应该如何充分发挥编译器的优势？编译器具有广泛的适用性和强大的可扩展性，能够处理新问题。最后，我们要如何充分发挥可编程性的优势，将所有这些因素融合在一起？</p>\\n<p><strong>Alessio：记得你提过，那些对某些领域不熟悉，无法做出贡献的人会被甩在身后，而 CUDA 在这方面做得很棒。对于类似后模块化世界中的 AI 开发，你有什么看法？</strong></p>\\n<p><strong>Chris：</strong> 我的信念是人类很了不起，但我们无法将所有知识都记在脑中，人们各有所长，如果能让他们协同合作，并理解这些架构的内部原理，就能创造出超越个体能力的成果，实现新的突破。</p>\\n<p>我认为，关键在于如何促进良性循环，如何推动创新？如何让更多了解问题的不同领域的人切实展开合作？这就是为何我们在 Mojo 和 AI 引擎方面的工作如此重要，我们真正致力于消除这个问题的复杂性。</p>\\n<p>许多已构建的系统只是简单地聚合在一起，这就好比一个有用的工具可以帮助我解决问题，然而，它们并没有从上到下经过精心设计。我认为，<strong>模块化提供了一个更简单、更正交、更一致、更有原则的堆栈，使得我们能够消除整个堆栈的复杂性</strong>。但如果你的工作建立在所有这些碎片化的历史基础设施之上，就只能自求多福。</p>\\n<p>此外，尤其很多 AI 研究系统都存在这种预设路径，只有按照准确的演示步骤，才能正常运行。一旦稍作改变，一切就会崩溃，性能会下降甚至停止工作。这就是底层碎片化的产物。</p>\\n<p><strong>Swyx：所以，编译器和语言是人类可以进行合作或跨界的媒介。</strong></p>\\n<p><strong>Chris：</strong> 我从事编译器方面的工作已有很长时间，但我们要从问题出发，如果编译器或编译器技术有助于解决问题，就可以使用，不能本末倒置。</p>\\n<p>你可能会问，编译器有何用处？它的根本目的是什么？这在于你不需要过多地了解硬件。你可以为每个问题编写十分底层的汇编代码，但是编译器、编程语言或者 AI 框架的真正作用是让你能够以更高的抽象级别表达事物，且这一目标具有多重功能。</p>\\n<p><strong>其中一个目标是，让一切变得更容易，另一个目标则是，如果你能够剔除大量无关的复杂性，就会为新的复杂性腾出空间。</strong> 因此，这实际上是减少非核心复杂性，使你能够专注于解决问题的本质复杂性。另一个目标是通过抽象化提供更多的可能性，尤其像我们正在构建的现代编译器，它们无限关注细节。</p>\\n<p>人类不具备这种能力。事实证明，如果你手工编写一堆汇编代码，然后遇到类似的问题，就只需要稍作调整，而无需进行全面的基本原则分析来解决问题，而编译器实际上能够比人类做得更好。</p>\\n<p>更高级别的抽象还会给你带来其他的能力。我认为，深度学习系统与像 Modular 正在构建的系统非常令人兴奋的一点在于，它已经将计算提升到了图形级别。一旦你将计算从混乱的 for 循环和充斥着分号的代码中分离出来，转换为更具声明性的形式，就可以对计算进行转换。</p>\\n<p>这是许多人尚未意识到的一点，某种程度上说，在现有系统中实现这一点非常困难。因为抽象化提供的很大一部分能力就是能进行像 Pmap 和 Vmap 这样的操作，即对计算进行转换。而我在 Google 工作期间获得的灵感之一是，可以从最底层开始，从单节点 GPU 机器到集群，再到异步编程，进行诸如此类的大量基础性工作。</p>\\n<p>当我离开 Google 时，我们已经在 Jupyter Notebook 中使用每秒超过一千亿次的超级计算机进行研究和训练。这在技术方面是一个巨大的飞跃，要归功于很多层次清晰且设计良好的系统，基于许多创新的 HPC 类型硬件以及一系列的突破。因此，我希望看到这项技术得到更广泛地采用、推广并普及，同时也希望它能够解决沿途累积的各种复杂性。</p>\\n<p><strong>Alessio：你提到框架层和硬件层之间的关系。当你们的 AI 引擎首次发布时，它可以在 A100 上实现多少百万亿次浮点运算（petaflops）的性能，但我发现，你们的官网目前披露却都是关于 CPU 的强大性能信息。</strong></p>\\n<p><strong>Chris：</strong> 这正是我们从第一性原理出发开展工作的原因。你必须从头完成所有的工作，如果做得对，就不应跳过任何重要的步骤。很多人认为，如今的 AI 系统就是针对 GPU，都在争论 GPU 的性能，但在我看来，<strong>AI 实际上是一个大规模异构的并行计算问题。</strong></p>\\n<p>因此，AI 的起点通常是数据加载，但 GPU 并不负责加载数据，你必须进行数据加载、预处理、网络传输等一系列工作，然后进行很多矩阵乘等计算。接下来，你还需要进行后处理，并将结果通过网络发送或存储到硬盘上。而事实证明，CPU 是驱动 GPU 所必需的。</p>\\n<p>同时，当你为加速器准备软件时，会发现自己只是在解决问题的一部分，因为他们只将矩阵乘法或他们认为重要的部分当作问题的重点，所以你构建的系统只能做到芯片能做的一部分。</p>\\n<p>你从来没有时间去解决真正的主要问题，所以使用像 TensorFlow 或 PyTorch 这样的工具时，你会发现 CPU 的工作在很大程度上是通过 Python 运行的，比如 tf.data 等。这些工具不可编程，不可扩展，在许多情况下速度非常慢，很难进行分布式处理，非常混乱。</p>\\n<p>因此，现在的 CPU 已经拥有张量核心。它们只是被赋予了一些有趣的名字，类似于 AMX 指令，原因在于以前的 CPU 和 GPU 完全不同。随着时间的推移，GPU 变得更加可编程并更类似于 CPU，而 CPU 则变得更加并行化，我们正处于这种技术的多样化发展过程中。</p>\\n<p>当我们开始模块化时，如果从技术角度来看待这个问题，那么构建一个通用的架构用处很大，因为可以对其进行专用化。</p>\\n<p>正如我在对 XLA 和其他一些堆栈的研究中发现，<strong>从专用化的架构开始再进行泛化非常困难。</strong> 此外，实际情况表明，不同的人在 AI 不同方面花费不同的成本，投入不同的领域，但训练扩大了研究团队的规模，推理扩大了产品和用户群的规模以及其他方面。因此，现如今仍有很多推理任务是在 CPU 上完成的。因此，我们应该从 CPU 开始改进其架构。</p>\\n<p>CPU 也更容易使用，它们不会缺货，并且出于多种其他原因更加容易处理。我们证明了可以构建一个通用架构，在不同加速器系列之间进行扩展。我们还展示了可以处理 Intel、AMD 以及ARM Graviton 等不同类型的处理器，甚至对于 Intel CPU 内部各种奇特组合的情况，也能提供大量支持。</p>\\n<p>针对不同的向量长度和其他复杂操作，我们可以通过更通用和灵活的编程方法提供优于供应商提供的软件。我们正在针对 GPU 进行针对性开发。你将获益于一个经过深思熟虑、层次清晰，并具备有正确 DNA 的堆栈。因此，随着时间的推移，我们将逐步扩展到不同类型的加速器。</p>\\n<p><strong>2</strong></p>\\n<h1>构建 AI 引擎的挑战</h1>\\n<p><strong>Alessio：在我看来，CPU 是人们关注的重点，这就是为什么会看到LLAMA.cpp 等各种技术和方法，大多数人认为通过对模型进行量化，使其能够在 CPU 上运行，但你们的想法更加激进，你们是否重新设计了整个引擎？在方法上有何不同？实际构建引擎的挑战是什么？</strong></p>\\n<p><strong>Chris：</strong> 模块化的特点之一是，用更困难的方式来获得更好的结果。我们团队中的很多人都参与过各种系统的开发，甚至包括行业中创造出 ONNX Runtime 等一些奇特工具或技术的优秀人才。很多人一直从事不同的系统开发工作，而这些系统的挑战在于，其中许多系统是在五至八年前设计的，随着系统的演变，要进行根本性的改变就变得越来越困难。因此，我们决定从头开始，这样做确实更加困难，但我和团队喜欢构建新事物。</p>\\n<p>其实 TensorFlow、PyTorch 等所有这些系统仍然使用与 Caffe 相同的线程池（thread pool）。人们普遍认为，这是一个巨大的漏洞，会导致严重的性能问题，使得推理时的延迟变得极其不可预测。针对这一问题，需要做出一系列非常具体的设计选择，使线程池阻塞并保持同步。</p>\\n<p>就像整个底层的架构出现错误一样。一旦出现这种错误，就无法挽回。因此，我们的线程池假设没有测试可以阻塞。这就需要非常轻量级的线程，直接关系到构建在其上的所有内容。然后会进行到如何表达 kernel，我们仍然希望能够手写 kernel，从 C++ 开始进行原型设计，然后进一步进入 Mojo 阶段。</p>\\n<p>因此，我们构建了一个非常复杂的自动融合编译器，采用了所有最先进的技术，同时超越了现有技术，因为我们知道用户讨厌静态形状的限制和缺乏可编程性。他们不想仅仅局限于张量，例如，许多 LLM 使用不规则张量等等。</p>\\n<p>所以你希望做的是，通过第一性原理，可以将在其他系统中所经历和感受到的所有困扰汇总起来，而以往由于进度安排、各种限制等原因，还从未有机会对此做出任何改变。但现在，我们可以构建正确的东西，使其具备可扩展性。这就是我们所采取的方法。</p>\\n<p>因此，其中很多工作都十分常规，但这是非常深入的设计工程，需要切实了解每个决策的二阶和三阶影响。幸运的是，其中很多内容不再处于研究阶段。</p>\\n<p><strong>Swyx：你非常注重第一性原理的思考方式，但我认为你的原则与大多数人不同。这些洞察力是你多年来研究大量AI工作的结果，具体是什么呢？</strong></p>\\n<p><strong>Chris：</strong> 我不确定自己有一套具体的原则，我们的很多工作是要释放许多硬件的潜力，并以十分易用的方式实现。因此，我们的出发点往往不是启用一个新的功能，而是消除人们难以处理的复杂性，从而达到目标。</p>\\n<p>所以这不是研究，而是设计和工程。关于这一点，我们追求的也是让任何给定的硬件发挥最大的性能。因此，如果你和一家花费了 2 亿美元购买特定内存大小的 A100 GPU 的 LLM 公司交谈，他们想要的是从那块芯片中获得最大的价值，而不是采用最小公约数的解决方案。</p>\\n<p>另一方面，许多人希望拥有更高的可移植性、通用性和抽象性。因此，<strong>挑战在于如何实现并设计一个系统，在默认情况下获得抽象化，而不放弃全部的性能？</strong> 那些针对机器学习等类型的编译器系统，实际上已经放弃了全部的性能，它们只是试图在空间中覆盖某一个特点。因此，拥有并为此设计一个系统，对我们当前的工作非常重要。</p>\\n<p>而在其他方面，则要对用户表示同情，很多对技术痴迷的人忽略了使用技术的人与构建技术的人之间的巨大差异。<strong>在开发者工具中，理解使用工具的开发者并不想过多了解技术，这一点非常重要。</strong></p>\\n<p>在编译器工作中非常有趣的一点是，没有人想要关心编译器。你可能正在开发一个 Mojo 应用程序或 C 应用程序，或者其他类型的应用程序，但你只是希望编译器不会妨碍你，或者在出错时告诉你。只有当编译器变得太慢、出现故障或存在其他问题时，你才会在意它的重要性。</p>\\n<p>因此，AI 技术也是一样。用户在构建和部署模型时有多少时间被用于与工具的斗争，而不是因为它遇到了特殊情况让你偏离了正轨，然后你会从某个工具中获得一堆令人疯狂的 Python 堆栈跟踪信息。因此，对用户的同情心非常重要，尤其是 AI 基础设施还不够成熟，而同情心从来不是工具构建者的核心价值观之一。</p>\\n<p><strong>3</strong></p>\\n<h1>Mojo 的诞生</h1>\\n<p><strong>Alessio：迄今为止最容易理解的应该是 Mojo，它是 Python的超集。我确信这并不是一时兴起，你看到了哪些局限性？</strong></p>\\n<p><strong>Chris：</strong> 当我们展开 Modular 项目时，当时并没有打算要构建一种编程语言。我们只是必须创造一种语言来解决问题。接下来就是处理线程池和其他非常基本的流程。</p>\\n<p>我们如何与现有的 TensorFlow 和 PyTorch 系统集成？事实证明，这在技术上非常复杂。但是接下来进入了更深层次的范畴，即推动硬件加速。我们决定发明一整套非常专业、底层的编译器技术，具备自动融合等功能，其设计是面向云计算的，毕竟，世界上不止一台计算机。</p>\\n<p>由于硬件的复杂性，人类在算法上表现出色，但注意力广度不总是解决问题的关键，这就引出了一些需求。因此，我们构建了这种纯粹的编译器技术并进行验证，以展示我们可以生成性能非常高的 kernel。</p>\\n<p>下一阶段我们手动编写了这些非常底层的 MLIR 代码。当时我们对它很满意，但团队讨厌手动编写这些代码。因此需要一种语法规则。当时可以选择创建一个类似于 Halide 的专用领域嵌入式语言 （Domain-Specific Language，DSL），或者其他解决方案。</p>\\n<p>构建一种编程语言是一种更为困难的方式，但可以得到更好的结果。当时也有 Halide 或者 OpenAI Triton 等类似工具，虽然其演示效果很好，问题在于它们的调试器很糟糕，相关工具也不便于使用，通常情况下，最擅长使用这些工具的人是工具创建者。</p>\\n<p>因此，我们决定构建一门完整的编程语言。我曾构建过 Swift ，因此知道该怎样做，但也明白这是一项艰巨的任务。</p>\\n<p>显然，机器学习社区都是聚焦于 Python 的，深入进行决策时，发现有很多类似 Python 的语言，但它们并没有被广泛采用，并且存在着巨大的问题，可能还会导致社区分裂等后果。因此我们尝试构建一种需要时间来完善的语言，最终它将成为 Python 的超集。</p>\\n<p>实际上，Python 的语法并不是最重要的，关键在于社区，即程序员的经验和技能积累。所以构建一个看起来像 Python，但实际上不是 Python 的语言从来都不是我们的目标。我们最终的目标是通过构建 Mojo 实现质量更高的结果，即便这需要花费更长的时间。</p>\\n<p>如今，Modular 公司拥有最重要的 Mojo 开发者。在构建一门语言时，亲身使用实际上非常重要。这是我们在 Swift 开发过程中犯的一个错误，我们构建 Swift 是为了解决人们不喜欢 Objective-C 语法的问题，但在我们推出之前，还没有内部用户。</p>\\n<p>相比 Swift，Mojo 是更轻量级的语言。实际上，我们正在使用 Mojo，它是驱动引擎中所有内核的动力源。我们认为它对其他人也有价值和吸引力，所以我们将其作为一个独立项目发布。</p>\\n<p>Mojo 总有一天会走向成熟，我们希望能够围绕 Mojo 构建大型社区，为达成这一目标，我们将对 Mojo 开源。这是一件意义非凡的事情，一旦决定要做，我们就希望能够做到最好。</p>\\n<p><strong>Alessio：我们经历了 Python 2 到 Python 3 的一段混乱时期，这是一个所有人都不愿回忆的阶段。Mojo 的一些特性非常优秀，为什么不支持 Python 2？从长远来看，你们计划如何保持这两种语言的同步？如何相互关联？</strong></p>\\n<p><strong>Chris：</strong> Guido 在 Mojo 发布之前就对它有所了解，他与我们团队进行了很多沟通，并且偶尔会在 Discord 上向我们提问。</p>\\n<p>Guido van Rossum（Python 之父）喜欢拿我们开涮，当然，这是一件好事。Mojo 是 Python 大家庭的一员，此外还有 PyPy、Cython 等。我们想成为 Python 大家庭中的一员，希望 Python 能够持续发展并添加新的功能。</p>\\n<p>Mojo 也将如此。这就好比是回到三十年前 C 语言诞生时，之后在 1983 年左右， C++ 突然出现，它是带有类（class）的 C 语言。而 Python 不仅有类，还包括通常在 C 语言下进行的所有操作。最初，C 和 C++ 是两个不同的社区，但是这两个社区进行了很多交流，彼此之间相互影响，导致彼此拥有了对方的一些特征。</p>\\n<p><strong>我希望同样的融合发生在 Python 和 Mojo 之间。</strong> Python 3 与 Mojo 就像是 C++ 与 C 语言。Python 3 由其运行时及一种特定的对象模型定义，而 Mojo 则通过一种更丰富的表达能力来定义，我们拥有精巧的 MLIR 编译器等，因此能够实现类似的表达能力。我希望 Mojo 能够成为 Python 的超集，即在所有功能方面都更加强大，但是这些功能会并行发展。我们的目标不仅仅是添加“Walrus 2”算子等通用功能，而是添加系统编程特性。</p>\\n<p><strong>4</strong></p>\\n<h1>为什么创办 Modular</h1>\\n<p><strong>Swyx：Modular 的灵感来自于 SciFive（一家创造自由开源处理器架构的公司），你曾与一些大型云服务提供商进行过对话，但对方对 Modular 并不感兴趣，最终你是否认为将 Modular 放在大型云平台上并不是最佳选择？你是如何想到要创办这家公司的？</strong></p>\\n<p><strong>Chris：</strong> 2016 年左右，我开始涉足 AI 等多个领域。在特斯拉，我参与了自动驾驶的研发工作；在 Google，我负责了一个硬件项目，并尝试改善 TensorFlow 的架构设计。当时，我对 Google以及它对 PyTorch 的不重视感到不满，所以离开 Google，加入了一家硬件初创公司。但 2020 年（恰好是疫情爆发之前）并不是一个合适的时机，因为当时一切充满了不确定性。PyTorch 仍在摸索中，他们有很多雄心勃勃的项目。</p>\\n<p>当时，我认为 Meta 会解决这些问题，因此我加入了一家硬件初创公司，以了解商业战略、商业化以及公司建设等方面的知识与经验。当时我们的一名软件人员，Tim Davis（现 Modular 联合创始人）也正在探索自己的道路。2017 年，Tim 和我一起加入 Google Brain 并密切合作，我从事数据中心 TPU 方面的工作，而他从事移动端（如 Android 等）的工作；我担任工程师，而 Tim 负责产品方面的工作。在工作上，我俩非常互补，后来，我们开始思考职业发展的下一步，2021 年年中，我们发现 AI 基础设施的问题仍然存在，仍未获得有效解决。</p>\\n<p>因此，我们开始分析 AI 领域的问题，简单来说，如果 AI 对世界如此重要（即便在 ChatGPT 之前），为什么所有的软件都非常糟糕？为什么模型部署如此困难？尽管我们做了大量的工作来简化模型的训练过程，但将其投入生产却仍然非常具有挑战性。</p>\\n<p>我们对这些问题进行了细化，认为世界上的软件可分为三类。<strong>第一种是硬件专有的软件</strong>，比如 CUDA、XLA 堆栈或苹果的神经引擎（CoreML）等。这不是硬件人员的错，因为没有现成的解决方案，他们必须为硬件构建垂直的软件堆栈。但这样做的结果是，在无意间造成了软件领域的分裂以及软件生态的碎片化。</p>\\n<p><strong>第二种是框架软件。</strong> 在框架方面，我们有 TensorFlow、PyTorch、TVM 等，这些框架已经出现八年左右。框架本身是研究性质的，创建于机器学习的不同时代，随着时间的推移以及新的硬件和用例的出现而不断发展演进，但从未根据现有情况进行精心设计。此外，由于 AI 对其所属公司非常重要，因此，这些公司在其中投入了大量资源。框架开发人员相当之多，但却缺乏清晰的愿景。</p>\\n<p><strong>事后来看，理解 AI 在五年后会是什么样子要比预测其形态更容易。</strong> 我们会遇到很多已知的问题，以 PyTorch 为例，它的部署相当困难，不太适用于许多非 NVIDIA 硬件，而且对于语言大模型的扩展性也存在问题。这些都是众所周知的问题，但要从根本上解决这些问题却非常困难。PyTorch 的工程师们非常努力，他们正在努力解决这些问题，但由于所处的环境，修复这些问题确实很困难。</p>\\n<p>硬件导致了软件的碎片化，而框架则与最初的架构绑定。许多人希望能够简化 AI，使其变得简单，因此，MLOps 这个概念得以出现。我认为，很多人试图通过极简化的 API 来实现 AI 的简化。AutoML 就是其中一个例子。</p>\\n<p><strong>第三种是试图在这个庞大而复杂的系统之上增加一层 Python 封装，以简化 AI 操作。然而，通过在上层添加一层 Python 封装无法解决可编程性、性能、硬件能力、新型算法或安全性等核心问题。</strong> 所以，我们决定回归到第一性原理，思考导致这一混乱的根本原因。我们认为，<strong>这主要是因为缺少统一的接入平台</strong>，因此，我们决定从最基层开始构建软件，以解决这个棘手问题。</p>\\n<p><strong>Alessio：你的职业生涯很长，是一位令人惊叹的工程师。突然之间你就创办了自己的公司，成为了 CEO，并承担着产品负责人和融资负责人的角色，同时还要负责建设团队、指导员工等，在这些方面，你有哪些经验和教训？</strong></p>\\n<p><strong>Chris：</strong> 在 Modular，我和联合创始人 Tim 之间有着非常紧密的关系，我们能够彼此互补，有一个志同道合的合伙人非常重要，这是我在 Google或 Apple 工作时所没有的体验。另外，我曾在他人的框架下建立过很多团队、产品和技术。而现在，我们有了自己的框架，这让我们不再受制于其他框架存在的问题。</p>\\n<p>做任何事情之前，我们都要从第一性原理出发进行思考。我们理解 AI 领域所面临的问题、理解 AI 从业者所经历的痛苦，这也是我们成立 Modular 的出发点。Tim 在 Google 工作时，对这种痛苦也深有感受，在初创公司工作和在 Google是两种非常不同的感受。</p>\\n<p>在创办公司时，我们分工合作。作为工程领导者，我主要负责工程构建、建立工程团队等，而 Tim 则负责产品和业务方面的工作，他访谈了上百家不同的公司，了解这些公司的痛点和他们面临的挑战，以便更好地向这些公司提供帮助。通过这些深入访谈，我们真正明确了公司的发展愿景，并将同事真正地团结在一起。</p>\\n<p>Modular 面临的挑战是，我们正试图解决一项非常困难的技术问题，而且相当抽象。尽管现在项目已经开始运行且各部分正在合作，我们可以公布一些成果，但要解决这个问题需要花高价聘请大型科技公司的专家，这在很大程度上决定了我们公司的最初状态以及我们思考问题的出发点。</p>\\n<p>当我们以第一性原理思考，发现出于上述原因，必须筹集大量资金、激励员工、提供丰厚薪酬等，此外还要提供一个舒适的工作环境，上述需求塑造了我们的行事风格。这个过程非常有趣。</p>\\n<p>回头来看，<strong>TensorFlow 或 PyTorch 是否是产品呢？我的答案：不是。</strong> 但我也要自我反思，我曾经参与过 Swift 或 Xcode 等项目，从某种意义上，它们算是产品，其中有产品经理和一个工程团队参与开发，并会将其交付给客户，但它们并非公司的核心产品。Xcode 并不赚钱，苹果没有从中获利，它与客户之间的联系是间接的，有一定的独立性。这容易导致团队或 TensorFlow、PyTorch 等支持团队的研究项目与客户的需求脱节。</p>\\n<p>Modular 团队直接面向客户，我们了解客户所历经的痛苦和需求。众所周知，在 AI 领域，构建和部署这些东西非常困难。当然，你可以在顶层加入一层 Python，这种做法看似简单，但这并非领先公司和构建这些东西的顶级人才所面临的痛点。</p>\\n<p>问题在于，他们周围存在很多无用之物，<strong>而我们的愿景之一就是将这些东西都统一起来，利用更少的东西，发挥更大的作用。</strong> 我们的愿景源于与一些团队的交流，他们所面临的问题是，由于需求的不断演进，他们构建的产品在不断变化，这些公司使用了多个模型，这导致与我们合作的团队通常都相当复杂，为了各种不同的用例，这些公司演变出很多混乱的系统，造成很大麻烦，因此常常会有如下疑问：我是否需要大量工程师来部署这个模型？为什么需要这样做？</p>\\n<p><strong>Swyx：有一个地方我比较困惑，你提到 Modular 没有云服务，但人们是通过你们的云服务来使用 Modular 推理引擎，也就是说 Modular 有云工程师。</strong></p>\\n<p><strong>Chris：</strong> 我们确实有云工程师。实际上，我们的产品是在用户的云平台上使用的。我们提供了一个 Docker 容器，使得 Modular 的推理引擎可以在云端、本地服务器或笔记本电脑上运行。</p>\\n<p>Modular 的设计理念非常重视模块化，用户可以根据自己的需求使用各种不同的平台。尽管一些用户不想自己管理，但 Modular 团队真正关注的是如何满足用户的需求，因此我们打算逐渐构建一个托管产品以为用户提供更多方便。在我们看来，如果要求用户将所有内容都迁移到 Modular 的云平台上，对于那些不愿意自己管理的用户来说是有价值的，但这会减缓技术的采用速度。</p>\\n<p><strong>Swyx：你有大量搭建团队和招聘工程师的经验，在这方面你拥有独特的优势，人们都很乐意与你一起工作。在工程管理方面，你有什么经验、建议想要分享？</strong></p>\\n<p><strong>Chris：</strong> 我的工作是帮助团队取得胜利，我会尽一切努力去达成这一目标。有些人认为，想要获胜是理所当然的事情，但你必须先搞清楚什么是获胜，也就是说，团队要有一个清晰的愿景、明确的目标，团队成员之间要彼此协调，特别是，当你有一群非常优秀的人，他们都希望在自己的工作中大放异彩。如果成员之间目标一致，那我们就能快速取得进展，而如果成员之间彼此对立，就会影响我们的工作进程。</p>\\n<p>我喜欢凡事亲力亲为，从头做起。这样做不仅能直接向团队成员展示如何做事，同时也塑造了我们的团队文化。</p>\\n<p>效率对团队来说非常重要，如果你只是随便坐在哪里，花24小时或者三周来等 CI 运行，这会拖慢一切。这意味着测试策略，意味着这些核心软件工程问题非常重要。一旦在团队中营造出独立、不依赖第三方等文化，你就能吸引很多优秀人才，你需要在招聘和后续工作中发掘这些人才的潜力。我深信，一个真正优秀的人能够解决任何问题，并从中取得进步。</p>\\n<p>如果你有非常优秀并充满热情的同事，并让他们从事真正想做的事情，那么他们就会拥有超能力。所以，我们需要确保团队成员处理正确的问题，培养他们做事和解决问题的能力，帮助他们成长。尤其是在一个高速发展的团队当中，你不仅要着眼于具体事务，还要查看自己是否作出了贡献等。</p>\\n<p>通常来说，人们会非常关注产品，关注客户和客户所面临的问题，但如果没有团队，就无法解决问题和构建产品。</p>\\n<p>上述问题最终会形成良性循环，对于领导者和团队构建来说，以上问题至关重要。这也是我喜欢 Tim，并且能和他愉快共事的原因之一，他能有效弥补我不擅长的地方，我们彼此都能从对方身上学到很多。</p>\\n<p>总之，要打造一个团队，我们需要做的事情还有很多。我喜欢构建事物，但不会为此放慢脚步。</p>\\n<p><strong>5</strong></p>\\n<h1>关于 AI 领域的发展</h1>\\n<p><strong>Alessio：在 AI 领域，有什么是已经实现，但你曾认为需要花费更长时间的东西？</strong></p>\\n<p><strong>Chris：</strong> ChatGPT是对用户界面的一种创新，它的爆发式发展让人们意识到了 AI 的力量。但回顾过去，我本来认为 AI 进入公众视野还需要好几年时间。</p>\\n<p><strong>Swyx：在 AI 领域解决的问题中，你最感兴趣的是什么？</strong></p>\\n<p><strong>Chris：</strong> 不同的人对 AI 有着不同的定义，一些人认为，一切都应该用端到端的神经网络来实现，而不是软件。我认为，训练算法和智能设计算法之间的平衡是一个开放式问题，个人而言，这并非是一个非此即彼的选择，比如，如果你想要构建一个猫咪检测器，那么使用卷积神经网络是一个非常好的方法，但如果你想编写操作系统，那么应该选择循环语句。</p>\\n<p>但随时间推移，这些技术会发生何种变化，我们该如何让应用开发者更加一致地思考这些问题，而不局限于A类或B类的范畴等，这些都是开放式问题。</p>\\n<p>作为软件开发方法的 AI 最终将成为人们思考构建应用的工具之一。这里的应用不仅仅局限于 iPhone 应用等，而是云服务、数据管道等等，涉及到一系列复杂的环节，最终构建出用户可用的产品。我认为，AI 行业仍处于发展青春期，我们尚未完全掌握这一点。</p>\\n<p><strong>Alessio：有趣的是，ChatGPT的出现还不到一年，而我们已经从谈论 AI 安全问题发展到了 AI 将会毁灭世界，而这一切仅仅是因为一个聊天机器人的出现，这不得不令人深思。</strong></p>\\n<p><strong>Chris：</strong> 不得不承认，2017 年的时候已经有很多人关注 AI 安全问题了，但当时我对此非常不解，我不认为这个问题有什么可讨论的，但事实证明，这些人很有远见。</p>\\n<p>这也是我欣赏 Geoffrey Hinton 等人的原因之一，他们在 AI 走进大众视野之前就进入了这一行业，见证并融合了更长的时间跨度，能以一种更全面的角度来看待问题，并取得更大的成就。</p>\\n<p><strong>欢迎 Star、试用 OneFlow 最新版本：</strong></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FOneflow-Inc%2Foneflow%2F\" title=\"https://github.com/Oneflow-Inc/oneflow/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/Oneflow-Inc…</a></p>', 'author': 'OneFlow一流科技', 'reading_time': 29, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 45, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '3f326f28a13450357adbf5a0c16bc7918995438547bf02fa70075296be9288af', 'title': '44.83%！ChatGPT的艺术风格识别是这个水平', 'url': 'https://juejin.cn/post/7285724223594283065', 'published_at': '2023-10-05T23:17:51Z', 'created_at': '2023-10-06T07:12:57.911298Z', 'changed_at': '2023-10-06T07:12:57.911298Z', 'content': '<p>原文：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJ9eX-Vj9QXlmgeCl645kzA\" title=\"https://mp.weixin.qq.com/s/J9eX-Vj9QXlmgeCl645kzA\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">44.83%！ChatGPT的艺术风格识别是这个水平</a></p>\\n<p>在\\xa0<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5NzY0NjY1MA%3D%3D%26mid%3D2247485512%26idx%3D1%26sn%3D8d4e0a730c5b0fbd73163d20dabba863%26scene%3D21%23wechat_redirect\" title=\"https://mp.weixin.qq.com/s?__biz=MzU5NzY0NjY1MA==&amp;mid=2247485512&amp;idx=1&amp;sn=8d4e0a730c5b0fbd73163d20dabba863&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT“与图对话”初体验</a>\\xa0文中，我们对ChatGPT的艺术鉴赏能力做了初步的测试，其结论是“艺术风格识别：一半一半”，具体来讲：</p>\\n<blockquote>\\n<p>对于风格强烈的画风（比如立体主义、现实主义等）识别率高；对于风格不那么强烈或者比较细分的流派（比如风俗画，被误认为是洛可可），识别率低。</p>\\n<p>因此在艺术鉴赏方面，ChatGPT还需要进一步提高。</p>\\n</blockquote>\\n<p>有读者留言表示对这部分比较感兴趣，希望能了解更多信息；我自己对ChatGPT的艺术鉴别挺好奇，它到底能多大程度上理解不同的艺术流派和风格，于是有了进一步的探索：我想定量测试一下ChatGPT对不同艺术风格的识别率。</p>\\n\\n<h3><strong>方法与过程</strong></h3>\\n<p>正好前不久看到一篇非常有意思的文章：</p>\\n<p>Exploring Art History with Midjourney Prompts</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fnftartwithlauren.com%2Fexploring-art-history%2F\" title=\"https://nftartwithlauren.com/exploring-art-history/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">nftartwithlauren.com/exploring-a…</a></p>\\n<p>顾名思义，用Midjourney提示词探索艺术史：作者让ChatGPT罗列出了59种（文中最后有图片呈现的风格是58种）不同时期流行的艺术风格，然后让Midjourney（现在最流行的文字生成图片的AI之一）以一个芭蕾舞者为主题生成不同风格的图片，从而完成一次对艺术史的视觉探索。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0855d1dabd7547b1af3035a9ab8e6e9c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=878&amp;s=206590&amp;e=png&amp;b=fefefe\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>我觉得这是一次非常有创意的艺术教育历程，可以非常直观地感受各种各样艺术风格的流变，从古埃及到现在：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4e65b957d1714a61bbb586eb94046e7c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=434&amp;s=581515&amp;e=png&amp;b=f2e8e7\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>在这个工作的基础上，我将Midjourney画的不同风格的画给ChatGPT看，让它识别是哪一种艺术风格，将其识别结果与Midjourney提示词种设定的风格做对比，从而得到一个ChatGPT艺术风格识别能力的定量描述。相当于让Midjourney扮演考官，给ChatGPT出题，做对打勾 ✅，做错打叉 ❌，最后评个分，看看及格不及格。</p>\\n<p>下面是我与ChatGPT的对话：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a063176443364f738f84f27ea116f065~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=921&amp;s=463236&amp;e=png&amp;b=2e2f38\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>其Prompt为：</p>\\n<blockquote>\\n<p><em>I will give you some pictures of a beautiful girl each time, and the pictures are in different art styles listed following. Which kind of art style does each uploaded picture of the beautiful girl belong to? Give the answer in the format: Picture {#NO} -&gt; {#STYLE}. The STYLE should be exactly one of the listed kinds of art style I give to you. If you can not tell which one, the STYLE should be &#39;unknown&#39;. The listed art styles are (separated by ;): Egyptian Art;Greek Art;Roman Art;Byzantine Art;Romanesque Art;Gothic Art;Early Christian Art;Islamic Art;Carolingian Art;Ottonian Art;Pre-Romanesque Art;Insular Art;Minoan Art;Hiberno-Saxon Art;Mycenaean Art;Viking Art;Celtic Art;Coptic Art;Macedonian Art;Persian Art;Etruscan Art;Chinese Art;Japanese Art (Heian period);Indian Art (Gupta period);Pre-Columbian Art;Rococo;Neoclassicism;Romanticism;Genre Painting;Portraiture;Hudson River School;Realism;Pre-Raphaelite Brotherhood;Impressionism;Post-Impressionism;Symbolism;Art Nouveau;Orientalism;Landscape Painting;Fauvism;Cubism;Expressionism;Academic Art;Dada;Surrealism;Abstract Expressionism;Pop Art;Op Art;Minimalism;Conceptual Art;Stukism;Photorealism;Street Art;Digital Art;New Media Art;Hyperrealism;Post-Internet Art;Insta-Art/Social Media Art</em></p>\\n<p>中文翻译：</p>\\n<p><em>每次我会给你一些美女的图片，这些图片采用下列的不同艺术风格。每张上传的美女图片分别属于哪种艺术风格？请以这种格式回答：图片 {#编号} -&gt; {#风格}。风格应该是我给你列出的艺术风格中的一个。如果你无法判断，风格应为“未知”。</em></p>\\n<p><em>列出的艺术风格如下（以分号分隔）：</em></p>\\n<p><em>埃及艺术；希腊艺术；罗马艺术；拜占庭艺术；罗曼式艺术；哥特式艺术；早期基督教艺术；伊斯兰艺术；查理曼艺术；奥托尼安艺术；前罗曼式艺术；岛屿艺术；米诺安艺术；希伯尼亚-撒克逊艺术；迈锡尼艺术；维京艺术；凯尔特艺术；科普特艺术；马其顿艺术；波斯艺术；伊特鲁里亚艺术；中国艺术；日本艺术（平安时代）；印度艺术（古普塔时代）；前哥伦布艺术；洛可可；新古典主义；浪漫主义；题材画；肖像画；哈德逊河派；现实主义；前拉斐尔兄弟会；印象派；后印象派；象征主义；新艺术；东方主义；风景画；野兽派；立体主义；表现主义；学院派艺术；达达派；超现实主义；抽象表现主义；波普艺术；Op艺术；极简主义；观念艺术；斯图基斯姆；摄影艺术；街头艺术；数字艺术；新媒体艺术；超现实主义；后互联网艺术；Insta-Art/社交媒体艺术。</em></p>\\n</blockquote>\\n<p>在此之前，我先和它进行了如下对话：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7799193d5d114c33ba1b6ac5138ec9cd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=648&amp;s=115168&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/755af00aa51e41b59db23b376cb2041d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=731&amp;s=211811&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>这样做的目的是确认ChatGPT知道这些艺术风格具体是什么。</p>\\n<p>整个过程总结如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0f6d22b7b3d34dbcafedc65fe38372b3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=572&amp;s=43416&amp;e=png&amp;b=ffffff\" alt=\"图片\" loading=\"lazy\"/></p>\\n\\n<h3>** **</h3>\\n<h3><strong>结果与分析</strong></h3>\\n<p>因为ChatGPT每次对话现在最多上传4张图片，58张不同风格的图片要经历15轮对话才能识别完成：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/600cf4a7396148f89959695c03f993a0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=909&amp;s=461869&amp;e=png&amp;b=31323c\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>最后将结果汇总到下面的“艺术风格识别.excel”文件中：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/24326841554744bebce5b1c4d06fb742~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=833&amp;s=405205&amp;e=png&amp;b=fdfcfc\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>然后再借助ChatGPT的“Advanced Data Analysis”（原Code Interpreter）功能（彻底变懒了，Excel公式都不愿意敲），让分析出识别率：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4741a96ba5f4481db07d20b48002f60f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=263&amp;s=90513&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e6850489de2a493a8386428b16eb1c12~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=896&amp;s=214891&amp;e=png&amp;b=444654\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><strong>44.83%</strong> ！58种艺术风格，ChatGPT能正确辨认的只有26种，接近开头所述“一半一半”的影像。</p>\\n<p>进一步分析得到容易被其他风格误识别的五种风格：</p>\\n<ol>\\n<li>浪漫主义 (<a href=\"https://link.juejin.cn?target=\" title=\"\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Romanticism</a>) - 3次</li>\\n<li>印象派 (<a href=\"https://link.juejin.cn?target=\" title=\"\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Impressionism</a>) - 3次</li>\\n<li>数字艺术 (Digital Art) - 2次</li>\\n<li>米诺安艺术 (Minoan Art) - 2次</li>\\n<li>立体主义 (Cubism) - 2次</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46f70b926ed24e259c09fa8cc2b919e4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=527&amp;s=142475&amp;e=png&amp;b=41434f\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>其分布图</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ac45e7e35616488b88ec0462746b1532~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=955&amp;s=106008&amp;e=png&amp;b=ffffff\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>ChatGPT分析的误识别可能原因包括：</p>\\n<ol>\\n<li>\\n<p>训练数据的问题：如果训练数据中某些风格的样本数量很少或不均衡，那么模型在识别这些风格时可能会出现问题。</p>\\n</li>\\n<li>\\n<p>模型复杂度：简单的模型可能无法捕捉到各种艺术风格之间的细微差异。</p>\\n</li>\\n<li>\\n<p>特征选择：用于训练模型的特征可能不是最佳的，可能需要考虑更多或不同的特征。</p>\\n</li>\\n<li>\\n<p>艺术风格的模糊性：某些艺术风格可能在某些方面与其他风格非常相似，这可能导致模型在识别时出错。</p>\\n</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b4c46084562c4669b71dbf7e2eeebed9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=412&amp;s=185328&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>我仔细看了一下识别错误的结果，确实可以得到一些印证：</p>\\n<ol>\\n<li>不同艺术风格的相似性。比如科普特艺术和古埃及艺术，本身就有继承性：</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/776d46b837ba4d03ad520eb2751b43ad~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=580&amp;s=1097441&amp;e=png&amp;b=876848\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>还比如印象派 vs 后印象主义，我是看不出有什么太大的不同：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/899bff7bc6d7437ab95918a4a1e9d277~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=577&amp;s=842317&amp;e=png&amp;b=e2d3cb\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>还有这个让人抓狂的“超写实主义” vs “照相写实主义”，你能分辨？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/abb59d13303a4fc79b22a830d83c38b7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=575&amp;s=705342&amp;e=png&amp;b=2b2826\" alt=\"图片\" loading=\"lazy\"/></p>\\n<ol>\\n<li>ChatGPT就是不认识这种艺术风格。比如Dada艺术，尽管我试了非常多次，它都会被识别成立体主义，这两者之间的风格差异还是非常大的。</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3b34e8d64a0e41dc9681ff7eb1a3a9fe~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=570&amp;s=894282&amp;e=png&amp;b=e3d8ce\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>还比如伊斯兰艺术，总是会被误识别成印度艺术（古普塔时期）这种明显不同的艺术：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/502c4163016e4ff7b65d2e6c8deaf1f5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=580&amp;s=896948&amp;e=png&amp;b=c1a075\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>这可能是训练数据的问题，也有可能是模型复杂度的问题，前者可能性更大。</p>\\n\\n<h3><strong>总结</strong></h3>\\n<p><strong>44.83%</strong> ！</p>\\n<p>这是ChatGPT现在对58种艺术风格识别的准确性。这个数字可能并不精确，可以多跑几次取平均值，不过大致应该就是这个水平。</p>\\n<p>ChatGPT对艺术风格的识别肯定是超过一般人，但离高水平的艺术史家，还有一段长长的微调的路要走。</p>\\n<p>【完】</p>\\n\\n<p><strong>【往期相关】</strong></p>\\n<p><strong><a href=\"https://link.juejin.cn?target=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5NzY0NjY1MA%3D%3D%26mid%3D2247485512%26idx%3D1%26sn%3D8d4e0a730c5b0fbd73163d20dabba863%26chksm%3Dfe51041dc9268d0b888bf86daf1fe46efbe455acf27758bc9a113d3ace2e03c799be1389e541%26scene%3D21%23wechat_redirect\" title=\"http://mp.weixin.qq.com/s?__biz=MzU5NzY0NjY1MA==&amp;mid=2247485512&amp;idx=1&amp;sn=8d4e0a730c5b0fbd73163d20dabba863&amp;chksm=fe51041dc9268d0b888bf86daf1fe46efbe455acf27758bc9a113d3ace2e03c799be1389e541&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT“与图对话”初体验</a></strong></p>\\n<p><strong><a href=\"https://link.juejin.cn?target=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5NzY0NjY1MA%3D%3D%26mid%3D2247485380%26idx%3D1%26sn%3D9416eb1a621c962e10a54eecf64fb207%26chksm%3Dfe510b91c9268287912b120aad13f44697722c5af6d588771748e6c35c2d2fd1ee0e527ff773%26scene%3D21%23wechat_redirect\" title=\"http://mp.weixin.qq.com/s?__biz=MzU5NzY0NjY1MA==&amp;mid=2247485380&amp;idx=1&amp;sn=9416eb1a621c962e10a54eecf64fb207&amp;chksm=fe510b91c9268287912b120aad13f44697722c5af6d588771748e6c35c2d2fd1ee0e527ff773&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT发布语音和图像能力重大更新：要点解读、应用畅想与原文翻译</a></strong></p>', 'author': '无人之路', 'reading_time': 8, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 44, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '4c4c3d9489f4739e8a40718252e3b91acd20fd1a7cee0f04885ce1d6f6279765', 'title': '了解基于 Elasticsearch 的站内搜索，及其替代方案', 'url': 'https://juejin.cn/post/7285763028603650048', 'published_at': '2023-10-05T08:58:22Z', 'created_at': '2023-10-05T10:47:03.87627Z', 'changed_at': '2023-10-05T10:47:03.87627Z', 'content': '<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/855a36175d644bf198199c223b7c6a51~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=900&amp;h=383&amp;s=308093&amp;e=png&amp;b=2353d6\" alt=\"了解基于Elasticsearch 的站内搜索，及其替代方案.png\" loading=\"lazy\"/></p>\\n<p>对于一家公司而言，数据量越来越多，如何快速去查找这些信息是一个很难的问题，在计算机领域有一个专门的领域IR（Information\\xa0Retrival）研究如何获取信息，做信息检索。在国内的如百度这样的搜索引擎也属于这个领域，要自己实现一个搜索引擎是非常难的，不过通过ElasticSearch就可以构建自己的站内搜索引擎。</p>\\n<h3>一、什么是基于Elasticsearch的站内搜索？</h3>\\n<p>基于Elasticsearch的站内搜索可增强网站的搜索功能，Elasticsearch是一种开源分布式搜索和分析引擎，为网站访问者提供快速准确的搜索结果。通过实施基于Elasticsearch的站内搜索，网站所有者可以大大改善用户体验并增加网站的参与度。</p>\\n<h3>二、使用Elasticsearch进行站内搜索的好处</h3>\\n<p><strong>1.提高搜索准确性和相关性</strong></p>\\n<p>使用Elasticsearch进行站点搜索的关键优势之一是其能够提供高度准确且相关的搜索结果。它采用高级排名算法，以考虑到关键字相关性，文档受欢迎程度和用户行为等因素，以确定给定查询的最相关结果。</p>\\n<p>Elasticsearch还支持模糊的匹配和部分匹配，即使用户犯了拼写错误或仅记住关键字的一部分，也可以找到相关的结果。这样可以确保用户能够快速，轻松地找到自己寻找的东西。</p>\\n<p><strong>2.更快的搜索速度</strong></p>\\n<p>搜索速度对于在网站上提供无缝的用户体验至关重要。\\xa0Elasticsearch凭借其分布式架构并优化了索引和查询机制，即使处理大量数据，Elasticsearch也可以提供闪电般的搜索性能。</p>\\n<p>Elasticsearch通过使用倒置索引来实现此速度，这使其可以快速找到与给定查询相匹配的文档。它还利用缓存技术将经常访问的数据存储在内存中，从而减少了磁盘I/O操作的需求并进一步提高搜索速度。</p>\\n<p><strong>3.扩展性和灵活性</strong></p>\\n<p>网站通常会在流量和内容更新中遇到波动。\\xa0Elasticsearch的分布式体系结构使其具有很高的可扩展性，并能够处理增加搜索负载而不会牺牲性能。随着网站流量在特定时期内的增长或高峰，可以将其他节点添加到Elasticsearch群集中以平均分配工作负载。</p>\\n<p>此外，Elasticsearch在数据建模和示意性索引方面具有灵活性。这意味着网站可以随着时间的流逝而随着内容的发展而轻松调整其搜索功能。可以将新字段添加到索引中，而无需进行任何架构修改或停机时间。</p>\\n<h3>三、Elasticsearch的替代方案</h3>\\n<p>在实施站内搜索功能时，Elasticsearch长期以来一直是许多网站主、开发人员和数据分析师的首选解决方案。但是，现在有一种替代方案也是有很多的优势和好处的。</p>\\n<p>\\xa0<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2beb596fba2e4c5e94427b7575bca529~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1843&amp;h=887&amp;s=159918&amp;e=jpg&amp;b=7f7f7f\" alt=\"\" loading=\"lazy\"/></p>\\n<p><strong>HelpLook</strong>可以说是Elasticsearch的很优秀的替代品之一。它为网站所有者提供了简化的设置和配置过程，使实施强大的站内搜索功能比以往任何时候都更容易。使用HelpLook，你可以快速启动并运行，而<strong>无需广泛的技术知识或专业知识</strong>。</p>\\n<p>\\xa0\\nHelpLook作为Elasticsearch替代方案的主要优点之一是其<strong>成本效益</strong>。Elasticsearch的设置和维护相对来说是比较贵的，但HelpLook提供了更实惠的解决方案，而且在功能方面也不会差。</p>\\n<h3>总结</h3>\\n<p>虽然Elasticsearch长期以来一直是一流站点搜索功能的代名词，但HelpLook凭借其简化的设置流程，成本效益脱颖而出。无论您是开发人员、数据分析师还是仅需要强大的搜索功能的人，HelpLook是不错的选择。</p>', 'author': 'looklook同学', 'reading_time': 3, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 43, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'dfabacc7d5d5236ae5f3e574c14b79b927e078e200eac167e7ed56d7c03a1744', 'title': 'Midjourney 要完蛋了吗？DALL·E 3 效果如何？', 'url': 'https://juejin.cn/post/7285346707138658358', 'published_at': '2023-10-05T08:34:42Z', 'created_at': '2023-10-05T10:47:03.843107Z', 'changed_at': '2023-10-05T10:47:03.843107Z', 'content': '<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/191a655ea28f4fabac9331fa09bfec7e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=482427&amp;e=png&amp;b=2d1f17\" alt=\"\" loading=\"lazy\"/></p>\\n<p>OpenAI 已经为部分 ChatGPT Plus 用户开放了 DALL·E 3 模型。从此，Plus 用户在 ChatGPT 内部就可以进行 AI 绘图啦。</p>\\n<p>免费 ChatGPT 可以在 Bing 的网站上使用预览版，如果你还未曾使用过的话，可以在浏览器上输入 bing.com/create。新用户可以免费生成 100 张图片，超过 100 张的话也是免费的，只是图片生成的速度会慢些。</p>\\n<p>我非常好奇 DALL·E 3 和 Midjourney 这两个 AI 绘图工具的差异，于是深度测试了一下。我主要围绕以下几个维度展开：人物姿态效果，人物构图能力，前景背景分离能力，文本生成能力，悬空，画面布局能力，动作效果，超现实能力这几个维度展开。这几个维度基本上是 Midjourney 非常薄弱的方面，通过下面要展示的图片，我发现两者之间存在的不可弥合的巨大差异。展示的图片是我经过几轮的生成，并挑选出效果最好的进行对比。</p>\\n<h2>对比</h2>\\n<h3>人物姿态效果</h3>\\n<p>我在以往的绘图中发现，Midjourney 对于人物姿态的理解真是弱爆了。如果用非常精准的提示词描述人物的姿态，那生成的图片一定让你感到失望。</p>\\n<p>比如在下面的提示词中，我非常精准地描述了一位芭蕾舞者优美的姿势。</p>\\n<blockquote>\\n<p>Prompt: A ballet dancer showcasing exceptional athleticism and grace. En pointe on her right foot, her left leg extends seamlessly upward in a perfect straight line, toes elegantly pointed. Her torso and head gracefully lean back, face tilted upward as if reaching for an unseen light. Arms outstretched and upward, reflecting the fluidity and poise of her dance. (芭蕾舞者展现出非凡的运动能力和优雅气质。她右脚点地，左腿完美地向上伸展，脚尖优雅地指向前方。她的躯干和头部优雅地向后仰，脸部向上倾斜，仿佛在寻找一束看不见的光。双臂向上伸展，体现出她舞蹈的流畅和优雅。)</p>\\n</blockquote>\\n<p>第一张图是 Midjourney 生成的。图中可以看到，从服装上看她并不是非常专业的芭蕾舞女，而且也没有翩翩起舞，动作比较僵硬。</p>\\n<p>第二章图是 DALL·E 3 生成的，从服装上就能看出她是非常典型的芭蕾舞女。每一个姿势都符合我在提示词里要求的。整体画面简洁，灯光效果也很好地突出了她优美的线条。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8a38770cf69d44cb9548fd2f0a99ea9d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=457904&amp;e=png&amp;b=31271d\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3c145869fd024b8596dad8b6ca4e77de~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=42972&amp;e=jpg&amp;b=eeeeee\" alt=\"\" loading=\"lazy\"/></p>\\n<p>我们再来看一个例子，从这个例子中你会发现 Midjourney 竟然连最基本的瑜伽姿势都不懂。</p>\\n<blockquote>\\n<p>Prompt: a young woman gracefully balancing on a city rooftop at sunrise with the warrior 1 yoga pose (日出时分，一位年轻女子在城市屋顶上优雅地保持着战士 1 号瑜伽姿势的平衡)</p>\\n</blockquote>\\n<p>第一张图是 Midjourney 生成的，我已经选了最好的一张，至少手臂是向上伸展的。其他的三张要么就是原地站立，要么就是动作扭曲。</p>\\n<p>第二张图是 DALL·E 3 生成的。她的瑜伽姿势非常标准，虽然人物的服装没有 Midjourney 生成的那么精细。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa5fa243cd94457e98127d1e4a224f62~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=397727&amp;e=png&amp;b=e2d1bf\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/303add295a624121b54b13e6384dba0d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=77080&amp;e=jpg&amp;b=ead1b8\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>人物构图能力</h3>\\n<p>我们经常需要生成一张包含多人的图片，其中需要指定人物的位置。比如下面的提示词：</p>\\n<blockquote>\\n<p>Prompt: sunlit ballet studio, poised 20-year-old ballerina on tiptoes, perfect ballet posture, extended arms in a soft curve, joyful expression, three 5-year-old girls sitting on polished wooden floor, gazing up in wonder and admiration (阳光照耀下的芭蕾舞室，20 岁的芭蕾舞演员踮起脚尖，完美的芭蕾舞姿态，伸展的双臂划出柔和的弧线，表情愉悦，三个 5 岁的小女孩坐在光洁的木地板上，仰望着，惊叹不已。)</p>\\n</blockquote>\\n<p>前两张图片是 Midjourney 生成的。我尝试了两种宽高比，都无法把提示词中的四个人物纳入到图片中。最后一张是 DALL·E 3 生成的，基本上完美再现了我提示词中的内容，令人惊艳。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b0932762d6a6436384d990f1f0b49cc6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=466795&amp;e=png&amp;b=362511\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c89d5c1f7e204406b090eb1bb639a806~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=545997&amp;e=png&amp;b=34251d\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/17dd0e586317416da2d034c6a63d1615~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=96199&amp;e=jpg&amp;b=e6d9cd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>我们再来看个例子，一张包含三个人的照片，这是用于生成图片的提示词。</p>\\n<blockquote>\\n<p>Prompt: a portrait photograph featuring three individuals, with the woman positioned at the top, making a basketball throw while seated on the shoulders of the two men beneath her. (这是一张三人肖像照片，其中妇女坐在最上面，坐在下面两名男子的肩膀上投掷篮球。)</p>\\n</blockquote>\\n<p>第一张图片是 Midjourney 生成的。我在 Midjourney 中无论如何也无法做到让女人坐在两个男人的肩膀上。这已经是最接近提示词的图片了。</p>\\n<p>第二张图片是 DALL·E 3 生成的。看上去有些古怪，尤其是他们的眼神光。但无论如何，人物的构图和姿势达到要求了。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/441b1aa1b2c140ebb8870e65fbe28987~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=548179&amp;e=png&amp;b=090a08\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3a25643350bd4a82abe82bcdd42b65ed~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=98351&amp;e=jpg&amp;b=473b39\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>前景背景分离能力</h3>\\n<p>不知道你是否发现，Midjourney 生成的图片的背景很容易和前景相混淆。请看下面两个例子。</p>\\n<p>第一个例子中，我要生成一张超现实的茶杯的照片，以云朵为背景。</p>\\n<blockquote>\\n<p>Prompt: dainty porcelain teacup, whimsical cloudscape background (精致的瓷茶杯，奇异的云景背景)</p>\\n</blockquote>\\n<p>第一张图片是 Midjourney 生成的。你可以看到，云朵居然跑到茶杯上去了，虽然照片还挺好看的。</p>\\n<p>第二张是 DALL·E 3 生成的，前景和背景并没有混淆，云朵并没有跑到杯子上。但天空的颜色跟茶杯有些接近。</p>\\n<p>这个例子可以看出，DALL·E 3 能更好地理解我的提示词，虽然艺术效果比 Midjourney 要差。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/33e2c2a8159a4b7cb10ad86ede6cc12f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=365919&amp;e=png&amp;b=b8cbdc\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5377707dfa1348128218a4e5dd0ae7df~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=76885&amp;e=jpg&amp;b=e7dec5\" alt=\"\" loading=\"lazy\"/></p>\\n<p>下面这个例子也同样，Midjourney 生成的图片背景中大理石的纹路居然跑到表盘上了。DALL·E 3 就能很好地理解我的提示词，表盘不仅没有大理石纹路，细节更是复杂到可怕。</p>\\n<blockquote>\\n<p>Prompt: luxury wristwatch, intricate dial, leather strap, background with marble texture (奢华腕表，复杂表盘，皮革表带，大理石纹理背景)</p>\\n</blockquote>\\n<p>Midjourney:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fcd74af009eb4c64b7daee9a9022cc41~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=537374&amp;e=png&amp;b=78808c\" alt=\"\" loading=\"lazy\"/></p>\\n<p>DALL·E 3:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5ac97506abb046b3b0730f35fbffbad9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=215634&amp;e=jpg&amp;b=201c23\" alt=\"\" loading=\"lazy\"/></p>\\n<p>当然，Midjourney 也并不是完全那么糊涂，一定会混淆前景和背景。通过一些技巧可以让两者分离。比如我可以在提示词里详细描述表盘的细节，和大理石背景区别开来。比如我写了下面的提示词：</p>\\n<blockquote>\\n<p>Prompt: luxury wristwatch boasting an intricate dial featuring delicate filigree arabesque designs gracefully intertwine, placing on the background with marble texture (奢华腕表，表盘上精致的阿拉伯花丝图案优雅地交织在大理石纹理的背景上)</p>\\n</blockquote>\\n<p>生成的图片如下所示，可以看到表盘已没有了大理石纹理。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d2f120a33fa24421b80511dcfa10d1f3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=525147&amp;e=png&amp;b=171e26\" alt=\"\" loading=\"lazy\"/></p>\\n<p>或者通过局部重绘功能重新生成表盘的细节。</p>\\n<h3>文本生成能力</h3>\\n<p>众所周知的是，Midjourney 很难在图片中生成文字。请看下面这个提示词。</p>\\n<blockquote>\\n<p>Prompt: Create an elegant perfume ad on a magazine page with a woman in a flowy dress amidst roses, script font saying &#34;Elegance in Every Scent&#34;. (在杂志页面上制作一个优雅的香水广告，广告中一位女士身着飘逸的连衣裙，在玫瑰花丛中翩翩起舞，脚本字体写着 &#34;Elegance in Every Scent&#34;。)</p>\\n</blockquote>\\n<p>第一张图片是 Midjourney 生成的。它并没有按照要求生成杂志页面，在图片中仅仅看到香水瓶上一堆看不懂的文字。</p>\\n<p>第二张是 DALL·E 3 生成的图片，有了杂志的样子，杂志上的标题文字也基本符合符合我的要求。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/13cfc62612944eb1b9dd265d919d1701~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=530145&amp;e=png&amp;b=8a544a\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d7eef53f8baa47839403ee3f443e0eb1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=140680&amp;e=jpg&amp;b=ddc6ba\" alt=\"\" loading=\"lazy\"/></p>\\n<p>下面这个例子中第二章图是 DALL·E 3 生成的，文字完全对应提示词中所要求的。Midjourney 还是生成了一堆火星文。</p>\\n<blockquote>\\n<p>Prompt: Whimsical illustration of a cat wearing aviator goggles, piloting a tiny plane, title text saying &#34;Adventure awaits in every corner&#34; (异想天开的插图，一只猫戴着飞行员护目镜，驾驶着一架小飞机，标题文字写着 &#34;冒险在每个角落等待着&#34;。)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00a110cf47a84089aeef0c49891dbe73~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=591039&amp;e=png&amp;b=0e1a2b\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e1452392d13a40adb3670b78027f8df3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=211366&amp;e=jpg&amp;b=b5dcdc\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>悬空</h3>\\n<p>物体悬空会创造出一种超现实的效果，在很多场景下都很有用，比如美食摄影。</p>\\n<p>但在 Midjourney 中实现悬空的效果很难，我过去尝试过上百个提示词，只有很少的一部分才能成功。请看下面这个提示词：</p>\\n<blockquote>\\n<p>Prompt: Yogi meditating mid-air amidst serene mountain scenery, levitating effortlessly, enveloped by the tranquility of nature (在宁静的山景中，瑜伽师在半空中冥想，毫不费力地悬浮在空中，被大自然的宁静所笼罩)</p>\\n</blockquote>\\n<p>第一张图片是 Midjourney 生成的，瑜伽师并没有悬浮在半空中。DALL·E 3 却做到了，虽然画面中的人物看起来并不像瑜伽师。 <img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff819fa1f76148eababe5c2eb21eca7e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=587908&amp;e=png&amp;b=1e1d19\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50e213911be64eea807aa491491af0ad~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=80652&amp;e=jpg&amp;b=c0d5cf\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在下面这个例子中，Midjourney 好不容易实现了一些悬浮效果，但并没有按照提示词的要求生成白色的盘子。</p>\\n<blockquote>\\n<p>Prompt: low angle shot of uniformly cut and evenly spaced banana slices suspending in mid-air, floating banana slices in perfect symmetry above a shallow white dish (低角度拍摄均匀切割的香蕉片悬浮在半空中，香蕉片完美对称地漂浮在白色浅盘上方)</p>\\n</blockquote>\\n<p>第二张图片是DALL·E 3生成的，不仅完全做到了悬空的效果，且香蕉片按照提示词的要求对称分布。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe96b41245fa47cdbe6699757ddc7e38~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=434210&amp;e=png&amp;b=120f03\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/16b73ae973264627a64fd021613e7843~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=73582&amp;e=jpg&amp;b=857b72\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>画面布局能力</h3>\\n<p>AI 绘画工具有一个很大的优势就是我们仅仅通过提示词就可以让画面中的元素按照特定的图形进行排列。在现实世界中实现这样的效果还是很困难的。请看下面这个例子：</p>\\n<blockquote>\\n<p>Prompt: bird&#39;s eye view of heart shape arrangement of standing lipsticks (站立口红心形排列鸟瞰图)</p>\\n</blockquote>\\n<p>第一张图片是 Midjourney 生成的，虽然图案确实是心型的，但是口红并不是站立的，而且看上部并不像口红，反而是像子弹和石子的混合物。</p>\\n<p>第二张图片是 DALL·E 3 生成的，画面的布局完全符合我的要求，只是画面看上去糙了点。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/950d35af91e840b2993266a68851985f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=572656&amp;e=png&amp;b=211f25\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8d7d7e32d07546b684769bf6eb90ff67~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=171214&amp;e=jpg&amp;b=cbd3d9\" alt=\"\" loading=\"lazy\"/></p>\\n<p>下面这个例子就比较有趣了。请看下面的提示词：</p>\\n<blockquote>\\n<p>Prompt: bird&#39;s eye view of gourmet chocolates neatly arranged on a rustic wooden background with a geometric honeycomb pattern (鸟瞰美食巧克力，整齐地摆放在带有几何蜂巢图案的乡村木质背景上)</p>\\n</blockquote>\\n<p>下面两张图片分别是 Midjourney 和 DALL·E 3 生成的。我让巧克力按照蜂巢图案排列，但没有要求巧克力必须是六边形的。Midjourney 生成的巧克力全部都是六边形的，DALL·E 3 为了生成了很多不同形状的巧克力。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/263c532ba6e2427f8298f52fa2642605~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=748131&amp;e=png&amp;b=301e14\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d00434b832b48eebe562db3d5982708~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=252614&amp;e=jpg&amp;b=867468\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>动作效果</h3>\\n<p>Midjourney 对动作的理解程度比不上 DALL·E 3。比如我在下面的提示词中要它们生成运动模糊效果的照片。Midjourney 并没有生成模糊的效果，而是用飞扬的尘土体现运动效果。DALL·E 3 则用模糊和线条阐释了运动模糊，虽然画面的艺术效果比不过 Midjourney。</p>\\n<blockquote>\\n<p>Prompt: a skateboarder&#39;s sneakers in mid-air, motion blur (滑板运动员的运动鞋在半空中，动态模糊)</p>\\n</blockquote>\\n<p>Midjourney:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ad6949414954f1f81d70b270f646533~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=451600&amp;e=png&amp;b=dcb582\" alt=\"\" loading=\"lazy\"/></p>\\n<p>DALL·E 3:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e879da4eba54b2b9a5e50007ada6a15~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=85144&amp;e=jpg&amp;b=161d29\" alt=\"\" loading=\"lazy\"/></p>\\n<p>尽管如此，Midjourney 并不是无法生成运动模糊的图片。你可以在提示词中使用 “速度线” 等描述运动效果的词达到目的。比如下面这个例子中，Midjourney 就生成了效果很赞的运动模糊效果。</p>\\n<blockquote>\\n<p>Prompt: speed lines of a mechanical gaming chair in motion blur, gaming setup background, ergonomic design, immersive gaming experience, close-up shot (运动模糊中机械游戏椅的速度线，游戏场景，人体工学设计，身临其境的游戏体验，特写镜头)</p>\\n</blockquote>\\n<p>Midjourney:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/72b2afba7d454203b71f1db0602d7c11~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=376301&amp;e=png&amp;b=19222f\" alt=\"\" loading=\"lazy\"/></p>\\n<p>DALL·E 3:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/28219859f1b446c5b09a41adfebc50ae~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=187566&amp;e=jpg&amp;b=0d101b\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>超现实能力</h3>\\n<p>如果你用过 DALL·E 3 的话，会发现它几乎可以实现你任何疯狂的想法，Midjourney 生成的图片会更加符合现实。请看下面两个例子。</p>\\n<blockquote>\\n<p>Prompt: a photo of a camel running at full speed through a desert landscape, kicking up swirls of dust, a woman sitting on its back typing on the keyboard of a laptop open in front of her (一张骆驼的照片，骆驼在沙漠中全速奔跑，卷起漫天尘土，一位妇女坐在骆驼背上，敲打着面前打开的笔记本电脑的键盘)</p>\\n</blockquote>\\n<p>Midjourney:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7d0e543a6efe4289a306cad9379cbe3f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=522842&amp;e=png&amp;b=d0ad89\" alt=\"\" loading=\"lazy\"/></p>\\n<p>DALL·E 3:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0001b897646c47dd92a52e93d2187f0d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=113734&amp;e=jpg&amp;b=f1e8cc\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a photograph of a young boy joyfully riding an electric guitar like a surfboard as it soars through a blue sky, musical notes floating around him</p>\\n</blockquote>\\n<p>Midjourney:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3a8ffbf14d564dd9b508f3cca365edc1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=500220&amp;e=png&amp;b=436494\" alt=\"\" loading=\"lazy\"/></p>\\n<p>DALL·E 3:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/df0b05f578204acea2e9c02deb9e856c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=143653&amp;e=jpg&amp;b=e1dbd4\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>两者结合的效果</h2>\\n<p>通过上面的例子，相信你也看出了两者之间的差异：Midjourney 并不像 DALL·E 3 那样精通人类的自然语言并精确地实现提示词里要求的复杂效果。但它生成的图片更加逼真，艺术效果更佳。那么是否可以结合两者的优势呢？</p>\\n<p>我把上面所有 DALL·E 3 生成的图片作为垫图，用同样的提示词在 Midjourney 中重新生成一遍。看看 Midjourney 能否学习 DALL·E 3 生成的图片并产生艺术效果更佳的图片。结果是令人失望的，两者之间的差异是如此巨大。只有下面几张图片比 Midjourney 原先生成的图片效果好了一些。</p>\\n<p>１.  悬空的香蕉片变得更多了，盘子也变成了白色。<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c85e8e1ea6dc4f17b9f290dfe8ca74dd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=351461&amp;e=png&amp;b=34281a\" alt=\"\" loading=\"lazy\"/></p>\\n<p>2.下图中的人物也坐到了骆驼上，但是从女人变成了男人。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8d3bc4b265a446808e171260a1c06f99~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=536510&amp;e=png&amp;b=e9dcc9\" alt=\"\" loading=\"lazy\"/></p>\\n<p>我只能多次迭代和局部重绘才生成了这张效果还比较满意的照片。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/757e86841c81453bb2f954d237864007~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=481139&amp;e=png&amp;b=f0e5d4\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>为何差异如此巨大</h2>\\n<p>之所以存在如此大的差异，原因是两者使用不同的神经网络。</p>\\n<p>Midjourney 使用的是 Diffusion 模型。它通过多个步骤逐渐将随机噪声转化为可识别的形状和场景来创建图像。所以你在 Discord 里面看到的是 Midjourney 一步步把模糊的图像变得清晰。在每一步中，模型都会根据给出的文本描述添加更多细节和清晰度，直到出现连贯的图片。 它可以准确地渲染精细的细节，因此 Diffusion 模型更擅长生成逼真的图像。代价是需要更多的时间和计算能力来完成完整的扩散过程。 但最终的结果是看起来更自然，更有艺术效果。</p>\\n<p>DALL·E 3 使用的是 Transformers 模型，它更善于理解人类的自然语言，因此可以生成不仅看起来不错而且与所提供的文字描述非常吻合的图像。它速度更快，生成的图像是一次性创建的，并非像Diffusion 模型那样一步步创建。因此图像有时可能看起来不太真实或连贯，尤其是在提示词不够详细的情况下。但它的灵活性使它能够以新颖的方式组合概念、属性和风格。</p>\\n<p>我并不是技术领域的专家，如果有不足之处，欢迎专家给我留言。</p>\\n<h2>总结</h2>\\n<p>我认为 DALL·E 3 并不会取代 Midjourney。DALL·E 3 更简单易用，并不像 Midjourney 那样使用起来有些复杂。它也会被放到 ChatGPT 里面，适合给文章配图，对艺术效果不高的场景。</p>\\n<p>Midjourney 生成的图片艺术效果更好，生成的图片可以用于专业领域，对审美要求比较高的场景。它的学习难度更高一些，因此受众群体会不如 DALL·E 3 那样广泛。</p>\\n<p>你觉得呢？欢迎留言告诉我你的观点。</p>\\n<blockquote>\\n<p>分享完毕，感谢阅读🤗 欢迎点赞，收藏，评论</p>\\n<p>更多免费原创教程，🚀关注公众号：我的AI力量</p>\\n</blockquote>', 'author': '我的AI力量', 'reading_time': 15, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 42, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '583047153957e331a8c909001df14249d69e9bbcff820b636b836d77d77f8bf8', 'title': 'RankVicuna，开源LLM Reranking的一大步丨论文解读', 'url': 'https://juejin.cn/post/7285540804734877750', 'published_at': '2023-10-05T02:53:22Z', 'created_at': '2023-10-05T05:32:50.435609Z', 'changed_at': '2023-10-05T05:32:50.435609Z', 'content': '<h1>零、前言</h1>\\n<p><strong>📕欢迎访问</strong>：</p>\\n<blockquote>\\n<p>个人主页：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fconqueror712.github.io%2F\" title=\"https://conqueror712.github.io/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">conqueror712.github.io/</a></p>\\n<p>Github主页：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FConqueror712%2F\" title=\"https://github.com/Conqueror712/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/Conqueror71…</a></p>\\n</blockquote>\\n<p>注意，本文不涉及太多复杂的公式与数学推导，初学者或者像笔者一样数学不好的读者可以放心阅读！</p>\\n<blockquote>\\n<p>论文链接：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fabs%2F2309.15088\" title=\"https://arxiv.org/abs/2309.15088\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/abs/2309.15…</a></p>\\n<p>Github链接：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fcastorini%2Frank_llm\" title=\"https://github.com/castorini/rank_llm\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/castorini/r…</a></p>\\n</blockquote>\\n<h1>一、摘要</h1>\\n<p>将LLM应用于<strong>信息检索</strong>环境中的Reranking（我们会在下面给出介绍）已经不是什么秘密了，但迄今为止，这类工作大多建立在不透明API端点后面的闭源模型上，这导致了由这种方法产生的实验结果<strong>不可复制</strong>且<strong>非确定性</strong>，影响到了真实性。</p>\\n<p>本文为了解决这一重大缺点，提出了<strong>RankVicuna</strong>，能够在zero-shot情况下执行高质量的列表Reranking。就结果而言，RankVicuna可以通过使用更小的7B参数模型实现与GPT3.5的zero-shot进行Reranking相当的效果，尽管RankVIcuna的效果仍略逊于GPT4，但是作者希望其工作为将来关于使用现代LLM进行Reranking的研究提供基础。</p>\\n<h1>二、什么是Reranking</h1>\\n<p><strong>Reranking是指在信息检索过程中对搜索结果进行再次排序的过程</strong>。通常情况下，搜索引擎会根据某种算法对相关性进行评估，然后将搜索结果按照相关性排序并呈现给用户。然而，这种排序可能并不总能准确地满足用户的需求，好在我们已经有了神奇的LLM。通过使用LLM进行Reranking，可以利用其强大的语言理解和生成能力，对搜索结果进行更准确和个性化的排序。LLM可以通过对查询和搜索结果进行建模，生成相关性得分，然后将结果重新排序以提供更好的搜索体验。</p>\\n<p>LLM还可以用于zero-shot Reranking，这意味着模型在没有针对具体任务进行训练的情况下也可以进行Reranking。这种能力使得<strong>LLM在各种领域和任务中都可以应用</strong>，而无需进行昂贵的任务特定的训练。</p>\\n<h1>三、介绍</h1>\\n<p>依赖于闭源模型的LLM Reranking虽然在作为API时支持快速原型开发，但基于它们的实验结果的<strong>可重复性是存疑的</strong>。大家希望能够获得完全开源的LLM和相关的代码基础设施，来执行高质量的重新排序。</p>\\n<p>RankVicuna正是提供了这一需求，在构建RankVicuna过程中有一些见解：</p>\\n<ul>\\n<li>闭源LLM确实在zero-shot方式下进行Reranking时是有效的，但除了不可复制性的明显问题外，这些模型的结果也是非确定性的，这使得它们在严格的科学研究中<strong>不可靠</strong>。</li>\\n<li>通过研究第一阶段检索方法对下游Reranking效果的影响，发现<strong>RankVicuna改善了基线检索结果</strong>。特别是使用有效的第一阶段检索器，即使只对前20个候选项进行Reranking（即Top20），也能带来类似于对前100个候选项进行Reranking的改进。</li>\\n<li>通过实验，揭示了<strong>数据增强对系统鲁棒性的重要性</strong>，这在面对文档清洗和初始检索质量变化时起着至关重要的作用。</li>\\n</ul>\\n<h1>五、方法</h1>\\n<h2>5.1 Prompt Design</h2>\\n<p>最近的研究表明，基于zero-shot列表级别的LLM的Reranking优于点对点方法，因为前者可以<strong>同时关注多个文档</strong>，确定它们在相关性排序中的相对位置。本文作者在这一发现的基础上进行了进一步研究，并将排序问题定义如下：</p>\\n<blockquote>\\n<p>给定用户查询qqq和来自前一阶段的候选文档{D1，...，Dn}\\\\{D1，...，Dn\\\\}{D1，...，Dn}，任务是返回一个重新排序的输入文档标识符列表，以改进检索度量，如nDCGnDCGnDCG。</p>\\n</blockquote>\\n<p>本文的zero-shot列表级别Reranking的Prompt模板类似于RankGPT Prompt，但考虑到Vicuna和GPT之间的差异，作者使用了Vicuna的默认系统描述。对于Vicuna而言，系统描述是<strong>一个好奇的用户和人工智能助手之间的聊天</strong>，助手对用户的问题给出有帮助、详细和礼貌的回答。</p>\\n<h2>5.2 RankVicuna</h2>\\n<p>作者将RankGPT3.5作为Vicuna的teacher model（解释如下），用于Prompt Decode生成高质量的排序列表。</p>\\n<blockquote>\\n<p>Teacher model是在机器学习中用于<strong>指导</strong>和训练其他模型的模型，它通常是一个在大规模数据集上进行了预训练的模型，具有较高的性能和知识，其的任务是生成高质量的输出，可以用作<strong>参考或目标</strong>，以帮助训练其他模型。本文使用了RankGPT3.5提供的10万个训练集查询生成的排序列表来训练RankVicuna。</p>\\n</blockquote>\\n<p>为了生成这个数据集，作者从MS MARCO v1段落排名训练集中随机抽取了10万个查询，并使用Pyserini的BM25对每个查询检索了20个候选文档。然后，将这些候选文档传入RankGPT3.5以生成参考排序，然后将其精炼为student model RankVicuna。由于RankGPT3.5和RankVicuna都没有直接接触过人工标记的相关查询-段落对，所以RankVicuna可以被认为是zero-shot。</p>\\n<p>为了确保训练出<strong>更高质量和更稳健</strong>的模型，作者采取了以下额外的步骤：</p>\\n<ul>\\n<li>作者从训练集中<strong>排除了具有错误列表格式、缺少文档标识符或重复的示例</strong>。这一点很重要，因为作者发现大约12%的输出结果存在格式错误，而模型能够一致地生成格式正确的排序显然是我们所希望的。</li>\\n<li>除了包含teacher model提供的原始生成结果之外，RankVicuna还包括了输入顺序被重排的情况。作者希望这样可以使模型接触到更复杂的Reranking任务，同时不增加额外的数据生成成本。不过，作者仍保留原始的BM25输入顺序，因为他认为<strong>模拟成功</strong>对于模型在推理过程中的输入最接近是很重要的。除非另有说明，本文中的所有RankVicuna设置都涉及此数据增强过程。</li>\\n</ul>\\n<p>这神奇的RankVicuna的训练情况如何呢？</p>\\n<ul>\\n<li>7B参数的RankVicuna模型进行了两个阶段的训练，有效batch大小为128128128，学习率为2×10−52 × 10^{−5}2×10−5。</li>\\n<li>训练过程大约花费了80小时。在四个NVIDIA RTX A6000 GPU上进行。</li>\\n</ul>\\n<h1>六、结论与尾声</h1>\\n<p>在信息检索中，Retrieval和Reranking共同构成了多阶段的文本排序流程。过去的研究主要使用<strong>点对点</strong>的方法进行Reranking，而最近的研究开始采用<strong>成对和列表级别的方法</strong>来提高Reranking的效果。一些商业API端点的出现也表明Reranking模型已经在实际应用中得到广泛使用。</p>\\n<p>在LLM的检索增强时代，准确可靠的信息检索系统的需求不断增长，RankVicuna的开源模型和代码基础设施为未来的研究奠定了坚实的基础，并促进了重新排序领域的进一步探索和创新。</p>\\n\\n<p>FIN</p>', 'author': 'Conqueror712', 'reading_time': 6, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 41, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '1c419f828a509b0a2478664658e8b644f7cd4414511f030d9784398550b7dd28', 'title': '实测 亚马逊 AI 编程助手 Amazon CodeWhisperer（全网最全)', 'url': 'https://juejin.cn/post/7285538670609432635', 'published_at': '2023-10-05T02:41:22Z', 'created_at': '2023-10-05T05:32:50.420692Z', 'changed_at': '2023-10-05T05:32:50.420692Z', 'content': '<blockquote>\\n<p>文章作者：疯狂的驸马</p>\\n</blockquote>\\n<h2>一、CodeWhisperer 简介</h2>\\n<p>CodeWhisperer 是亚⻢逊出品的一款基于机器学习的通用代码生成器，可实时提供代码建议。</p>\\n<p>在编写代码时，它会自动根据我们现有的代码和注释生成建议。从单行代码建议到完整的函数，它可为我们提供各种大小和范围的个性化建议。</p>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">亚马逊云科技开发者社区</a>为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，点<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpassport.amazoncloud.cn%2Fsignin%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://passport.amazoncloud.cn/signin?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">这里</a>让它成为你的技术宝库！</p>\\n</blockquote>\\n<p>CodeWhisperer 还可以扫描我们的代码以突出显示和定义安全问题。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b05c72a8bf7e46c4934b2a205fe72c19~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=403&amp;s=183126&amp;e=png&amp;b=094162\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Faws.amazon.com%2Fcn%2Fcodewhisperer%2F%3Ftrk%3Dcndc-detail%26continueFlag%3Ddfb258fdbd1fb3efdcb4d090f78f424f\" title=\"https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail&amp;continueFlag=dfb258fdbd1fb3efdcb4d090f78f424f\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><em>codewhispereropen in new window</em></a>：亚马逊官方出品</p>\\n<p>目前仅以插件的形式在<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fcode.visualstudio.com%2F%3Ftrk%3Dcndc-detail\" title=\"https://code.visualstudio.com/?trk=cndc-detail\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><em>VS Codeopen in new window</em></a>、<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fcode.visualstudio.com%2F%3Ftrk%3Dcndc-detail\" title=\"https://code.visualstudio.com/?trk=cndc-detail\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><em>Jetbrainsopen in new window</em></a>等IDE里面使用，如果想试一试可以用 VS Code，目前无限制免费使用免费，并且对环境要求不高，还轻量 ~</p>\\n<p>亚马逊在 2022 年 6 月发布了 CodeWhisperer 预览版，现在它支持 Python、Java 和 JavaScript。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/605f19eefc724a8a90907d47b6437d9e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=259&amp;s=234704&amp;e=png&amp;b=19222e\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>CodeWhisperer 经过数十亿行代码的训练，由机器学习提供支持，旨在实现相同的目标。无论我们是学生、新开发人员，还是经验丰富的专业人士，CodeWhisperer 都有助于我们提高工作效率。</p>\\n<p>其支持多种 IDE 和语言。要开始使用，我们只需安装合适的\\xa0<a href=\"https://link.juejin.cn?target=https%3A%2F%2Faws.amazon.com%2Fcn%2Fdeveloper%2Ftools%2F%3Ftrk%3Dcndc-detail\" title=\"https://aws.amazon.com/cn/developer/tools/?trk=cndc-detail\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><em>Amazon IDE Toolkit</em></a>，启用 CodeWhisperer 功能，输入我们的预览访问代码，然后开始键入：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/47c29237a5294bb5aac59baf7c3ac41b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=356&amp;h=201&amp;s=8825&amp;e=png&amp;a=1&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>CodeWhisperer 可以持续检查我们的代码和注释，并为我们提供语法正确的推荐。这些推荐根据您的编码风格和变量名称合成，而不仅仅是代码段。</p>\\n<p>CodeWhisperer 使用多个上下文线索来提供推荐，包括源代码中的光标位置、光标前面的代码、注释以及同一项目中其他文件中的代码。您可以按原样使用推荐，也可以根据需要对其进行改善和自定义。正如我之前所提到的，我们使用从开源存储库、内部 Amazon 存储库、API 文档和论坛中提取的数十亿行代码训练（并将继续训练）CodeWhisperer。</p>\\n<p>在亚马逊云科技博客的<a href=\"https://link.juejin.cn?target=https%3A%2F%2Faws.amazon.com%2Fcn%2Fblogs%2Fcompute%2Fintroducing-amazon-codewhisperer-in-the-aws-lambda-console-in-preview%2F%3FcontinueFlag%3Ddfb258fdbd1fb3efdcb4d090f78f424f%3Ftrk%3Dcndc-detail\" title=\"https://aws.amazon.com/cn/blogs/compute/introducing-amazon-codewhisperer-in-the-aws-lambda-console-in-preview/?continueFlag=dfb258fdbd1fb3efdcb4d090f78f424f?trk=cndc-detail\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><em>一篇文章</em></a>中，Mark Richman 解释说，CodeWhisperer 的模型是在“包括 Amazon 开源代码在内的各种数据源”上训练的。有了这个语料库（显然<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.techrepublic.com%2Farticle%2Faws-getting-better-at-open-source%2F%3FcontinueFlag%3Ddfb258fdbd1fb3efdcb4d090f78f424f%3Ftrk%3Dcndc-detail\" title=\"https://www.techrepublic.com/article/aws-getting-better-at-open-source/?continueFlag=dfb258fdbd1fb3efdcb4d090f78f424f?trk=cndc-detail\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><em>确实存在</em></a>）完善 CodeWhisperer 的模型，编写从 S3 读取文件的代码应该是一个很好的测试用例。</p>\\n<p>在使用 CodeWhisperer（CW）时，我们需要写一个注释，描述我们希望函数去做什么。注释的描述性和准确性越高，系统就越能更好地推断出我们想要的逻辑。</p>\\n<p>Function to open an S3 file</p>\\n<p>比如注释以 Function 开头，让 CW 知道你想要创建一个函数。也就是说，你需要添加一个注释，作为给 CW 的提示。</p>\\n<p>CW 分析注释并生成一个函数的定义。此时，你可以在生成函数体之前修改函数定义。CW 还可能提供多种函数定义供你选择。</p>\\n<p>IntelliJ 集成 CodeWhisperer 的截图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b3aebb1f6c454e359805659629447dd7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=230&amp;s=90705&amp;e=png&amp;b=faf9f9\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>点击“插入代码”，你的函数就在注释的下方创建好了。注意 CodeWhisperer 不仅插入了代码，还创建了一个文档字符串。</p>\\n<pre><code>\\ndef open_s3_file(filename):\\n    &#34;&#34;&#34;\\n    :param filename:\\n    :return:\\n    &#34;&#34;&#34;\\n    s3 = boto3.resource(&#39;s3&#39;)\\n    return s3.Object(bucket, filename).get()[&#39;Body&#39;].read()\\n</code></pre>\\n<p>看起来不错！这段代码实现了你的注释所期望的功能，并且是在几秒钟内就生成了。</p>\\n<p>节省了查找 boto3 API 的时间，你只需要检查代码，确保语义正确。</p>\\n<p>接着看看提出更多要求时，会发生什么。</p>\\n<p>这有一个很有用的例子：写一个函数从 S3 的文件中返回前“n”行。</p>\\n<pre><code>\\ndef get_first_n_lines_from_s3(filename, n):\\n    &#34;&#34;&#34;\\n    Get the first n lines of a file in S3\\n    :param filename:\\n    :param n:\\n    :return:\\n    &#34;&#34;&#34;\\n    lines = []\\n    for line in open_s3_file(filename):\\n        lines.append(line)\\n        if len(lines) &gt;= n:\\n            break\\n    return lines\\n</code></pre>\\n<p>真厉害！CodeWhisperer 使用了之前创建的辅助方法 open_s3_file，并生成了正确的函数。</p>\\n<p>在 CW 生成函数体之前，你可以修改参数名，提高代码的可读性。例如，如果我们输入：</p>\\n<pre><code>\\n</code></pre>\\n<p>CW 会提示：</p>\\n<pre><code>def find_common_elements(list1, list2)\\n</code></pre>\\n<p>在生成函数体之前，我们可以修改函数定义，使其更具描述性：</p>\\n<pre><code>def find_common_elements(first, second)\\n</code></pre>\\n<p>如果我们接受它的函数定义，CodeWhisperer 将使用函数签名和注释来构建函数体。得到的函数将使用改进后的参数名。</p>\\n<pre><code># Function to find common elements in lists\\ndef find_common_elements(first, second):\\n        return list(set(first) &amp; set(second))\\n</code></pre>\\n<p>CodeWhisperer 不仅仅可以创建整个函数；它还可以在函数中插入代码片段，并在过程中推断相关变量。</p>\\n<pre><code># reverse the list order\\n  lines.reverse()\\n</code></pre>\\n<h3>1.1 CodeWhisperer 是一个重要的生产力助推器</h3>\\n<p>使用 CodeWhisperer，我能够更快地编写代码。到目前为止，最大的价值是 S3 示例所演示的功能。</p>\\n<p>如果我自己写代码，大部分时间将会用来查找 boto API 文档，以确保写出正确的调用。使用 CodeWhisperer，则只需三秒钟。</p>\\n<p>这让我想到了一段曾花了很多时间的烦人代码：给 CloudWatch 发送数据。让我们看看 CodeWhisperer 能否有所帮助，试试提供注释：“Function to emit a CloudWatch metric”。</p>\\n<pre><code>\\ndef put_metric_data(metric_name, metric_data):\\n    &#34;&#34;&#34;\\n    Put a metric data into cloudwatch\\n    :param metric_name:\\n    :param metric_data:\\n    :return:\\n    &#34;&#34;&#34;\\n    cloudwatch = boto3.client(&#39;cloudwatch&#39;)\\n    cloudwatch.put_metric_data(\\n        Namespace=&#39;my_namespace&#39;,\\n        MetricData=[\\n            {\\n                &#39;MetricName&#39;: metric_name,\\n                &#39;Dimensions&#39;: [\\n                    {\\n                        &#39;Name&#39;: &#39;string&#39;,\\n                        &#39;Value&#39;: &#39;string&#39;\\n                    },\\n                ],\\n                &#39;Unit&#39;: &#39;Seconds&#39;,\\n                &#39;Value&#39;: metric_data\\n            },\\n        ]\\n    )\\n</code></pre>\\n<p>非常有用！CW 为我节省了许多打字和查阅 API 文档的时间。</p>\\n<p>我有可能需要重构这段代码，但即使我从零开始编写它，第一步也要写出类似这样的原始逻辑。如今只花三秒钟，我节省了编写大量样板代码的时间，有了一个很好的自定义或重构的起点。</p>\\n<h3>1.2 CodeWhisperer 的实际应用</h3>\\n<p>我在 PyCharm 中安装了 CodeWhisperer ，并对其进行了测试。以下是一些示例，向您展示了它可以执行的操作。我想构建一个质数列表。我键入 # See if a number is pr。CodeWhisperer 完成此操作，然后我按下 TAB（实际键特定于每个 IDE）接受推荐：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/18e22f8791a84335aa2bfef6050bc480~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=565&amp;h=185&amp;s=13373&amp;e=png&amp;a=1&amp;b=fbf9f9\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在下一行，我按下 Alt+C（同样是 IDE 特定），然后我可以在一对函数定义之间进行选择。我接受第一个函数，CodeWhisperer 推荐函数体，以下是我所拥有的：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/028faf3bba614973a149938274b529b1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=455&amp;h=212&amp;s=17287&amp;e=png&amp;a=1&amp;b=fdfcfc\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>我编写 for 语句，CodeWhisperer 推荐循环的主体：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ffd4baaf428b497db96b253ed81c72b2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=337&amp;h=142&amp;s=8726&amp;e=png&amp;a=1&amp;b=fcfaf9\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>CodeWhisperer 还可以帮助我编写用于访问各种亚马逊云科技服务的代码。我从 # create S3 bucket 开始，然后按 TAB 键完成剩余部分：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f78756cbdb61437ca729e1a948250c0c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=511&amp;h=209&amp;s=26646&amp;e=png&amp;a=1&amp;b=fdfcfc\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>看到这里，大家是不是觉得还不错，接下来教大家如何安装，很简单的~</p>\\n<h2>二、CodeWhisperer 安装教程</h2>\\n<h3>2.1 IntelliJ IDEA 安装 CodeWhisperer 插件</h3>\\n<p>在 IDEA 中打开配置窗⼝，选择 Plugins，搜索&#34;<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fso.csdn.net%2Fso%2Fsearch%3Fq%3DAWS%26amp%253Bspm%3D1001.2101.3001.7020%26continueFlag%3Ddfb258fdbd1fb3efdcb4d090f78f424f%3Ftrk%3Dcndc-detail\" title=\"https://so.csdn.net/so/search?q=AWS&amp;amp%3Bspm=1001.2101.3001.7020&amp;continueFlag=dfb258fdbd1fb3efdcb4d090f78f424f?trk=cndc-detail\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\"><em>Amazon</em></a>\\xa0Toolkit&#34;，点击Install，点击OK按钮，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/555973c22b0c42d5b9878ee2affc8d2f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=523&amp;s=168875&amp;e=png&amp;b=373a3d\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>安装完之后重启 IDEA，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0078e95e0cab47a5880e68372f945417~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=370&amp;h=146&amp;s=10201&amp;e=png&amp;b=3d4042\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>打开 Amazon Toolkit 视图（菜单 View/Tool Windows/AWS Toolkit），点击&#34;Developer Tools&#34;tab⻚⾯，选择“CodeWhisperer/Start&#34;，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/05486e3460474ea88ec0d8d929cd3443~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=450&amp;s=142297&amp;e=png&amp;b=3c3f41\" alt=\"image.png\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f95f01842b2e41b4b6ebdb89ade34626~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=631&amp;h=362&amp;s=20863&amp;e=png&amp;b=3c4042\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>弹出的窗⼝中选择“Use a personal email to sign up and sign in with Amazon Builder ID&#34;，点击“Connect”按钮，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf34cb580e304e558264f6346c925451~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=532&amp;h=378&amp;s=28876&amp;e=png&amp;b=3c3f41\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在弹出的窗⼝中，选择“Open and Copy Code”，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa5e49b50aa04b368e46ea778cb16ddd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=637&amp;h=132&amp;s=14211&amp;e=png&amp;b=3c3f41\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>此时会在浏览器中打开⼀个⻚⾯，按 ctrl-v 粘贴 code 值，点击“Next“，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e0f74355524e4c6d8484987d497de9a8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=360&amp;h=405&amp;s=20930&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>输⼊邮箱地址，点击&#34;Next&#34;，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6828d7b35954441dbe5c6b39bc35aaa1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=396&amp;h=578&amp;s=42752&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>输⼊名字，点击“Next”，CodeWhisperer会向邮箱中发送⼀个验证码，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2212adc037bd4550a63e17c4cc843674~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=338&amp;h=592&amp;s=73804&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>打开邮箱，可以看到验证码，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b6e13414fbd240829f6c009b5d84c512~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=615&amp;h=505&amp;s=41052&amp;e=png&amp;b=fffefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>复制验证码，粘贴到输入框，点击“Verify”按钮，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8010a15216d340bf8aededddf2ec437c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=374&amp;h=524&amp;s=34890&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>设置密码，点击“Create AWS Builder ID“，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/713a4fb09c8849ba9296b23da134bf27~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=395&amp;h=707&amp;s=40639&amp;e=png&amp;b=fdfcfc\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在最后⼀个⻚⾯中点击“Allow”按钮，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa7a633949a744a280ca51909602a3fb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=625&amp;h=429&amp;s=56967&amp;e=png&amp;b=fcfcfc\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>出现如下提示后，即表示注册 Amazon builder ID 成功，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/de53075d87514c928f50ba83f4ea9bcc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=369&amp;h=334&amp;s=28239&amp;e=png&amp;b=f9f8f6\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>返回 IDEA，在 Amazon Toolkit 视图中的 Developer Tools 中可以打开或关闭代码⽣成功能，如下图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/21a5985da35042a5964ebb0b941ee226~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=631&amp;h=277&amp;s=22294&amp;e=png&amp;b=3c3f41\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>此时，就可以愉快的使用 AI 编程了。</p>\\n<h3>2.2 VSCode 安装 CodeWhisperer 插件</h3>\\n<p>安装 Amazon Toolkit 插件，具体到 VS Code 侧边栏搜索并安装</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/197dea541366419187d63fd725035ee5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=391&amp;s=185539&amp;e=png&amp;b=222222\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>侧边栏点击aws，&gt;&gt; DEVELIOPER TOOL &gt;&gt; CodeWhisper &gt;&gt; Start</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/115140b5bae7402c90270e8ae6e20b68~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=492&amp;h=348&amp;s=67815&amp;e=png&amp;b=262626\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在下拉菜单中点击 Use a personal email to sign up and sign in with Amazon Builder ID</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/05c191ca2f4f4e3b91ba092a5c9039cd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=539&amp;h=163&amp;s=43681&amp;e=png&amp;b=2b2b2c\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>点击 Copy Code and Proceed，这将自动复制代码</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7571a84fb83e4677a322477b68823fd4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=425&amp;h=137&amp;s=21546&amp;e=png&amp;b=f9f8f8\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>一般会提示外部网站打开提醒，选择打开</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ceed67341f1f4e50a0d3978d9c47d92d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=307&amp;h=127&amp;s=20544&amp;e=png&amp;b=f8f7f7\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>5.打开网站后，输入点击 Copy Code and Proceed 时得到的代码，点击 Next</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f482e4d6212247668ae6a838192faa1e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=303&amp;h=331&amp;s=23650&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>6.输入自己的邮箱地址(同时登录你的邮箱等待验证码)，点击 Next</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1772a2eb77e741c89416acc35dccf82a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=335&amp;h=337&amp;s=38568&amp;e=png&amp;b=fefcfc\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>7.重复之前步骤，但是由于已经在浏览器上登录成功了，所以步骤非常简单 粘贴代码后将会提示以下内容，点击 Allow 即可</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5b99593b2e704f7eadd18700ff7686e2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=643&amp;h=432&amp;s=70946&amp;e=png&amp;b=fbfafa\" alt=\"image.png\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c0411a9be9254e0cb893a7c5cbdcf69d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=564&amp;h=372&amp;s=23245&amp;e=png&amp;b=fafafa\" alt=\"image.png\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d5e75f84d77c4b96b3ad095732b2e333~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=482&amp;h=333&amp;s=34530&amp;e=png&amp;b=f9f7f6\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>等左下角的亚马逊云科技扩展颜色正常，对勾状态，说明连接成功</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ddfe1810e8174d9b8b5df946a990c79f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=437&amp;h=485&amp;s=29636&amp;e=png&amp;b=262627\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>此时，就可以愉快的使用 AI 编程了。</p>\\n<h3>2.3 PyCharm 安装 CodeWhisperer</h3>\\n<p>1.打开 Pycharm 插件管理</p>\\n<p>在 JetBrains IDE 中，导航到设置菜单（在 macOS 上为⌘ + ，在 Windows 上为文件→设置），然后单击左侧菜单上的“插件”。</p>\\n<p>在菜单顶部，单击 Marketplace 并在搜索栏中键入 “Amazon Tookit”。然后点击安装。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0a2dd47f996a41c189ed760a35389339~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=469&amp;s=190910&amp;e=png&amp;b=2c2e31\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>2.安装完成后重启 IDE</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/17c52c3fdfcb4ffcb2364587e7d45282~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=603&amp;h=85&amp;s=37257&amp;e=png&amp;b=464c4e\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>3.将 Pycharm 连接到亚马逊云科技（Amazon Build ID创建），重复之前步骤</p>\\n<p>IDE 重新启动后，您将看到一个新的 Amazon Toolkit 工具窗口。还可以通过 View -&gt; Tool Windows -&gt; Amazon Toolkit（视图 -&lt; 工具窗口 -&lt; Amazon Explorer）访问此窗口。</p>\\n<p>然后点击&#34;Developer Tools&#34;标签，选择“CodeWhisperer/Start&#34;</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4d0049f702784620b9af1f93532e9ccc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=537&amp;h=416&amp;s=106966&amp;e=png&amp;b=3c4043\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>此时，就可以愉快的在 PyCharm 里使用 AI 编程了。</p>\\n<h3>2.4 快捷键使用方法</h3>\\n<p>全程需要按快捷键调用 Codewhisperer，主要的几个用法如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c3aabf1e169e4c0095daddf7cb9aee19~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1420&amp;h=570&amp;s=57018&amp;e=png&amp;b=fcfcfc\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h2>三、简单自动编码演示</h2>\\n<p>回到 IDEA，看到如下界面，即表示注册并启动成功：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b8f1e47ffa1d44abb5c03e7b0996cf7b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=427&amp;s=106486&amp;e=png&amp;b=2a2f37\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>下面我们新建任意 Java 文件,看看他如何能够帮助我们自动生成代码。</p>\\n<h3>3.1 单行代码自动补全</h3>\\n<p>输入字符串 public。根据输入，CodeWhisperer 生成了建议列表</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/22146960d5b9471aaee9021ff385f330~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=347&amp;s=84534&amp;e=png&amp;b=2a2e36\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>3.2 CodeWhisperer 生成完整代码</h3>\\n<p>CodeWhisperer 可以根据编写的注释生成完整的函数。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f19157e132e64e23899ab56d4cc8ecd3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=317&amp;s=76044&amp;e=png&amp;b=292d35\" alt=\"image.png\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a35b5daf23864a1386b38e45bdf0c465~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=717&amp;s=132488&amp;e=png&amp;b=282c34\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>类似于 if/for/while 等代码块的生成。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f6532921e4724f4d97869907c7f1351c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=447&amp;s=113237&amp;e=png&amp;b=292d35\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>大家如果看到这有疑问，可以接着看第四部分，有详细的测试说明。</p>\\n<h3>3.3 其根据签名自动生成代码</h3>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/75ab8d808f664c7a8e1a3ecd41ec63c6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=700&amp;h=493&amp;s=84377&amp;e=png&amp;b=292d35\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>看到这里，大家应该有个简单的了解了，咱们接着往下看~</p>\\n<h2>四、AI 编程代码实测及项目体验</h2>\\n<h3>4.1 简单函数代码测试</h3>\\n<p>首先，我先用中文注释了“写一个读取 csv 文件的函数” 回车后可以看到 CodeWhisperer 自动显示出来</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ada06b6447b6467fbedbefa012ab3524~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=432&amp;h=383&amp;s=32325&amp;e=png&amp;b=202124\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>按快捷键 Tab 确认键入后，接着回车后 CodeWhisperer 又自动生成了以下代码：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/768aaff207a9438c9f246f86d5c32a79~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=413&amp;h=346&amp;s=35030&amp;e=png&amp;b=212225\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>感觉这个 CodeWhisperer 自动生成的代码还不错，我选择了 Insert Code 键入使用。</p>\\n<p>紧接着用中文又注释了“创建一个简单的爬虫简单函数”</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a9ae1de711a049de92ec6584f9b31bf9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=572&amp;h=369&amp;s=41756&amp;e=png&amp;b=202124\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在这选择了第三个自动代码，同样 Tab 键入。</p>\\n<p>接下来是注释“冒泡排序”后回车</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2dc4390ed55449a9929e05daf1de0248~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=570&amp;h=337&amp;s=35752&amp;e=png&amp;b=202124\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>注释“选择排序”后回车</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5ecc73ad3c7946f393081a7f389ce8c8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=577&amp;h=320&amp;s=32947&amp;e=png&amp;b=202124\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>中文注释实测完后又对英文注释进行了测试，如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/557256674a46432eb913a658afb0b498~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=542&amp;h=490&amp;s=60623&amp;e=png&amp;b=202124\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>乘胜追击，又对创建和上传文件代码进行自动生成测试，同样得到了预期的效果。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/28e3f35a42814948b4c8389b819b8d99~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=593&amp;h=413&amp;s=54432&amp;e=png&amp;b=202124\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>对于简单的函数实测让我感觉这个工具还挺不错的，因为可以根据自己想要的注释自动生成多个代码建议，这样不仅节省了大量的编码时间，而且多种代码建议可供选择，一定程度上提升了自己编写代码的质量。</p>\\n<h3>4.2 代码安全扫描实测</h3>\\n<p>接下来用 CodeWhisperer 去扫描我们代码中的安全漏洞，以下是运行按钮的位置</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/964b44e6af4e41e8afa69614cd421aaa~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=332&amp;h=309&amp;s=24796&amp;e=png&amp;b=2d2f32\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>打开我想扫描的文件，然后运行安全扫描</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/64c9473c9b8a4e16889efe2a2c8bf09c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=304&amp;s=153117&amp;e=png&amp;b=27292c\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>结果发现 CodeWhisperer 探测到该文件中有两个安全问题。 通过查看问题栏，点击事件定位到代码中，我们将鼠标移入到突出显示的代码，查看建议的措施，如下所示，问题指出在该例子中未加密的亚马逊云科技凭证被记录下来，是一个安全漏洞，它建议我们重写代码并且修复该漏洞。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/850f5a0e928a4b02895d25841936af4f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=321&amp;s=147889&amp;e=png&amp;b=232528\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>同样查看第二个问题，CodeWhisperer 告诉我们讲应该把标识设置为 True。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f14b038b825c471ba810f3d83491f38f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=312&amp;s=129452&amp;e=png&amp;b=232528\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>根据 CodeWhisperer 的建议，我进行了对应修改，两处安全问题修改如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/98300940bf93494dacf59ac922c0cf1b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=471&amp;h=264&amp;s=92166&amp;e=png&amp;b=1f2223\" alt=\"image.png\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/22902a0e79044b908918f85bb0248008~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=587&amp;h=281&amp;s=81519&amp;e=png&amp;b=1e2122\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>修改后重新扫描了文件，扫描完成，显示没找到问题。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dad1adcbdc6245638f733edfb008187c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=282&amp;s=133920&amp;e=png&amp;b=27292c\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>对于安全扫描这项功能，我还是挺惊艳的，没想到能实测找出问题并给出对应的解决问题，因为这一点我对 CodeWhisperer 的好感又增加了。</p>\\n<h3>4.3 代码引用跟踪功能实测</h3>\\n<p>在这里想实现一个函数来创建一个 dynamo DB 表。 如下图所示进行了实际测试 “implement a function to create a dynamoDB table”</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/21462a08c924453ba80cb5297761630f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=398&amp;s=196063&amp;e=png&amp;b=232527\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>以下是 CodeWhisperer Reference Log 给出的建议：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4306ce6f59034ca6b28f90013111b8a9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=422&amp;s=134686&amp;e=png&amp;b=202324\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>根据代码建议标注被文件跟踪器标记为 MIT 许可证，其引用了 Apache-2.0 许可证下的代码。我选择接受了这些标记代码，然后发现 CodeWhisperer 自动记录了引用我需要的许可证信息，这样我就可以在我的代码上添加适当的许可证和归属信息。 这个功能实测完感觉也挺有用的，对代码涉及到的许可证和归属信息问题进行了有效解决。</p>\\n<h3>4.4 计算机视觉工程项目实测</h3>\\n<p>在对官网显示的几项功能测试完后，又在平时用到的实际项目上进行了测试，我选取了常用的一些 CV 检测模型，以下是 ShuffleNet 的实际测试效果。</p>\\n<p>首先，我在实例化训练数据集上，对缺失的代码段进行测试，看 CodeWhisperer 是否能联系上下文进行代码补全。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/419e68c9713a40948c9367ca09daa305~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=312&amp;s=158555&amp;e=png&amp;b=212121\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>进行回车操作后，CodeWhisperer 显现出来了所给的建议。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3ece131865c24ebdbcdd484289d8e690~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=326&amp;s=168352&amp;e=png&amp;b=212121\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>目前看所给的建议正确，接着我又对冻结权重部分进行了测试，</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6cba5212d51842f19d299d0cd85d7e66~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=390&amp;s=204710&amp;e=png&amp;b=202020\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>这回我又利用了 CodeWhisperer 的快捷键 ALT+C 进行了测试，</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ffb84aea5fae4355887b5611f39150c4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=307&amp;s=169232&amp;e=png&amp;b=202020\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>结果显示自动生成了 False 选项，测试到这里真的让我觉得这个工具是真的好，可以帮助我解决代码中遇到的很多问题。 在训练代码中测试完，我又接着对检测模型代码进行了测试，如下图所示在 transform 指向中为我建议了 img。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/28ac5cf836f442c686243eb4a6b5e3c6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=365&amp;s=195793&amp;e=png&amp;b=212121\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>最后，我对检测模型代码进行了运行测试，正确的检测出图片为蒲公英 dandelion。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/029e7f31fe48434a88ff49fa052d83b0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=464&amp;s=216992&amp;e=png&amp;b=f7f4f4\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>总的来说，CodeWhisperer 辅助我完成了 CV 模型的代码编写和模型检测，达到了官网所展示的预期。接下来，我要让 CodeWhisperer 帮助我完成更复杂的任务了，哈哈</p>\\n<h2>五、Amazon CodeWhisperer 实测体验总结</h2>\\n<h3>5.1 CodeWhisperer 可以帮助我成为一个更好的开发者吗?</h3>\\n<p>通过以上的测试，我觉得它可以帮助我成为一个更好的开发者。</p>\\n<p>首先，它可以为我节省大量的时间和精力，让我能够专注于改进、重构和测试。</p>\\n<p>其次，它通过承担一些同质化的繁重工作，让我有机会成为一个更好的程序开发人员。</p>\\n<p>比如上面的测试的例子是 Amazon 工具（经过 Amazon 开源代码训练）能够表现出色的例子。</p>\\n<p>当然，在大多数开发人员需要花费很多时间的地方，比如编写领域相关的逻辑时，我又多测试了一下，让我们看看 CodeWhisperer 会不会也有帮助。</p>\\n<p>比如从 Python 文档中的数据类示例开始。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/80226e6aec9e428a95ddc3fb1c1f1921~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=850&amp;h=259&amp;s=84204&amp;e=png&amp;b=333333\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>其实我想知道 CodeWhisperer 是否可以向这个类添加一个方法。让我们看看如果添加注释：&#34; Function that return this item costs more than $10&#34;，会发生什么？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1381bc64f2b2467c998ecbf0a65bffbf~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=850&amp;h=354&amp;s=124520&amp;e=png&amp;b=333333\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>结果是非常酷的。值得注意的是，CodeWhisperer 给函数起了一个直观的名字，并包含了对 self 的引用。</p>\\n<p>接着，让我们尝试用 CodeWhisperer 来做测试，看是否会触及它的极限。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/417b5598e14347c5a09a573f0b68b282~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=847&amp;h=332&amp;s=108907&amp;e=png&amp;b=333333\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在上面的代码中，我输入了注释，CW 自动完成了剩下的工作。 测试似乎是一个极好的证明 CW 可以节省时间的例子。我不需要浪费时间去想测试的值，也不用输入所有的成员变量和方法。</p>\\n<p>总的来说，可以帮助我成为一个更好的开发者，但是任何辅助工具都有利有弊，CodeWhisperer 也是才发布不久，通过测试我也想邀请大家去进行实际测试，可以对使用 CodeWhisperer 遇到的问题大家一起相互讨论，一起促进这个编程助手迭代和完善~</p>\\n<p>文章来源：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F64f87f242a1d01763ead6ed1%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/column/article/64f87f242a1d01763ead6ed1?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">dev.amazoncloud.cn/column/arti…</a></p>', 'author': '亚马逊云开发者', 'reading_time': 17, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 40, 'user_id': 1, 'feed_id': 2, 'status': 'read', 'hash': '49634455a8ec12d6e8b1d960736e7b5171655ee844d54044d3f90489af523693', 'title': '自然语言处理的分类', 'url': 'https://juejin.cn/post/7284881709045973051', 'published_at': '2023-10-04T03:56:27Z', 'created_at': '2023-10-04T14:11:40.368094Z', 'changed_at': '2023-10-04T14:12:23.966934Z', 'content': '<p>动动发财的小手，点个赞吧！</p>\\n<p><img src=\"https://s2.loli.net/2023/09/28/GhXTF6QeBusdaI1.png\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>简介</h2>\\n<p>作为理解、生成和处理自然语言文本的有效方法，自然语言处理（NLP）的研究近年来呈现出快速传播和广泛采用。鉴于 NLP 的快速发展，获得该领域的概述并对其进行维护是很困难的。这篇博文旨在提供 NLP 不同研究领域的结构化概述，并分析该领域的最新趋势。</p>\\n<p>在<a href=\"https://link.juejin.cn?target=https%3A%2F%2Ftowardsdatascience.com%2Fa-taxonomy-of-natural-language-processing-dfc790cb4c01\" title=\"https://towardsdatascience.com/a-taxonomy-of-natural-language-processing-dfc790cb4c01\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">本文</a>中，我们研究以下问题：</p>\\n<ul>\\n<li>NLP 研究哪些不同的研究领域？</li>\\n<li>NLP 研究文献的特点和随时间的发展是什么？</li>\\n<li>NLP目前的趋势和未来工作的方向是什么？</li>\\n</ul>\\n<p>尽管 NLP 的大多数研究领域都是众所周知的和明确的，但目前还没有常用的分类法或分类方案试图以一致且易于理解的格式收集和构建这些研究领域。因此，了解整个 NLP 研究领域的概况是很困难的。虽然会议和教科书中列出了 NLP 主题，但它们往往差异很大，而且往往要么太宽泛，要么太专业。因此，我们开发了一个涵盖 NLP 广泛不同研究领域的分类法。尽管该分类法可能不包括所有可能的 NLP 概念，但它涵盖了广泛的最受欢迎的研究领域，因此缺失的研究领域可以被视为所包含研究领域的子主题。在制定分类法时，我们发现某些较低级别的研究领域必须分配给多个较高级别的研究领域，而不仅仅是一个。因此，一些研究领域在 NLP 分类中被多次列出，但被分配到不同的更高级别的研究领域。最终的分类法是与领域专家一起在迭代过程中凭经验开发的。</p>\\n<p>该分类法作为一种总体分类方案，其中 NLP 出版物可以根据至少一个所包含的研究领域进行分类，即使它们不直接涉及其中一个研究领域，而只是其子主题。为了分析 NLP 的最新发展，我们训练了一个弱监督模型，根据 NLP 分类法对 ACL Anthology 论文进行分类。</p>\\n<h2>NLP的不同研究领域</h2>\\n<p>以下部分对上述 NLP 分类法中包含的研究概念领域进行简短说明。</p>\\n<h3>多模态</h3>\\n<p>多模态是指系统或方法处理不同类型或模态输入的能力。我们区分可以处理自然语言文本以及视觉数据、语音和音频、编程语言或结构化数据（例如表格或图表）的系统。</p>\\n<h3>自然语言接口</h3>\\n<p>自然语言接口可以基于自然语言查询处理数据，通常实现为问答系统或对话系统。</p>\\n<h3>语义文本处理</h3>\\n<p>这一高级研究领域包括试图从自然语言中获取含义并使机器能够从语义上解释文本数据的所有类型的概念。这方面最强大的研究领域之一是尝试学习单词序列的联合概率函数的语言模型。语言模型训练的最新进展使这些模型能够成功执行各种下游 NLP 任务。在表示学习中，语义文本表示通常以嵌入的形式学习，可用于比较语义搜索设置中文本的语义相似度。此外，可以合并知识表示（例如以知识图的形式）来改进各种 NLP 任务。</p>\\n<h3>情绪分析</h3>\\n<p>情感分析试图从文本中识别和提取主观信息。通常，研究的重点是从文本中提取观点、情感或极性。最近，基于方面的情感分析成为一种比一般情感分析提供更详细信息的方法，因为它旨在预测文本中给定方面或实体的情感极性。</p>\\n<h3>句法文本处理</h3>\\n<p>这一高级研究领域旨在分析文本的语法和词汇。这种情况下的代表性任务是句子中单词依赖性的句法分析、将单词标记到各自的词性、将文本分割成连贯的部分，或者纠正语法和拼写方面的错误文本。</p>\\n<h3>语言学与认知 NLP</h3>\\n<p>语言学与认知 NLP 处理自然语言的基础是这样的假设：我们的语言能力牢牢植根于我们的认知能力，意义本质上是概念化，语法是由用法决定的。目前存在许多不同的语言理论，它们普遍认为语言习得受到所有典型发育人类所共有的通用语法规则的控制。心理语言学试图模拟人脑如何获取和产生语言、处理语言、理解语言并提供反馈。认知建模涉及以各种形式，特别是计算或数学形式对人类认知过程进行建模和模拟。</p>\\n<h3>推理</h3>\\n<p>推理使机器能够使用演绎和归纳等技术，根据可用的信息得出逻辑结论并得出新知识。论据挖掘自动识别和提取自然语言文本中表达为论据的推论和推理结构。文本推理通常被建模为蕴涵问题，自动确定是否可以从给定前提推断出自然语言假设。常识推理使用文本中未明确提供的世界知识来连接前提和假设，而数值推理则执行算术运算。机器阅读理解旨在教会机器根据给定的段落确定问题的正确答案。</p>\\n<h3>多语言能力</h3>\\n<p>多语言处理涉及多种自然语言的所有类型的 NLP 任务，并且通常在机器翻译中进行研究。此外，语码转换可以在单个句子内或句子之间自由交换多种语言，而跨语言迁移技术则使用一种语言可用的数据和模型来解决另一种语言的 NLP 任务。</p>\\n<h3>信息检索</h3>\\n<p>信息检索涉及从大型集合中查找满足信息需求的文本。通常，这涉及检索文档或段落。</p>\\n<h3>信息提取与文本挖掘</h3>\\n<p>该研究领域的重点是从非结构化文本中提取结构化知识，并能够分析和识别数据中的模式或相关性。文本分类自动将文本分类为预定义的类别，而主题建模旨在发现文档集合中的潜在主题，通常使用文本聚类技术将语义相似的文本组织到相同的集群中。摘要生成文本摘要，其中在更小的空间中包含输入的关键点，并将重复保持在最低限度。此外，信息提取和文本挖掘领域的研究还包括命名实体识别，处理命名实体的识别和分类，共指解析，旨在识别对同一实体的所有引用话语、术语提取，旨在提取相关术语，例如关键字或关键短语、关系提取，旨在提取实体之间的关系，以及开放信息提取，以促进关系元组的领域独立发现。</p>\\n<h3>文本生成</h3>\\n<p>文本生成方法的目标是生成人类可以理解且与人类创作的文本无法区分的文本。因此，输入通常由文本组成，例如在释义中，以不同的表面形式呈现文本输入，同时保留语义，问题生成旨在根据给定的段落生成流畅且相关的问题和目标答案，或对话响应生成，旨在生成与提示相关的自然外观文本。然而，在许多情况下，文本是作为其他模式输入的结果生成的，例如在数据到文本生成的情况下，基于结构化数据（例如表格或图表）生成文本 、图像或视频的字幕，或将语音波形转录为文本的语音识别。</p>\\n<h2>NLP的特点和发展</h2>\\n<p><img src=\"https://s2.loli.net/2023/09/28/VhFxKaBekTiA4u8.png\" alt=\"\" loading=\"lazy\"/></p>\\n<p>考虑到 NLP 方面的文献，我们从研究数量作为研究兴趣的指标开始分析。 50年观察期内的出版物分布如上图所示。虽然第一批出版物出现于 1952 年，但每年出版物的数量增长缓慢，直到 2000 年。相应地，2000 年至 2017 年间，出版物数量大约翻了两番，而在随后的五年中，又翻了一番。因此，我们观察到 NLP 研究的数量呈近指数增长，表明研究界的关注日益增加。</p>\\n<p><img src=\"https://s2.loli.net/2023/09/28/9uXriUslyBwJqmG.png\" alt=\"\" loading=\"lazy\"/></p>\\n<p>检查上图，揭示了 NLP 文献中最受欢迎的研究领域及其随着时间的推移的最新发展。虽然 NLP 的大多数研究都与机器翻译或语言模型有关，但这两个研究领域的发展却有所不同。机器翻译是一个经过深入研究的领域，已经建立了很长时间，并且在过去 20 年中经历了适度的增长。语言模型也被研究了很长时间。然而，自 2018 年以来，该主题的出版物数量才出现显着增长。在其他热门研究领域也可以观察到类似的差异。表征学习和文本分类虽然得到了广泛的研究，但其发展却部分停滞。相比之下，对话系统和对话代理，尤其是低资源 NLP，研究数量继续呈现高增长率。根据其余研究领域的平均研究数量的发展，我们观察到整体略有正增长。然而，大多数研究领域的研究明显少于最受欢迎的研究领域。</p>\\n<h2>NLP 的最新趋势</h2>\\n<p><img src=\"https://s2.loli.net/2023/09/28/2RaMjA6B7OKHIhx.png\" alt=\"\" loading=\"lazy\"/></p>\\n<p>上图展示了NLP研究领域的增长份额矩阵。我们用它来通过分析 2018 年至 2022 年间 NLP 各个研究领域相关论文的增长率和总数来考察当前的研究趋势和未来可能的研究方向。矩阵的右上部分由以下研究领域组成：总体上表现出高增长率和大量论文。鉴于本部分的研究领域越来越受欢迎，我们将它们归类为热门明星。右下部分包含非常受欢迎但增长率较低的研究领域。通常，这些是 NLP 所必需的研究领域，但已经相对成熟。因此，我们将它们归类为基础研究领域。矩阵的左上部分包含表现出高增长率但总体上论文很少的研究领域。由于这些研究领域的进展相当有前景，但总体论文数量较少，难以预测其进一步发展，因此我们将其归类为上升问号。矩阵左下角的研究领域由于论文总数低且增长率低而被归类为利基研究领域。</p>\\n<p>从图中可以看出，目前最受关注的是语言模型。根据该领域的最新发展，这一趋势可能会在不久的将来持续并加速。文本分类、机器翻译和表示学习位列最热门的研究领域之列，但仅显示出边际增长。从长远来看，它们可能会被增长更快的领域取代，成为最受欢迎的研究领域。</p>\\n<p>一般来说，与句法文本处理相关的研究领域总体上表现出可忽略不计的增长和较低的受欢迎程度。相反，与负责任和值得信赖的NLP相关的研究领域，例如绿色和可持续的NLP、低资源NLP和道德NLP，总体上往往表现出高增长率和高受欢迎程度。这种趋势也可以在 NLP 中的结构化数据、NLP 中的视觉数据以及 NLP 中的语音和音频中观察到，所有这些都与多模态有关。此外，涉及对话系统、对话代理和问答的自然语言界面在研究界变得越来越重要。我们的结论是，除了语言模型之外，负责任且值得信赖的 NLP、多模态和自然语言界面也可能成为不久的将来 NLP 研究领域的特征。</p>\\n<p>在推理领域，特别是在知识图推理和数字推理以及与文本生成相关的各个研究领域中，可以观察到进一步显着的发展。尽管这些研究领域目前还相对较小，但它们显然吸引了研究界越来越多的兴趣，并表现出明显的积极增长趋势。</p>\\n<h2>总结</h2>\\n<p>为了总结最近的发展并概述 NLP 的前景，我们定义了研究领域的分类并分析了最近的研究进展。</p>\\n<p>我们的研究结果表明，已经研究了大量的研究领域，包括多模态、负责任和值得信赖的 NLP 以及自然语言界面等趋势领域。我们希望本文能够对当前 NLP 领域提供有用的概述，并可以作为更深入探索该领域的起点。</p>\\n<p>本文由<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmdnice.com%2F%3Fplatform%3D2\" title=\"https://mdnice.com/?platform=2\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">mdnice</a>多平台发布</p>', 'author': 'Swindler', 'reading_time': 8, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 39, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '2ba5fce429295082b44a75fadd81722b9bb2e97ade747564dffc22ee66d76716', 'title': '浅谈生成式 AI 技术：检索增强生成 RAG', 'url': 'https://juejin.cn/post/7284940213544845373', 'published_at': '2023-10-04T02:28:54Z', 'created_at': '2023-10-04T14:11:40.363923Z', 'changed_at': '2023-10-04T14:11:40.363923Z', 'content': '<h2>引言</h2>\\n<p>如今，生成式 AI （Generative AI）的应用雨后春笋的涌现出来，让人应接不暇。而<strong>大型语言模型（LLM，Large Language Model）</strong> ，随着 ChatGPT 的发布而变得异常火热，是生成式 AI 应用的一个典型。但是，LLM 存在缺陷。其中一个比较大的问题在于<strong>幻觉（Hallucination）</strong> ：对于不熟悉的问题，LLM 会瞎编乱造，编造出看似专业却没有任何事实依据的答案。而为了解决这个问题，很多基于 AI 的知识问答系统采用了<strong>检索增强生成（RAG）</strong> 技术，从而让 LLM 能够做出基于事实的回答，从而消除幻觉。本篇文章将简单介绍 RAG 是如何在知识问答系统中发挥作用的。</p>\\n<h2>LLM</h2>\\n<p>要理解 RAG，我们首先需要简单理解一下 LLM。其实，LLM 在大量的参数训练下，已经可以完成很多难以置信的 NLP 任务，例如问答、写作、翻译、代码理解等等。但是，由于 LLM 的“记忆”停留在预训练时刻，肯定会存在它不知道的知识和问题。例如，OpenAI 开发的 ChatGPT 就不能回答 2021 年 9 月之后的问题。此外，由于幻觉的存在，LLM 会显得很有想象力且缺乏事实依据。因此，我们可以将 LLM 比做一个<strong>知识渊博且全能的智者，可以干很多事情，但却失忆了，记忆只停留在某一时间之前，而且不能形成新的记忆</strong>。</p>\\n<p>而为了让这个智者能够在现代试题考试中得到高分，我们该怎么做呢？答案就是 RAG。</p>\\n<h2>RAG</h2>\\n<p>RAG 并不是新技术。早在 2020 年 5 月，LLM 还没有普及的时候，RAG 模型就被提了出来，被用作处理知识密集的 NLP 任务。而如今，RAG 已经成为知识问答、文档检索等基于 AI 技术的应用中非常重要的技术。</p>\\n<p>回到之前的问题，如果想让 LLM 考试中得到高分，我们该如何做呢？RAG 又是什么原理呢？其实答案很简单，就是我们在 LLM 遇到新问题的时候，我们将教科书中跟问题相关的章节拿出来给 LLM 看，LLM 看到后会根据自己的理解加上相关章节内容结合得出答案。而这个生成的答案会基于事实（教科书），不再是胡说八道。没错，这就跟<strong>开卷考试</strong>一模一样！而这就是 RAG 的原理。</p>\\n<p>下图是典型知识问答的流程图。当一个问题（Query）来的时候，系统会从知识库（Knowledge Base）中提取相关的上下文（Relevant Context），再将问题和上下文一并喂给 LLM，让 LLM 自行判断并回答该问题，生成响应（Response）。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f945bc3b25754228b2ce83f5de44a509~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2106&amp;h=1114&amp;s=298242&amp;e=png&amp;b=fefefe\" alt=\"RAG原理\" loading=\"lazy\"/></p>\\n<h2>嵌入</h2>\\n<p>我们介绍了 LLM 以及 RAG 的简单工作原理，但我们应该如何从知识库中提取相关上下文的呢？这背后的技术就是<strong>嵌入（Embedding）</strong> 。嵌入听起来比较专业，但我们也可以比较形象的进行理解。这就好比我们提前将教科书（知识库）的每个章节或每个段落做了<strong>索引（Indexing）</strong> ，也就是用各种颜色和记号标记出来。当我们需要回答问题的时候，就从标记好的索引中找出相关的章节段落，再把其抽出来做回答就可以了。是不是很简单？</p>\\n<p>当然，嵌入技术的本质其实还是将非结构化数据（例如文字）转化为结构化数据（数值矩阵）。而结构化数据正是计算机能够处理和理解的。不光是文字，图片、视频、音频等都可以通过嵌入技术转化为计算机能够理解的结构化数据，而这也是图片搜索引擎的基础原理。</p>\\n<p>下图是输入文字与输出数值矩阵表示的示意图。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b948812d2400461cb479f7bd77240459~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1141&amp;h=313&amp;s=30592&amp;e=gif&amp;f=3&amp;b=fefefe\" alt=\"嵌入技术\" loading=\"lazy\"/></p>\\n<h2>总结</h2>\\n<p>我们通过简单的比喻，将 LLM 比做知识渊博但新记忆缺失的智者，RAG 比做开卷考试，嵌入比做教科书记号，生动形象的阐述了基于生成式 AI 技术的知识问答系统的核心技术。而笔者的智能阅读助手思阅（SRead）也正是基于这样的技术框架，能够让 AI 能够阅读并帮助读者回答与文章、论文、书本相关的问题。希望本文能够让读者对生成式 AI 技术有进一步了解。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f10d24c6c01b4381b07f7f10de6e180a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2432&amp;h=1438&amp;s=683507&amp;e=png&amp;b=fefefe\" alt=\"思阅 SRead 界面\" loading=\"lazy\"/></p>\\n<h2>社区</h2>\\n<p>如果您对笔者的文章感兴趣，可以加笔者微信 tikazyq1 并注明 &#34;码之道&#34;，笔者会将你拉入 &#34;码之道&#34; 交流群。</p>\\n<p>智能阅读助手思阅（SRead）上线啦，内测地址: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fsread.ai\" title=\"https://sread.ai\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">sread.ai</a>，欢迎试用。</p>', 'author': 'MarvinZhang', 'reading_time': 4, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 38, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'b703d6c871bfff4aeed3f6ac61c0f46829c8d0b3e5a01a22f93378d67f419eda', 'title': 'ChatGPT提示工程5篇合集 - 吴恩达和OpenAI出品', 'url': 'https://juejin.cn/post/7284962800668377151', 'published_at': '2023-10-03T14:56:33Z', 'created_at': '2023-10-04T14:11:40.362232Z', 'changed_at': '2023-10-04T14:11:40.362232Z', 'content': '<p>吴恩达和OpenAI合作开发了一套针对ChatGPT提示工程的最佳实践课程，课程名为《ChatGPT Prompt Engineering for Developers》。</p>\\n<p>这门课程时长为1个小时，内容简单易懂，还提供了实践的环境。关键是，课程免费，而且讲师是吴恩达（Andrew Ng，DeepLearning.AI创始人）和伊莎·富尔福德（Isa Fulford，OpenAI的技术人员），含金量非常高。</p>\\n<p>本文是该课程的5篇文章合集。</p>\\n<h3>1.编写Prompt的两个关键原则</h3>\\n<ul>\\n<li>原则一：编写清晰、具体的说明</li>\\n<li>原则二：给予模型思考的时间</li>\\n</ul>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FLzwRLNlHpVd-sEY0RNub9Q\" title=\"https://mp.weixin.qq.com/s/LzwRLNlHpVd-sEY0RNub9Q\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程的两个关键原则</a></p>\\n<h3>2.文本总结</h3>\\n<p>这篇主要简单的介绍了如何对内容进行总结。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FDUMutcPkLXRl_3u2Eel_LQ\" title=\"https://mp.weixin.qq.com/s/DUMutcPkLXRl_3u2Eel_LQ\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 总结</a></p>\\n<h3>3.文本推理</h3>\\n<p>可以让模型来识别一段内容的情绪，或者抽取指定的内容。</p>\\n<p>也可以一次性执行多个任务，从而节省多次请求的整体耗时和成本。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FAHNbS5USVWcRaCIAue2Hxg\" title=\"https://mp.weixin.qq.com/s/AHNbS5USVWcRaCIAue2Hxg\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 推理</a></p>\\n<h3>4.文本转换</h3>\\n<p>在这篇文章中，介绍了如何使用大型语言模型来进行文本转换工作，如语言翻译、语调调整和格式转换。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FiAaFSRJxOt0iZwZL7l0vMg\" title=\"https://mp.weixin.qq.com/s/iAaFSRJxOt0iZwZL7l0vMg\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 转换</a></p>\\n<h3>5.邮件回复与营销文案</h3>\\n<p>在这篇文章中，主要介绍如何使用大型语言模型来进行邮件自动回复、生成营销文案。</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F7qmfASBuq6M12f-3PaKvTA\" title=\"https://mp.weixin.qq.com/s/7qmfASBuq6M12f-3PaKvTA\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT提示工程 - 邮件回复、营销文案</a></p>', 'author': 'jsam', 'reading_time': 2, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 37, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '1fa84949423ede66eed2d5ab8709407655e28e1273487d08c9eeab4275257f6f', 'title': 'ChatGPT“与图对话”初体验', 'url': 'https://juejin.cn/post/7284982768336568375', 'published_at': '2023-10-03T13:15:20Z', 'created_at': '2023-10-04T14:11:40.359436Z', 'changed_at': '2023-10-04T14:11:40.359436Z', 'content': '<h1><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FGLOhKG4dQNmBhjAyLaae4A\" title=\"https://mp.weixin.qq.com/s/GLOhKG4dQNmBhjAyLaae4A\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT“与图对话”初体验</a></h1>\\n<p><a href=\"https://link.juejin.cn?target=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5NzY0NjY1MA%3D%3D%26mid%3D2247485380%26idx%3D1%26sn%3D9416eb1a621c962e10a54eecf64fb207%26chksm%3Dfe510b91c9268287912b120aad13f44697722c5af6d588771748e6c35c2d2fd1ee0e527ff773%26scene%3D21%23wechat_redirect\" title=\"http://mp.weixin.qq.com/s?__biz=MzU5NzY0NjY1MA==&amp;mid=2247485380&amp;idx=1&amp;sn=9416eb1a621c962e10a54eecf64fb207&amp;chksm=fe510b91c9268287912b120aad13f44697722c5af6d588771748e6c35c2d2fd1ee0e527ff773&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT发布语音和图像能力重大更新：要点解读、应用畅想与原文翻译</a></p>\\n<p>中提到：ChatGPT开放了“与图对话”的多模态能力，用户可以上传一张图片或多张图片，与ChatGPT展开基于图片的对话。想不到这么快就开放了：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/895a2ab852ff415f9c3fb7015ca468c9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=798&amp;h=398&amp;s=31464&amp;e=jpg&amp;b=5087f5\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>ChatGPT对图片的理解能达到什么水平呢？让我们一起来测试。</p>\\n\\n<h3><strong>单图理解：非常精确</strong></h3>\\n<p>我上传了一张我和女儿在一个室内动物园戴着动物面具的合影，让ChatGPT理解这张图片：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a587171168034ed2947547b710277255~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=986&amp;s=1094263&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>ChatGPT的描述是下面这样的：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8240cb1ca3ae4c4baf83b72b993e15ae~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=500&amp;s=225585&amp;e=png&amp;b=444653\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>它详细描述了：</p>\\n<ul>\\n<li>森林背景墙</li>\\n<li>我的穿着打扮</li>\\n<li>女儿的穿着打扮</li>\\n<li>父女俩的动作</li>\\n<li>整个画面的氛围</li>\\n</ul>\\n<p>这理解不能说是特别准确，那简直是十分精确！那个传神的表情“✌️”真的赞！</p>\\n<p>然后可以让ChatGPT以此图片为蓝本，讲一个你想要的故事：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4b034c76546643078009afe163f772f6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=762&amp;s=249689&amp;e=png&amp;b=434552\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>ChatGPT能否生成图像呢？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2ae5dfb84c5f4e2480c0d3b96bc21a03~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=468&amp;s=109241&amp;e=png&amp;b=444653\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>遗憾的是，无论是生成图片还是修改上传的图片，ChatGPT暂时还做不到。</p>\\n<p>ChatGPT能注意到更多的细节（那个插座）吗？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f59631d006834115ac8226a82639d91c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=648&amp;s=207094&amp;e=png&amp;b=424451\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>看来它是注意到了那个墙上的插座，而且知道它存在安全隐患，很赞！</p>\\n<p>它能注意到右上角那不太容易分辨的蜘蛛网吗？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/189ab0ec2b6e41a6b02b03903090164d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=600&amp;s=161364&amp;e=png&amp;b=3f404d\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>看来是没有注意到。我提示一下，会如何？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d188b54c31d445fb4f5a798d6e2963b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=657&amp;s=169947&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>经过提示，它注意到了蜘蛛网，不过它并不百分之百确认。看来这个部分的识别精确度比较低。</p>\\n<p>还能进一步理解吗？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/21f00056924a4f3b82e21e682a40d897~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=496&amp;s=146858&amp;e=png&amp;b=424350\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>嗯，能理解人物关系。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/48a4cccd588545d683ce0f6d2389da45~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=440&amp;s=154647&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>对背景森林的类型也能做比较准确的推断。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/06be2d4e44654fbb9f1239ef91626fbc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=329&amp;s=107643&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>这就有点强人所难了！</p>\\n<p>总的来说，ChatGPT对单张图片的理解是相当准确的，无论是场景描述、人物关系，还是整体氛围、局部细节。</p>\\n\\n<h3><strong>艺术风格识别：一半一半</strong></h3>\\n<p>对感兴趣的绘画作品，可以让ChatGPT做解释：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fadbcdb1f8d3435a892d0c4a5468a58f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=802&amp;s=1128127&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>这是它对其艺术风格和内容的描述：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a1272c3b6520430aa1cf475bba536bb0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=504&amp;s=173088&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>这的确是马赛克风格的绘画。它也知道不同画家的风格流派：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/16a401b9ca224b8ebfc837287050d1ef~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=410&amp;s=128857&amp;e=png&amp;b=41434f\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/046a0b24e1d5434981ea1d61ecc0d990~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=431&amp;s=138767&amp;e=png&amp;b=41434f\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>试试中国风：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a79f7fc7568241c9912324e70978a4d0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=729&amp;s=616223&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/351839d848e24767b93dd8e50c6654de~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=368&amp;s=145633&amp;e=png&amp;b=464855\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>不错，就是工笔画。</p>\\n<p>也有不尽人意的地方，比如将下面的“<a href=\"https://link.juejin.cn?target=\" title=\"\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大和绘</a>”风格的作品识别成“浮世绘”</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f23e65e50527487cbf7653e71fb407f5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=747&amp;s=712603&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/629b127aa21d40c9823c074572fec46b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=310&amp;s=133894&amp;e=png&amp;b=464855\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>对于“达达主义”风格并没有直接叫出其名字：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2d3fcf94ff3f47deaf4a6da6c5662848~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=834&amp;s=956962&amp;e=png&amp;b=d8ccb7\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b48f2db543cd475ab8acbbe081055f2b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=435&amp;s=201077&amp;e=png&amp;b=464855\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>我还尝试过现实主义、立体主义、世俗画、<a href=\"https://link.juejin.cn?target=\" title=\"\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">哥特主义</a>、奥托尼亚等不同风格的绘画作品，ChatGPT的水平基本上在半对半错之间。对于风格强烈的画风（比如立体主义、现实主义等）识别率高；对于风格不那么强烈或者比较细分的流派（比如风俗画，被误认为是洛可可），识别率低。</p>\\n<p>因此在艺术鉴赏方面，ChatGPT还需要进一步提高。</p>\\n\\n<p><strong>从图片到代码：差强人意</strong></p>\\n<p>网传在Twitter上有人给ChatGPT喂一张数据看板的截图，它直接生成了对应的代码。我也在这方面试了试，看看ChatGPT能在这方面做到什么程度。</p>\\n<p>对于想要给一个长截图就生成整个页面代码的想法，我劝大家还是现实一点。这是我提供给ChatGPT的一个网页长截图：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2e3d127fe9814090b7d6c8cf65f018d1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=2611&amp;s=187638&amp;e=jpg&amp;b=fbf6f3\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>然后让它“这是一个产品的landing page，请你生成一个html文件，来复刻这个网页”，当然它很乖地给出了html代码：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/455e0e7ab30142cbbb938ac4203484e5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=628&amp;s=146459&amp;e=png&amp;b=3d3f4b\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>我满怀希望地打开，结果效果是这样的😱：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/abc570f178224a248e49ac4a4898a921~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=616&amp;s=108267&amp;e=png&amp;b=fbfbfb\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>不能说是一模一样，简直是毫无相似之处。估计要通过非常多轮的对话，才能有一个稍微可用的版本。</p>\\n<p>然后我对任务做了简化，只让它复刻其中的一部分：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ab37de696a1240c18ca9a5b1666e9c8c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=667&amp;s=130834&amp;e=png&amp;b=393a46\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>这下得到的版本就好多了，加上图标就基本可用了：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d57fdca4d6f34193a98250d82dc44b3d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=241&amp;s=57551&amp;e=png&amp;b=fefefe\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>对于更加复杂一点的数据看板：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/32bd2458b2d046fd84da5babaeb371e8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=468&amp;s=110077&amp;e=png&amp;b=fcfcfc\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>多轮对话之后，可以得到下面的版本：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5212df20175740a6adc83390ec5d7342~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=556&amp;s=49516&amp;e=png&amp;b=fcfcfc\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>嗯，还算OK，调调能用。</p>\\n\\n<p><strong>多图理解：还挺不错的</strong></p>\\n<p>ChatGPT支持传多张图片（最多是四张）</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd3075821d914b0f95a471248dce8dec~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=113&amp;s=14692&amp;e=jpg&amp;b=de524d\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>那多图的理解能力如何呢？</p>\\n<p>我先传了一张女儿画的简笔画以及一张剪纸后的简笔画：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fbd5b784affc41e699075e21cc361c27~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=429&amp;s=316014&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>ChatGPT的理解挺到位的：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/981045bf0e6b4df880e491e56f66e28a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=577&amp;s=157111&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>当我上传四张图片之后，会发现上传的三张和第四张图片丢失（可能是服务不稳定造成的，我试过好几次都是如此）</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c4b85b5eac4d41288afd9c03af9d0277~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=739&amp;s=286082&amp;e=png&amp;b=3e3f4c\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>ChatGPT果然没有收到第三张和第四张图片</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8ccb1474565048de8ed100e867e6801b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=756&amp;s=248199&amp;e=png&amp;b=3c3e4a\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p>如果仅仅就两张图片，它可以讲一个“自以为是”的故事</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4863151f5fcb4bae9f63885d2af1b7dd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=411&amp;s=314940&amp;e=png&amp;b=343540\" alt=\"图片\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0aa3381cffc6449b9a7bafcfa9fd492f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=565&amp;s=201512&amp;e=png&amp;b=454754\" alt=\"图片\" loading=\"lazy\"/></p>\\n\\n<h3></h3>\\n<h3></h3>\\n<h3><strong>小结</strong></h3>\\n<p>就目前的体验来看，ChatGPT的识图能力：</p>\\n<ul>\\n<li>单图理解能力非常强悍</li>\\n<li>艺术风格识别一半一半</li>\\n<li>从图片生成代码差强人意</li>\\n<li>多图理解还挺不错</li>\\n</ul>\\n<p>期待ChatGPT多模态能力的不断增强！</p>\\n<p>【完】</p>\\n\\n<p>【往期相关】</p>\\n<p><a href=\"https://link.juejin.cn?target=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5NzY0NjY1MA%3D%3D%26mid%3D2247485380%26idx%3D1%26sn%3D9416eb1a621c962e10a54eecf64fb207%26chksm%3Dfe510b91c9268287912b120aad13f44697722c5af6d588771748e6c35c2d2fd1ee0e527ff773%26scene%3D21%23wechat_redirect\" title=\"http://mp.weixin.qq.com/s?__biz=MzU5NzY0NjY1MA==&amp;mid=2247485380&amp;idx=1&amp;sn=9416eb1a621c962e10a54eecf64fb207&amp;chksm=fe510b91c9268287912b120aad13f44697722c5af6d588771748e6c35c2d2fd1ee0e527ff773&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ChatGPT发布语音和图像能力重大更新：要点解读、应用畅想与原文翻译</a></p>\\n<p><a href=\"https://link.juejin.cn?target=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5NzY0NjY1MA%3D%3D%26mid%3D2247485454%26idx%3D1%26sn%3Db7a83d86a3e00384c0226b58ef878656%26chksm%3Dfe51045bc9268d4df2dae5e9bf1b8efc712a5bac098980d258759c6709853e954844b32a5d81%26scene%3D21%23wechat_redirect\" title=\"http://mp.weixin.qq.com/s?__biz=MzU5NzY0NjY1MA==&amp;mid=2247485454&amp;idx=1&amp;sn=b7a83d86a3e00384c0226b58ef878656&amp;chksm=fe51045bc9268d4df2dae5e9bf1b8efc712a5bac098980d258759c6709853e954844b32a5d81&amp;scene=21#wechat_redirect\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">【AI产品】Podwise：AI助我听播客</a></p>', 'author': '用户7366796250774', 'reading_time': 4, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 36, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '1c322e31065309fd56272ac562775620cdfa6858db79e86c482326214c229a94', 'title': '为什么开源大模型终将胜出？', 'url': 'https://juejin.cn/post/7284881709044760635', 'published_at': '2023-10-03T08:21:17Z', 'created_at': '2023-10-04T14:11:40.354Z', 'changed_at': '2023-10-04T14:11:40.354Z', 'content': '<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7b6dfb6d826845a682f42a40f16f9778~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=940&amp;h=529&amp;s=199594&amp;e=png&amp;b=252b29\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>自ChatGPT面世以来，以它为代表的闭源 AI 备受关注，简单易用的特性使其占据了行业主导。尽管以LLaMA 为代表的开源 AI 进展迅猛，但业内也流行三个反对开源的观点：开源 AI 无法与行业实验室的优势资源竞争；开源 AI 缺乏安全性；开源 AI 无法进行推理（reasoning）。</p>\\n<p>本文作者 Varun Shenoy 在 AI 基础设施公司 Baseten 从事机器学习推理工作，他并不认同当前闭源 AI 垄断的发展态势，并一一反驳了上述看法。在他看来，开源 AI 通过微调可用于各种领域各种细分场景和任务；相比推理，上下文长度和内容的真实性更为重要；通过亲自微调，用户能够获得更高的控制权和可见度，因此开源也能够提供隐私和安全性方面的保障。</p>\\n<p>基于此，随着开源模型日渐成熟，因其用户友好性和可定制性，开源将成为众多应用的最佳选择。</p>\\n<p>（以下内容经授权后由OneFlow编译发布，转载请联系授权。原文：<br/>\\n<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fvarunshenoy.substack.com%2Fp%2Fwhy-open-source-ai-will-win%25EF%25BC%2589\" title=\"https://varunshenoy.substack.com/p/why-open-source-ai-will-win%EF%BC%89\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">varunshenoy.substack.com/p/why-open-…</a></p>\\n</blockquote>\\n<p><strong>作者 | Varun Shenoy</strong></p>\\n<p><strong>OneFlow编译</strong></p>\\n<p><strong>翻译｜宛子琳、杨婷</strong></p>\\n<blockquote>\\n<p>Linux 具有颠覆性。五年前（1991年），谁曾想过一款世界级操作系统会以如此神奇的方式诞生，它由遍布全球、仅通过脆弱的互联网纽带连接的数千名兼职开发人员的努力构建而成。这完全出乎我的意料。</p>\\n<p>——《大教堂与集市》序言，作者Eric Raymond</p>\\n</blockquote>\\n<p>在不久的将来，每家公司都将从模型提供者（如OpenAI、Anthropic）那里租用一个“大脑”，并构建基于其认知能力的应用程序。换句话说，AI 正在朝着垄断寡头的方向发展，只有少数重要的语言大模型（LLM）供应商能够存续。</p>\\n<p>我并不认同这一观点。我坚信，开源对于未来的语言大模型和图像模型的影响将远超公众的预期。</p>\\n<p>我总会反复看到以下几个反对开源的论点。</p>\\n<p><strong>1.开源 AI 无法与行业实验室的资源相竞争。</strong> 构建基础模型的成本十分高昂，因此想要构建 AI 功能的非 AI 公司会将其智能层外包给专门从事该领域的公司。普通公司无法像一个资金充足的优秀研究团队那样，扩展语言大模型或产出创新结果。比如在图像生成领域，Midjourney 遥遥领先于其他任何公司的技术。</p>\\n<p><strong>2.开源 AI 并不安全。</strong> 那些在 GPU 上构建智能的疯狂科学家不会使其模型与人类的整体利益对齐，因为对齐可能会降低模型整体性能。</p>\\n<p><strong>3.开源 AI 缺乏推理（reasoning）能力。</strong> 开源模型不仅在基准测试中表现比闭源模型较差，而且缺乏涌现功能，例如实现智能体 workflow（agentic workflow）的能力。</p>\\n<p>这些观点看似合理，但我认为，它们缺乏充分的依据。</p>\\n<p><strong>1</strong></p>\\n<h1>LLM 对业务至关重要</h1>\\n<p>将任务外包是可行的，前提是非关键业务。</p>\\n<p>基础设施产品能够帮助用户节省学习 Kubernetes 或雇佣 DevOps 工程师团队的时间和成本。没有公司会自行开发人力资源/账单支付软件。基础设施类产品能够让公司专注于自己的核心业务，“让自家的啤酒口感更好”。</p>\\n<p>大部分情况下，LLM 并不属于这一类别。一些公司在现有产品上构建 AI 功能，会通过使用 OpenAI 的服务来节省雇佣机器学习工程师的成本。对他们来说，闭源 AI 是最合理的选择。</p>\\n<p>但还有一整个新的 AI 原生业务类别，对他们来说这种风险太大了。难道他们真的想将依赖于机密数据的核心业务外包给 OpenAI 或 Anthropic？或愿意将未来几年时间花费在“GPT 封装器”上？</p>\\n<p>当然不会。</p>\\n<p><strong>如果你正在构建一个 AI 原生产品，其主要目标是尽快摆脱对 OpenAI 的依赖。</strong> 理想情况下，你可以使用闭源供应商来启动智能层，通过与用户的互动建立数据飞轮，并对自己的模型进行微调，从而提高任务的准确性、降低时延并增强控制力。</p>\\n<p>每个企业都需要拥有自己的核心产品，对于 AI 原生初创公司来说，他们的核心产品就是基于专有数据训练的模型。在长期使用闭源模型供应商的过程中，会给 AI 原生公司带来不必要的风险。</p>\\n<p>开源LLM 身上累积了太多期许而不能失败，许多公司都和开源LLM命运攸关。即使是谷歌也承认他们在开源大模型面前没有护城河。</p>\\n<p><strong>2</strong></p>\\n<h1>推理（reasoning）其实并不重要</h1>\\n<p>LLM 的通用能力使其能够适用于各类用例。其中最重要的任务显而易见：总结、解释并从一段文本中创建列表（或其他结构）等。</p>\\n<p>对于 85% 的用例，推理并不重要，这种推理通过扩大这些模型规模而获得。研究人员乐于分享其模型的强大能力：2000 亿参数的模型可以解决复杂的数学问题或根据草图构建网站，但我认为，对于大多数用户（或开发者）来说，这种能力并非刚需。<br/>\\n实际上，开源模型在最具价值的任务上表现十分出色，在产品收集到足够的标记数据后，还可以进行微调，以覆盖可能 99% 的用例。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be4f5d262536432c9a2a220e8df6d41c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=614&amp;s=60539&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>经微调的 Llama 2 vs. GPT-4 (来自 Anyscale)</p>\\n<p>作为研究人员追求的圣杯，推理可能并没有人们想象的那么重要。</p>\\n<p><strong>相比推理，上下文长度和内容的真实性更为重要。</strong></p>\\n<p>我们先从上下文长度谈起。语言模型的上下文长度越长，能输入的提示和聊天记录就越长。</p>\\n<p>原始 Llama 模型的上下文长度为 2k，Llama 2 为 4k。今年早些时候，一位独立的 AI 黑客发现，对 Llama 2 的 RoPE 嵌入进行一行代码修改，就可以直接扩展上下文长度至 8 K，而无需额外训练。</p>\\n<p>上周，另一个独立研究项目发布了 YaRN，将 Llama 2 的上下文长度扩展至 128k，而我甚至还没来得及体验 32k 的 GPT-4 ，这体现了开源模型迅猛的发展速度。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4f5309ad9ecd4d86b46bb72bd2c60486~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=949&amp;s=466286&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>尽管上下文长度不断扩展，但运行大模型所需的硬件要求也在不断降低。借助 Llama.cpp 等项目，你现在可以在自己的 MacBook 上运行最先进的语言大模型，能够在本地使用这些模型可以显著提高安全性、降低成本。极端情况下，你可以在用户的硬件上运行自己的模型。模型的规模不断缩小，同时质量保持不变。微软的 Phi-1.5 模型仅有 13 亿个参数，但在多个基准测试中达到了 Llama2 7B 的水平。随着消费者硬件和 GPU 匮乏趋势带来的挑战，开源 LLM 实验将继续蓬勃发展。</p>\\n<p>关于真实性（truthfulness）：相对于闭源模型，开源模型在真实性方面可能会更差，但这在某些情况下实际上是可以接受的，特别是在涉及到讲故事等创造性任务时，幻觉可以被看作是一种特性而不是缺陷。</p>\\n<p>闭源的 AI 模型有一定的过滤机制，使其内容听起来人工感更强，但趣味性也随即降低。仅有130 亿参数的 MythoMax-L2 讲的故事比 Claude 2 或 ChatGPT 好得多。关于事实性，最新的开源 LLM 在检索增强生成方面表现良好，并且它们还会不断改进。</p>\\n<p><strong>3</strong></p>\\n<h1>控制性至上</h1>\\n<p>接下来简要了解一下图像生成领域。</p>\\n<p>我认为，Stable Diffusion XL（SDXL）这个最佳开源模型的性能几乎与 Midjourney 持平。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eb618615f3064781b9159f4c1b1f3ed3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=385&amp;s=488891&amp;e=png&amp;b=7e4b38\" alt=\"\" loading=\"lazy\"/></p>\\n<p>SDXL 根据提示“一名宇航员和一只羊驼在火星上一起弹吉他”生成的图像。</p>\\n<p>尽管 SDXL 在使用中略显不便，但它的用户可获得数百个社区制作的 LoRA、微调和文本嵌入。用户很快发现 SDXL 在处理手部图像时存在问题，随后的几周时间内，网上就出现了一个修复手部问题的 LoRA。</p>\\n<p>ControlNet 等其他开源项目在用户构建输出方面为 Stable Diffusion 的用户提供了更强大的助力，而 Midjourney 则不够出色。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/92ba1069f0ce485bb80b79379e959f8c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1034&amp;h=432&amp;s=242567&amp;e=png&amp;b=292f3b\" alt=\"\" loading=\"lazy\"/></p>\\n<p>Stable Diffusion 和 ControlNet 的工作流程图</p>\\n<p>此外，Midjourney 模型没有提供 API 接口。因此，如果你想构建一个带有图像扩散功能的产品，就需要以某种方式使用 Stable Diffusion 模型。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6ed2871a0df749ee9ac0fc9e7ac83f7a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=868&amp;h=866&amp;s=1017285&amp;e=png&amp;b=89785a\" alt=\"\" loading=\"lazy\"/></p>\\n<p>图片采用了 Stable Diffusion 与 ControlNet 技术。目前，Midjourney 模型还无法生成这类图片。</p>\\n<p>开源语言模型 LLM 提供了类似的可控特性和优化功能。</p>\\n<p>LLM 的 logits，即每次迭代的 token-wise 概率质量函数，可用于生成结构化输出。换句话说，你可以保证生成 JSON，而无需进入可能比较昂贵的“验证重试”循环，但如果你使用 OpenAI 的服务，则需要执行此操作。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff42971883724b66b97e9e93b1f3585f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&amp;h=326&amp;s=156376&amp;e=png&amp;b=fdfbfa\" alt=\"\" loading=\"lazy\"/></p>\\n<p>来自 NVIDIA 的 logits 示例。</p>\\n<p>开源模型更轻量级，并且在自己的专用实例上运行，从而降低了端到端的时延。你可以通过批处理查询和使用 vLLM 等推理服务器来提高吞吐量。</p>\\n<p>还可以应用更多技巧（如推测性采样、并发模型执行、KV 缓存）来改善时延和吞吐量。OpenAI 端点上的时延是封闭模型所能做到的最短时延，这使得它无法用于许多对时延有高要求的产品，而对于大型消费产品来说，它的成本又太高。</p>\\n<p>除此之外，你还可以在开源模型上使用最高程度的控制微调或训练自己的 LoRA。Axolotl 和 TRL 等框架简化了这一过程。虽然闭源模型供应商也有自己的微调端点，但与自己亲自进行微调相比，你无法获得同等的控制度和可见性。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8eb7590458024703b4b2581ed1f71cc7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=938&amp;h=776&amp;s=448732&amp;e=png&amp;b=42423a\" alt=\"\" loading=\"lazy\"/></p>\\n<p>迄今为止最强大的开源模型 Falcon 180B 于上周发布。</p>\\n<p>开源也提供了隐私和安全性方面的保障。</p>\\n<p>你对开源模型中数据的流入和流出进行控制。选择自主托管对许多用户来说是必需的，特别是那些在受监管的领域（如医疗保健）的用户。许多应用程序还需要在专有数据上进行训练和推理。</p>\\n<p>林纳斯定律（Linus&#39;s Law）可以很好地解释安全性：</p>\\n<blockquote>\\n<p>如果有足够多的测试者和共同开发者，那么就可以快速发现几乎每个问题，并迅速将其解决。</p>\\n<p>也就是说“如果有足够多的眼睛，那么错误将无处遁形”。</p>\\n</blockquote>\\n<p>开源是 Linux 成功的原因。用户可以准确地了解到他们将获得什么，并且可以报告错误或尝试通过社区的支持自行修复。</p>\\n<p>开源模型也是如此。即便是软件2.0，也需要进行审计。否则，底层变化可能会拖应用程序的后腿。这对大多数商业用例无法接受。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4924ca6ef7024545956e8ace9ae5aea6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1056&amp;h=758&amp;s=336334&amp;e=png&amp;b=f2f2f2\" alt=\"\" loading=\"lazy\"/></p>\\n<p>最近一份论文显示，OpenAI 的端点会随时间而发生变动。</p>\\n<p>采用开源进行 AI 技术的研发可以打造一个广泛的监督和平衡网络。全球的科学家和开发者可以互相评议、批评、研究和理解底层机制，从而提高安全性、可靠性、可解释性和信任度。此外，广泛传播知识可以有效推进技术的发展，同时减轻其被滥用的风险。HuggingFace 就是新的 RedHat。</p>\\n<p>你只能信任你所拥有和控制的模型，黑盒 API 则与此不同。这表明，反对开源 AI 安全性的论点毫无道理。历史经验表明，开源 AI 实际上更安全。</p>\\n<p><strong>4</strong></p>\\n<h1>炒作才是问题所在</h1>\\n<p>为何人们近期更偏向于模型闭源？主要有两个原因：简单易用，有知名度。</p>\\n<p>相较于闭源模型，开源模型更难上手一些。相比使用 OpenAI 的 API，使用开源模型，还需雇佣一个机器学习工程师团队进行开发。这在短期内不可避免，这是掌握控制权和提升创新速度的代价。愿意在前沿领域投入时间的人将有机会构建更优质的产品，其使用体验也将得到不断改进。</p>\\n<p>更棘手的是，闭源已经占据市场主导地位。</p>\\n<p>闭源模型供应商已经在这个 AI 炒作周期中占据了主导地位。人们没时间研究开源模型，也没有意识到开源模型的潜力。人们所熟知的为数不多的工具提供商是 OpenAI、Pinecone 和 LangChain。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7f913cd88e82461b977c7a312a79acce~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=888&amp;h=570&amp;s=257507&amp;e=png&amp;b=fdfcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p>“OPL”堆栈，来自Wen Yang。</p>\\n<p>人们常常误认为最出名的工具即为最合适的工具。在当前的大规模炒作下，闭源 AI 备受关注，可谓是人尽皆知。但随着开源模型的日益成熟，因其对用户的友好性和可定制性，将成为许多应用的更优选择。</p>\\n<p>与其被炒作所席卷，有前瞻性的组织将利用这一时期深入了解自身需求，并为充分利用开源 AI 奠定基础。他们将在开源技术上构建具备竞争优势和差异化的 AI 体验。长期来看，这将为他们带来可持续的竞争优势。</p>\\n<p>对于那些看穿炒作的实用主义者来说，他们始终聚焦于真正开源 AI 的巨大价值，前景依然光明。</p>\\n<p><strong>欢迎 Star、试用 OneFlow 最新版本：</strong></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FOneflow-Inc%2Foneflow%2F\" title=\"https://github.com/Oneflow-Inc/oneflow/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/Oneflow-Inc…</a></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8a68fb0ee41d43d1b92c7220ae30148c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=940&amp;h=529&amp;s=199594&amp;e=png&amp;b=252b29\" alt=\"image.png\" loading=\"lazy\"/></p>', 'author': 'OneFlow一流科技', 'reading_time': 10, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 35, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '780263a769644d0aa9f7c179fbaddb8baa9dca59299ede46bdc0dce71db91c53', 'title': '记录TritonServer部署多模型到多GPU踩坑 | 京东云技术团队', 'url': 'https://juejin.cn/post/7284513473317060669', 'published_at': '2023-10-03T07:40:29Z', 'created_at': '2023-10-04T14:11:40.352428Z', 'changed_at': '2023-10-04T14:11:40.352428Z', 'content': '<h2>一、问题是怎么发现的</h2>\\n<p>部署chatglm2和llama2到一个4*V100的GPU机器上遇到问题</p>\\n<p>config.pbtxt</p>\\n<p>中设置模型分别在指定gpu上部署实例配置不生效</p>\\n<p>如以下配置为在gpu0上部署本模型,部署count=1个实例,在gpu1上部署本模型,部署count=2个实例</p>\\n<p>instance_group [ { count: 1 kind: KIND_GPU gpus: [ 0 ] },</p>\\n<p>{ count: 2</p>\\n<p>kind: KIND_GPU</p>\\n<p>gpus: [ 1 ] } ]</p>\\n<p>部署时发现,所有模型实例都会被部署到gpu0上面, 由于gpu只有16g显存,在部署第一个模型实例成功后,第二个模型实例也会往gpu0上进行加载,最终导致cuda out of memery.</p>\\n<p>网上搜索发现有人遇到同样的问题,链接: <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Ftriton-inference-server%2Fserver%2Fissues%2F6124\" title=\"https://github.com/triton-inference-server/server/issues/6124\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/triton-infe…</a></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1b710c149368473bac73e33b2af42ffd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1073&amp;h=786&amp;s=89969&amp;e=png&amp;b=fffefe\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>二、问题带来的影响</h2>\\n<h2>三、排查问题的详细过程</h2>\\n<p>大佬回答解决方案:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8ebbb6f8013246a084831bad7976c3de~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=944&amp;h=836&amp;s=101750&amp;e=png&amp;b=fffefe\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>四、如何解决问题</h2>\\n<p>1.在model.py手动获取config.pbtxt配置的gpu编号gpus:[0]</p>\\n<p>instance_group [</p>\\n<p>{</p>\\n<p>count: 1</p>\\n<p>kind: KIND_GPU</p>\\n<p>gpus: [ 0 ]</p>\\n<p>}</p>\\n<p>]</p>\\n<p>2.设置可用的GPU编号</p>\\n<p>os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = str(device_id)</p>\\n<p>3.启动成功</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0e0b87b7a39c4e94ab18a8e1c9e2810e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1497&amp;h=473&amp;s=57111&amp;e=png&amp;b=131313\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>五、总结反思:是否可以更快发现问题?如何再次避免等。</h2>\\n<p>triton启动的使用使用 nvidia-smi -l 2 监控显卡想显存, 可以发现所有模型都在往第一个gpu,gpu[0]内加载,发现配置config.pbtxt不生效</p>\\n<blockquote>\\n<p>作者：京东科技 杨建</p>\\n<p>来源：京东云开发者社区 转载请注明来源</p>\\n</blockquote>', 'author': '京东云开发者', 'reading_time': 2, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 34, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'ead4bab2696fcbc4664f562a426217c165727930858d07d65a6816c56602ad13', 'title': 'LangChain大模型应用开发指南-传统编程范式思维的应用', 'url': 'https://juejin.cn/post/7284630352789569576', 'published_at': '2023-10-03T06:01:16Z', 'created_at': '2023-10-04T14:11:40.349544Z', 'changed_at': '2023-10-04T14:11:40.349544Z', 'content': '<h1>LangChain大模型应用开发指南-传统编程范式思维的应用</h1>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e406a92458eb4076a0dc4ee192530379~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3864&amp;h=2577&amp;s=1648232&amp;e=jpg&amp;b=17368b\" alt=\"\" loading=\"lazy\"/></p>\\n<p>上节课，我带领小伙伴们完成了baichuan2量化模型的OpenAI标准接口封装，并完成LangChain对大模型的调用与测试。没有看过的小伙伴可以点击链接查看：</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg2ODA5NTM1OA%3D%3D%26mid%3D2247484193%26idx%3D1%26sn%3D93986a4a4ed7b762e234797cff6b1844%26chksm%3Dceb0c730f9c74e26b424822c7e948d51851b230068157c8c770c3c00110d97f19cb48e8647b5%26token%3D520100448%26lang%3Dzh_CN%23rd\" title=\"https://mp.weixin.qq.com/s?__biz=Mzg2ODA5NTM1OA==&amp;mid=2247484193&amp;idx=1&amp;sn=93986a4a4ed7b762e234797cff6b1844&amp;chksm=ceb0c730f9c74e26b424822c7e948d51851b230068157c8c770c3c00110d97f19cb48e8647b5&amp;token=520100448&amp;lang=zh_CN#rd\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型OpenAI标准接口封装</a></p>\\n<blockquote>\\n<p>今天我们将正式开始LangChain大模型应用开发课程。</p>\\n</blockquote>\\n<h2>组件总览</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c3f51e50c7034c4f9c5b402cbf36e0b1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4963&amp;h=2880&amp;s=1014279&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<p>上图为LangChain的组件与架构图，基本完整描述了LangChain的组件与抽象层以及它们之间的相关联系。主要包括以下六个部分，基于同传统软件开发的比较，我将其分为两个类型：</p>\\n<ol>\\n<li>传统编程思想的AI工程化应用</li>\\n</ol>\\n\\n<pre><code> Chains：链，用于组合组件的结构化方式\\n Agents：代理，用于执行特定任务的组件\\n Callbacks：回调，用于在链中插入自定义逻辑的组件\\n</code></pre>\\n<p>2.  AI大模型衍生的新能力</p>\\n\\n<pre><code> Model I/O：模型输入输出，用于与语言模型交互的组件\\n Retrieval：检索，用于从外部来源获取上下文的组件\\n Memory：记忆，用于存储和更新上下文的组件\\n</code></pre>\\n<p>作为LangChain大模型应用开发指南第一课，我将介绍LangChain的六大核心组件和概念解析，帮助大家从全局的角度了解LangChain在AI应用开发流程中的能力边界，并将在最后给出一个使用到了以上所有相关组件和概念的实际使用示例。</p>\\n<p>受限于文章篇幅，本篇文章主要从传统编程出发，讲解LangChain应用开发中与传统IT应用开发相同的编程范式，本次课程整体提纲如下:</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/17e3eca01f05416bbf8b6986c5cd1226~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=991&amp;h=506&amp;s=67968&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>Chains</h3>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a3ea50f48371437cacb73fffb4818100~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=6176&amp;h=1984&amp;s=756113&amp;e=png&amp;b=f2fcf4\" alt=\"\" loading=\"lazy\"/></p>\\n<p>做过传统软件开发的小伙伴，对链这个名词肯定不陌生，比如Java设计模式里的责任链。LangChain中的Chain也便是大家熟知的链，是LangChain中用于组合组件的结构化方式，<strong>它可以让你按照一定的顺序和逻辑来执行不同的任务</strong>。你可以把它想象成语言模型进行流水线作业，每个环节都有一个特定的目标和功能。Chains有以下四种类型：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>类型</th><th>作用</th></tr></thead><tbody><tr><td>LLMChain</td><td>用于在语言模型周围添加一些功能的简单Chain，它由一个PromptTemplate和一个语言模型（LLM或chat model）组成，它可以使用输入键值和内存键值（如果有的话）来格式化PromptTemplate，然后将格式化后的字符串传递给语言模型，并返回语言模型的输出</td></tr><tr><td>RouterChain</td><td>用于创建一个动态选择下一个要使用的Chain的Chain的范式，它由两个组件组成：RouterChain本身（负责选择下一个要调用的Chain）和destination_chains（RouterChain可以路由到的Chain）</td></tr><tr><td>SequentialChain</td><td>用于将多个Chain连接起来，形成一个执行一些特定场景的管道的Chain，它允许将一个调用的输出作为另一个调用的输入</td></tr><tr><td>TransformChain</td><td>用于创建一个通用的转换Chain，它可以对输入进行一些预处理或后处理，然后将其传递给另一个Chain</td></tr></tbody></table>\\n<p>Chains是LangChain中完成业务逻辑拆解和组装的核心组件，它可以让你灵活地构建复杂的应用程序。你可以使用不同类型的Chains来实现大模型、现有数据等各种各样基础能力的组装与能力搭建。</p>\\n<ul>\\n<li>如下代码是使用Chain整合提示词模板，OpenAI接口，输出文本格式化解析器的一个简单示例</li>\\n</ul>\\n\\n<pre><code>from langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import ChatPromptTemplate\\nfrom langchain.schema import BaseOutputParser\\n\\nclass CommaSeparatedListOutputParser(BaseOutputParser):\\n    &#34;&#34;&#34;Parse the output of an LLM call to a comma-separated list.&#34;&#34;&#34;\\n\\n\\n    def parse(self, text: str):\\n        &#34;&#34;&#34;Parse the output of an LLM call.&#34;&#34;&#34;\\n        return text.strip().split(&#34;, &#34;)\\n\\ntemplate = &#34;&#34;&#34;You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.&#34;&#34;&#34;\\nhuman_template = &#34;{text}&#34;\\n\\nchat_prompt = ChatPromptTemplate.from_messages([\\n    (&#34;system&#34;, template),\\n    (&#34;human&#34;, human_template),\\n])\\nchain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\\nchain.invoke({&#34;text&#34;: &#34;colors&#34;})\\n\\n</code></pre>\\n<h3>Agents</h3>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2ecf3adde1314d7f971285706ebe5991~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=194&amp;s=37933&amp;e=png&amp;b=fdfdfd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>和责任链一样，Java设计模式还有一个设计模式叫代理模式。LangChain中Agents的概念与传统的代理模式基本一致，是通过对稳定的核心能力进行封装，通过Agents代理提供访问入口，既保证了对多样化业务能力需求的支持，也能保持核心能力的稳定性。</p>\\n<p>LangChain中Agents与传统代理模式的区别就在于，其代理的核心能力便是AI大模型。通过Agents，我们可以让语言模型具有主动性和智能性。你可以把它想象成语言模型的打工人，它可以根据你的需求和指示来完成各种各样的工作。LangChain基于对AI应用开发的总结抽象，主要提供以下六种代理类型：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>类型</th><th>作用</th></tr></thead><tbody><tr><td>Zero-shot ReAct</td><td>用于根据工具的描述来决定使用哪个工具的Agent，它可以使用任意数量的工具，但要求每个工具都有一个描述</td></tr><tr><td>Structured input ReAct</td><td>用于使用多输入工具的Agent，它可以根据工具的参数模式来创建一个结构化的动作输入，这对于更复杂的工具使用，如在浏览器中精确地导航，很有用</td></tr><tr><td>OpenAI Functions</td><td>用于与一些特定的OpenAI模型（如gpt-3.5-turbo-0613和gpt-4-0613）协作的Agent，这些模型已经被显式地微调过，以便检测何时应该调用一个函数，并返回应该传递给函数的输入</td></tr><tr><td>Conversational</td><td>用于在对话场景中使用的Agent，它的提示被设计成使Agent有助于和对话，它使用ReAct框架来决定使用哪个工具，并使用内存来记住之前的对话交互</td></tr><tr><td>Self-ask with search</td><td>用于利用一个名为Intermediate Answer的工具的Agent，这个工具应该能够查找问题的事实答案，这个Agent相当于原始的自问自答与搜索论文，其中提供了一个Google搜索API作为工具</td></tr><tr><td>ReAct document store</td><td>用于与文档存储交互的Agent，它使用ReAct框架，并且必须提供两个工具：一个Search工具和一个Lookup工具（它们必须被准确地命名为这样），Search工具应该搜索一个文档，而Lookup工具应该在最近找到的文档中查找一个术语，这个Agent相当于原始的ReAct论文，特别是Wikipedia示例</td></tr></tbody></table>\\n<p>Agents是LangChain工程化封装与能力复用核心手段，可以动态的帮我们选择和调用 Chain 或者已有的工具，实现面向多种应用场景的智能适配。</p>\\n<ul>\\n<li>如下代码是使用Self-ask with search完成查询链</li>\\n</ul>\\n\\n<pre><code>from langchain.llms import OpenAI\\nfrom langchain.utilities import SerpAPIWrapper\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType\\n\\nllm = OpenAI(temperature=0)\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name=&#34;Intermediate Answer&#34;,\\n        func=search.run,\\n        description=&#34;useful for when you need to ask with search&#34;,\\n    )\\n]\\n</code></pre>\\n<h3>Callbacks</h3>\\n<p>回调这个概念不用多说，各种编程语言的回调函数就是具体的应用。\\nLangChain中Callbacks是用于在链中插入自定义逻辑的组件，它可以让你对语言模型的行为进行控制和调整和调整。你可以把它理解成餐厅排队的叫号，当排队到你的时候就会触发提醒你用餐。同样在LangChain中，它可以在链中的任何位置执行你定义的函数或代码。。LangChain基于对AI应用开发的总结抽象，主要提供以下七种回调类型：</p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<table><thead><tr><th>类型</th><th>作用</th></tr></thead><tbody><tr><td>Async callbacks</td><td>用于在异步模式下执行回调函数的类，它可以让您在不阻塞主线程的情况下，监听和处理LLM应用的各个阶段的事件</td></tr><tr><td>Custom callback handlers</td><td>用于自定义回调函数的类，它可以让您实现一些特定的逻辑，如验证，过滤，转换等，以及定义回调函数在哪些事件上被触发</td></tr><tr><td>Callbacks for custom chains</td><td>用于为自定义的Chain添加回调函数的类，它可以让您在Chain的开始，结束，或每个组件的调用之前或之后执行一些操作</td></tr><tr><td>Logging to file</td><td>用于将LLM应用的日志记录到文件中的类，它可以让您指定日志文件的路径，格式，级别等，并在每次LLM应用运行时自动写入日志信息</td></tr><tr><td>Multiple callback handlers</td><td>用于同时使用多个回调函数的类，它可以让您将不同类型或功能的回调函数组合起来，并按照一定的顺序执行</td></tr><tr><td>Tags</td><td>用于给回调函数添加标签的类，它可以让您根据标签来筛选或分组回调函数，并在一些特定的场景中使用它们</td></tr><tr><td>Token counting</td><td>用于统计LLM应用使用的令牌数量的类，它可以让您监控和控制LLM应用的消耗，并在每次LLM应用运行时自动更新令牌计数</td></tr></tbody></table>\\n<p>Callbacks是LangChain中实现自定义逻辑插入的入口，它可以让你的任务链达到合适场景时执行你自定义的功能和效果。</p>\\n<ul>\\n<li>如下是LangChain内置的标准输出的回调演示</li>\\n</ul>\\n\\n<pre><code>from langchain.callbacks import StdOutCallbackHandler\\nfrom langchain.chains import LLMChain\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\n\\nhandler = StdOutCallbackHandler()\\nllm = OpenAI()\\nprompt = PromptTemplate.from_template(&#34;1 + {number} = &#34;)\\n\\n\\nchain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\\nchain.run(number=2)\\n\\n\\nchain = LLMChain(llm=llm, prompt=prompt, verbose=True)\\nchain.run(number=2)\\n\\n\\nchain = LLMChain(llm=llm, prompt=prompt)\\nchain.run(number=2, callbacks=[handler])\\n</code></pre>\\n<h2>总结</h2>\\n<p>本文以传统应用编程设计模式和思维为入口和对比对象，介绍了LangcChain中的Chain、Agent、Callback三大核心概念，并整理了LangcChain为众多开发者内置的能力与工具。</p>\\n<p>通过本文的指导，读者可以迁移在传统应用编程中累积的思维方式和经验，快速入门掌握AI应用编程。</p>\\n<p><strong>下一节课我们将着重讲解，LangChain架构在传统应用开发基础上通过AI大模型衍生出的新能力，请大家持续关注。</strong></p>', 'author': 'AI小智', 'reading_time': 11, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 33, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '4b9d3340e1ea8b6d7ce792648d54c4ab4babb7b07b641eeea8e40449978c2490', 'title': 'Fooocus 超过 180 种风格的傻瓜式 AI 图片生成工具', 'url': 'https://juejin.cn/post/7284645764788535332', 'published_at': '2023-10-03T04:53:33Z', 'created_at': '2023-10-04T14:11:40.346861Z', 'changed_at': '2023-10-04T14:11:40.346861Z', 'content': '<blockquote>\\n<p>Fooocus 对 Stable Diffusion 和 Midjourney 设计的重新思考，提供超过 180 种风格的插图，展示了丰富多样的艺术表现形式。</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6036573b78114cdfbc394c9990c4e3ab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1600&amp;h=960&amp;s=493865&amp;e=jpg&amp;b=06192e\" alt=\"image.jpg\" loading=\"lazy\"/></p>\\n<h3>什么是Fooocus？</h3>\\n<p>虽然 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FAUTOMATIC1111%2Fstable-diffusion-webui\" title=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">StableDiffusionWebUI</a> 拥有强大的功能，但很多用户更喜欢在 Midjourney 中使用提示的简单方法。受到 Stable Diffusion 和 StableDiffusionWebUI 的 ControlNet 等扩展的启发，ControlNet 的作者 lllyasviel 推出了一款名为 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Flllyasviel%2FFooocus\" title=\"https://github.com/lllyasviel/Fooocus\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Fooocus</a> 的新软件。</p>\\n<h4>简化的交互</h4>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Flllyasviel%2FFooocus\" title=\"https://github.com/lllyasviel/Fooocus\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Fooocus</a> 的目的是让用户只关注提示，而无需了解数百个设置。Fooocus 从 StableDiffusionWebUI、ComfyUI 和 Midjourney 的仅提示图像生成方法中汲取灵感，是 Stable Diffusion 的重新设计版本，以提示使用为中心，自动处理其他设置。目标是成为简单易用、高质量的图像生成软件。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7e6ff608b3764ca78bd358ee09d4b3c5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2034&amp;h=298&amp;s=31884&amp;e=jpg&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>上面为主界面，如 StableDiffusionWebUI，是使用Gradio构建的。而 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Flllyasviel%2FFooocus\" title=\"https://github.com/lllyasviel/Fooocus\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Fooocus</a> 仅由一个提示输入框和一个Generate按钮组成，保持界面简约、整洁。</p>\\n<h4>减少内存使用</h4>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Flllyasviel%2FFooocus\" title=\"https://github.com/lllyasviel/Fooocus\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">Fooocus</a> 的另一个值得注意的功能是成功生成 SDXL 图像所需的内存<code>VRAM</code>极少。官方声明称，即使只有 <code>6GB VRAM</code>，该软件也能有效运行。这是一项重大改进，与之前一样，使用 <code>SD 1.5</code> 生成 SDXL 图像需要更多的内存<code>VRAM</code>。现在，即使内存<code>VRAM</code>有限的用户也可以顺利创建 SDXL 图像。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/89e8bd53760c4ab2af1719ca5cca5a31~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1920&amp;h=685&amp;s=717740&amp;e=png&amp;b=fcfbfb\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>安装Fooocus</h3>\\n<p>从这里开始介绍如何安装。</p>\\n<p><strong>项目的github地址</strong>：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Flllyasviel%2FFooocus\" title=\"https://github.com/lllyasviel/Fooocus\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/lllyasviel/…</a></p>\\n<h4>Windows版本</h4>\\n<ol>\\n<li>从 Github 下载 zip 文件：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Flllyasviel%2FFooocus%2Freleases%2Fdownload%2Frelease%2FFooocus_win64_2-0-50.7z\" title=\"https://github.com/lllyasviel/Fooocus/releases/download/release/Fooocus_win64_2-0-50.7z\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/lllyasviel/…</a></li>\\n<li>解压并运行 <code>run.bat</code> 来进行安装\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ca5a6db944c54e13b1c92e7290b08ac8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=580&amp;h=265&amp;s=38431&amp;e=png&amp;b=202020\" alt=\"image.png\" loading=\"lazy\"/></li>\\n<li>首次运行时，软件会自动下载 <code>sd_xl_base_1.0_0.9vae.safetensors</code>、<code>sd_xl_refiner_1.0_0.9vae.safetensors</code>等大文件，大小均超过 <code>6GB</code>。这个过程可能需要一些时间。</li>\\n</ol>\\n<h4>MacOS / Linux 版本</h4>\\n<p>在继续之前，可以参阅 StableDiffusionWebUI 和 Python 设置的安装指南。如果已经安装了Python，则可以跳过这些步骤。</p>\\n<ol>\\n<li>创建目录，使用 Git 将 Fooocus 文件下载到文件夹</li>\\n</ol>\\n<pre><code>git clone https://github.com/lllyasviel/Fooocus.git\\n</code></pre>\\n<ol>\\n<li>设置虚拟环境（venv）<code>python3 -m venv env</code></li>\\n<li>激活虚拟环境 <code>source env/bin/activate</code></li>\\n<li>安装所需的Python包 <code>pip install pygit2==1.12.2</code></li>\\n<li>安装完成后，使用以下命令启动 Fooocus <code>python3 entry_with_update.py --share</code></li>\\n<li>首次运行时，软件会自动下载 <code>sd_xl_base_1.0_0.9vae.safetensors</code>、<code>sd_xl_refiner_1.0_0.9vae.safetensors</code>等大文件，大小均超过 <code>6GB</code>。这个过程可能需要一些时间。</li>\\n</ol>\\n<p>成功之后会生成一个零时的URL。</p>\\n<h3>基本用法</h3>\\n<p>使用 Fooocus 非常简单，只需在提示框中输入提示，然后单击<code>Generate 生成</code>即可。默认情况下，软件会生成两张具有电影风格的 <code>1152 x 896</code> 图像。</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/77af5fd6c8794caaa1c7bb510812ef3a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1669&amp;h=919&amp;s=864562&amp;e=png&amp;b=fcf8f8\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>高级设置</h3>\\n<p>会注意到提示输入框下方有一个<code>高级</code>按钮，可引导进行高级设置。</p>\\n<h4>Setting 设置</h4>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/390b7a1bfac94edcba6aa59351aa27c4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1584&amp;h=908&amp;s=819268&amp;e=png&amp;b=fefdfd\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ul>\\n<li><code>Performance</code> 性能：默认 <code>Speed</code> 选项使用30步骤生成图像，<code>Quality</code> 需要更长的时间，但使用60步骤可以获得更好的结果。</li>\\n<li><code>Aspect Ratios width × height</code> 宽高比：默认设置为 <code>1152 x 896</code>。</li>\\n<li><code>Image Number</code> 图像数量：生成的图像数量（最多32）。</li>\\n<li><code>Negative Prompt</code> 否定提示：不希望出现在生成的图像中的元素或概念。它允许指定应从图像输出中排除的内容。</li>\\n<li><code>Random</code> 随机种子：默认是随机的，也可以使用自定义 <code>Seed</code> 种子数。</li>\\n</ul>\\n<h3>Style 风格</h3>\\n<p>此部分设置生成的图像样式，默认设置为样式 <code>cinematic-default</code> 。有超过 100 种不同的款式可供选择。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4354010d98f54e0ab6ec74854a7daec8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3360&amp;h=2030&amp;s=2103013&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>虽然名称可能会对这种风格有一个大概的了解，但如果没有视觉参考，要完全理解通常是很困难的。为了更加直观的感受Style的效果，这里将使用提示 <code>1 beautiful girl</code> 为每种样式生成分辨率为 <code>1024 x 960</code> 的图像。这些参考图像提供了一种比较风格的实用方法。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/31e04742d9e149c4a5bfd8237c6f0420~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=616296&amp;e=jpg&amp;b=a2a3a6\" alt=\"Ads Fashion Editorial.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50b68108ac37478b88c49530c5a53990~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=618521&amp;e=jpg&amp;b=e0d7d2\" alt=\"Ads-Advertising.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f3345ea170f94313887ca2d278be306e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=455936&amp;e=jpg&amp;b=272a31\" alt=\"Ads-Automotive.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a1789973b21b409d833eb1764b6f3274~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=310277&amp;e=jpg&amp;b=f5f1f1\" alt=\"Ads-Corporate.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bdb56fc699a54957889c7d3266f3e0bf~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=483756&amp;e=jpg&amp;b=807b74\" alt=\"Ads-Food-Photography.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6377066d341548d1b8ce8af9bedc64b9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=725803&amp;e=jpg&amp;b=e0d5cc\" alt=\"Ads-Luxury.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/59beb204a780453498dcbc054d468665~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=595185&amp;e=jpg&amp;b=cdc4bb\" alt=\"Ads-Real-Estate.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/213bdee68fab49109f9645f7738f21ac~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=621972&amp;e=jpg&amp;b=d5d0cf\" alt=\"Ads-Retail.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e25efa34f1a14bd3bce8495551d52d2c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=701613&amp;e=jpg&amp;b=d8cb1c\" alt=\"Fortnite-Art-Style.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a541e3fd529a4e649ae8fc9cfb1ba74f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=678174&amp;e=jpg&amp;b=121c24\" alt=\"Futuristic-Biomechanical-Cyberpunk.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e56bb6d7f1a04776b4b9cb6ac8928901~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=546565&amp;e=jpg&amp;b=f5f1f6\" alt=\"Futuristic-Biomechanical.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8e187c92b71b4457889706a006ed66a0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=472703&amp;e=jpg&amp;b=3e2a1f\" alt=\"SAI-3D-Model.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b89054cc62654abfb5de9e8eefd1767e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=680686&amp;e=jpg&amp;b=f6a030\" alt=\"Pop-Art-2.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9e5878e2c6204892ad0873a1bdba2947~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=383846&amp;e=jpg&amp;b=1e1d17\" alt=\"SAI-Analog-Film.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3cbf2f6587d54c408ad98e0361221eb8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=808794&amp;e=jpg&amp;b=e7a973\" alt=\"SAI-Anime.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/75b6ed1fd53f4ef4bd48e0638d852cbb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=375203&amp;e=jpg&amp;b=37241b\" alt=\"SAI-Cinematic.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7254f10392bd44a5b25be6642d3049fa~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=814315&amp;e=jpg&amp;b=e6e0df\" alt=\"SAI-Comic-Book.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/100df6fb11104bc6845583c4c1f110dc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=472650&amp;e=jpg&amp;b=d7d2b6\" alt=\"SAI-Craft-Clay.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8280a2cc1b0e43bc96d94eeeb52efaad~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=560580&amp;e=jpg&amp;b=7f9096\" alt=\"SAI-Digital-Art.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c3142d568cb749b08893b35c2d36a2fe~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=689581&amp;e=jpg&amp;b=e5ccb6\" alt=\"SAI-Enhance.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b6ded85224594452b08944b80decf464~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=465812&amp;e=jpg&amp;b=e3ab86\" alt=\"SAI-Fantasy-Art.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/65651ad3b1e9469da7d79979808d2ff4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=742197&amp;e=jpg&amp;b=eb9c6f\" alt=\"SAI-Isometric.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46f1f725e476485baaf66b4fdce22d4b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=425857&amp;e=jpg&amp;b=f3f3f3\" alt=\"SAI-Line-Art.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff8d9c363f864cb991c8c84fb83a2085~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=960&amp;s=390238&amp;e=jpg&amp;b=f8f0ee\" alt=\"SAI-Lowpoly.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/521a5ce25b44478dbf825d04474defeb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2078&amp;h=980&amp;s=574320&amp;e=jpg&amp;b=2c1939\" alt=\"SAI-Neonpunk.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/134ac03647cc4bc2ad39effdfbd858b1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2078&amp;h=980&amp;s=866618&amp;e=jpg&amp;b=f7eee9\" alt=\"Whimsical-And-Playful.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/87ed0d4263cc4d689d2b9f133cbef68e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2078&amp;h=980&amp;s=637109&amp;e=jpg&amp;b=7b533f\" alt=\"SAI-Texture.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c2e80ebbf7074fe588eb352d38da8574~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2078&amp;h=980&amp;s=367297&amp;e=jpg&amp;b=678d62\" alt=\"SAI-Pixel-Art.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2c64a41426894dc8806c31899650c3a3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2078&amp;h=980&amp;s=413037&amp;e=jpg&amp;b=36221c\" alt=\"SAI-Photographic.jpg\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a1d5868ca1ce4ae697fefaff13b84d74~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2078&amp;h=980&amp;s=474718&amp;e=jpg&amp;b=d9c4df\" alt=\"SAI-Origami.jpg\" loading=\"lazy\"/></p>\\n<h3>总结</h3>\\n<p>虽然仍处于早期开发阶段，Fooocus 已经带来了相当大的兴奋。其简单的操作和资源节省功能是 Stable Diffusion 的用户友好增强功能，Stable Diffusion 是一种以其最初陡峭的学习曲线而闻名的工具。希望 Fooocus 能够不断发展成为一款功能强大且用户友好的软件，让更多的人能够参与到人工智能驱动的艺术创作中。</p>', 'author': '天行无忌', 'reading_time': 5, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 32, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'b5ac5a49d8611c17cb7396dab540d6debc8def7a35ca8c7a0262abf7ff9df095', 'title': '神器 CodeWhisperer', 'url': 'https://juejin.cn/post/7284619312643080248', 'published_at': '2023-10-03T03:24:06Z', 'created_at': '2023-10-04T14:11:40.344838Z', 'changed_at': '2023-10-04T14:11:40.344838Z', 'content': '<p>这两天看到了好多关于 Amazon CodeWhisperer 针对个人用户终身免费使用的消息，便抽空简单来重点介绍下如何在 VS Code 这款 IDE 上安装和使用 CodeWhisperer。</p>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">亚马逊云科技开发者社区</a>为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，点<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpassport.amazoncloud.cn%2Fsignin%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://passport.amazoncloud.cn/signin?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">这里</a>让它成为你的技术宝库！</p>\\n</blockquote>\\n<p>CodeWhisperer 是亚马逊出品的一款基于机器学习的通用代码生成器，可实时提供代码建议。类似 Cursor 和 Github</p>\\n<p>Amazon CodeWhisperer</p>\\n<p>Amazon CodeWhisperer 是 Amazon 于2021年12月推出的一款代码补全工具，与 GitHub Copilot 类似。主要的功能有:</p>\\n<p>代码补全</p>\\n<p>注释和文档补全</p>\\n<p>代码安全问题的辅助定位</p>\\n<p>CodeWhisperer 主要由 Java、Python、JavaScript、TypeScript、C# 相关语料训练而成，在支持上述语言的同时，也支持 Ruby、Go、PHP、C++、C、Shell、Scala、Rust、Kotlin、SQL 等。</p>\\n<p>安装教程：</p>\\n<ol>\\n<li>在菜单顶部，单击 Marketplace 并在搜索栏中输入“Amazon Tookit”。点击安装</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3922a778fe02473082cfe9bce2505a44~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1270&amp;h=1386&amp;s=861923&amp;e=png&amp;b=1e1e1e\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>点击“Developer Tools”标签，选择“CodeWhisperer/Start”在弹出的窗口中选择第一栏，点击“connect”</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3442786516334811b6bbcf58bc886eda~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1271&amp;h=1392&amp;s=883223&amp;e=png&amp;b=1e1e1e\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>点击“Copy Code”</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/76c1af66f56d416ea7c37504dfe8193f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1270&amp;h=1390&amp;s=523510&amp;e=png&amp;b=1f1f1f\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>点击“open”</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7f9a5fee3cc04724b9bb1d435aa8ad4e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=719&amp;h=782&amp;s=234264&amp;e=png&amp;b=1d1d1d\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>点击后自动弹出一个页面，将上一步复制的 Code 粘贴过来，点击“Next”</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c24ba9526d08469eae01f6aa033a1634~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=785&amp;s=235103&amp;e=png&amp;b=1d1d1d\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>输入信息并验证</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/979a4e6ac1724f69b709ded16ada5d6c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=582&amp;h=509&amp;s=92528&amp;e=png&amp;b=faf9f9\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>点击 allow</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/089298ce46504581bc164e3c094e8ea8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=699&amp;h=585&amp;s=150830&amp;e=png&amp;b=fdfcfc\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>出现下图后就说明注册成功啦</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e6149dfbc32410fbf8a956f9c8e8c4d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=684&amp;h=455&amp;s=58949&amp;e=png&amp;b=f9f8f8\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>下面谈谈我对亚马逊云科技的一些看法吧：</p>\\n<p>“像其他任何 IT 采购一样采购专网”，对于小型部署或者没有任务关键需求和业务关键需求并且可能没有大量资本支出用于投资的中小企业来说，它将5G专网视为通向更高价值服务和整个数字化过程的渠道。亚马逊云科技提供计算、机器学习分析和边缘计算，并支持联网资产上的应用程序和解决方案，以帮助企业实现数字化之旅。现在，它还提供专有蜂窝网络连接作为提升其所有更广泛产品的渠道。</p>\\n<p>当然其中也有不尽完美的地方：亚马逊云科技认为，用户会指定其所需的网络、容量等，并通过一次单击订购这些资源。然而，多数企业并不知道它们需要什么，并且需要有人（合作伙伴）在其探索过程中来指导它们。除了核心网和RAN公司以及 Dish（其 DNA 和与亚马逊云科技的合作伙伴关系值得单独报告）外，A亚马逊云科技能会加强拉拢其他合作伙伴，例如系统集成商和垂直领域专业企业，从而确保它们能够解决这些方案实施的复杂性——尤其是随着网络的发展，复杂性会愈发增强。</p>\\n<p>最后跟大家分享我个人使用 CodeWhisperer 的体验和观点吧：</p>\\n<ol>\\n<li>强大的自动补全功能：CodeWhisperer 的自动补全功能相当出色。它能够根据上下文提供准确的建议，节省了大量的时间和精力。我发现它能够准确预测我想要使用的变量、函数和类名，并提供相应的补全选项。这种智能化的补全功能使我的编码过程更加流畅，减少了犯错的可能性。</li>\\n<li>实时错误检测和建议：CodeWhisperer 具备出色的实时错误检测能力。它会在我编写代码的同时，发现并提示潜在的错误和问题。例如，当我遗漏了一个括号、忘记了引入某个模块或使用了错误的函数参数时，它会立即给出相应的警告。此外，CodeWhisperer 还提供了改进代码质量的建议，帮助我遵循最佳实践和规范。</li>\\n<li>智能重构和优化建议：CodeWhisperer 能够帮助我进行智能的代码重构和优化。它能够检测到重复代码、复杂的逻辑和低效的操作，并提供相应的重构建议。这使得我的代码更加清晰、易于理解和维护。我发现这个功能对于大型项目的开发尤为有用，它能够帮助我提高代码的可读性和性能。</li>\\n<li>多语言支持和广泛的框架集成：CodeWhisperer 支持多种编程语言和广泛的框架。无论是 Python、Java 还是 JavaScript，它都能提供高效的支持和建议。此外，CodeWhisperer 还与许多流行的开发工具和集成开发环境（IDE）兼容，如 Visual Studio Code、PyCharm 等。这为开发者提供了更大的灵活性和便利性。</li>\\n</ol>\\n<p>总结：CodeWhisperer 是一款卓越的编码助手，通过强大的自动补全、实时错误检测、智能重构和优化建议等功能，大大提高了我的编码效率和代码质量。</p>\\n<p>总之，感兴趣的小伙伴可以上手试一试啦。</p>\\n<p>文章来源：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F64f88344d75d3d261ac2a214%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/column/article/64f88344d75d3d261ac2a214?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">dev.amazoncloud.cn/column/arti…</a></p>', 'author': '亚马逊云开发者', 'reading_time': 5, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 31, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'e326fa7f80c8df55a93839cc915fc0c20b06beffbe53d7a9cde2a7f4ba90c75d', 'title': '90+ Midjourney 产品摄影提示词 (附图)，提升销量必备，建议收藏', 'url': 'https://juejin.cn/post/7284608063913623571', 'published_at': '2023-10-03T00:48:24Z', 'created_at': '2023-10-04T14:11:40.338121Z', 'changed_at': '2023-10-04T14:11:40.338121Z', 'content': '<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3304f67004d14efeb511bf33a5c71167~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4408&amp;h=2476&amp;s=3285765&amp;e=png&amp;b=e6e4e3\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在我们追求创新和差异化的数字时代，如何展示一个产品变得越来越关键。Midjourney，这个前卫的文本到图像的AI图像生成器，为我们提供了一个独特的方式，用语言的力量转化为惊艳的视觉体验。无论你是想要呈现一个细致的艺术风格，一个生动的构图，还是一个令人难以忘怀的背景，Midjourney 都可以为你打造出完美的图像。</p>\\n<p>接下来，我们将深入探讨如何利用 Midjourney 提示词，为你的产品打造出独特的视觉形象。从风格到构图，再到背景，每一个细节都有可能成为你产品故事的核心。本文不仅展示了 90 多个提示词，还为每一个提示词都附上了图片，希望给你带来灵感。</p>\\n<h2>风格</h2>\\n<p>在 Midjourney 中，我们可以通过提示词为产品指定不同的视角、风格和技术，将产品呈现得栩栩如生，令人难以忘怀。有无数的风格和技巧可以帮助我们实现这一目标。每一种风格和技术都有其独特的魅力和应用场景。现在，就让我们深入探讨这些风格和提示词技巧，看看它们是如何为我们的产品添彩的吧！</p>\\n<h3>高调</h3>\\n<p>高调风格主要以浅色调和色彩为主，很少有中间色调或阴影。它传达一种欢快、简洁、大气的氛围。这种风格可以强调产品的外形和细节，通常以白色或浅色背景为背景。你可以用这个模板来写提示词：high key product photography of [product]。</p>\\n<blockquote>\\n<p>Prompt: high key product photography of a sleek and minimalist desktop computer --ar 16:9 (时尚简约台式电脑的高调产品摄影)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9b700d376ee2423cad0bf2b640a45b60~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=368669&amp;e=png&amp;b=cde6f3\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: high key product photography of a high-end espresso machine, sleek lines, metallic sheen (高端意式咖啡机的高调产品摄影，流畅的线条，金属光泽)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3237e2ed844d4fd589cdacfa1130947e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=370131&amp;e=png&amp;b=ece6e5\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: high key product photography of a set of handcrafted ceramic vases, intricate details, vibrant colors (一组手工陶瓷花瓶的高调产品摄影，细节精致，色彩鲜艳)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/18ce3ed9daa74a6f8fa544745a8478d2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=425646&amp;e=png&amp;b=ddd7d3\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>低调</h3>\\n<p>低调风格突出明暗区域之间的强烈对比，尽量减少中间色调。这种风格表现情绪化、戏剧化。这种风格的产品图在较暗的背景下突出特定的细节或轮廓。你可以用这个模板来写提示词：low key product photography of [product]。</p>\\n<blockquote>\\n<p>Prompt: low key product photography of a high-end sound system --ar 16:9 (高端音响系统的低调产品摄影)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40b6fba7bcd1403e8d3fedb37b35d53e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=383684&amp;e=png&amp;b=0b0a0a\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: low key product photography of a high-end fragrance bottle, highlighting the intricate glass details and deep amber liquid (高端香水瓶的低调产品摄影，突出复杂的玻璃细节和深琥珀色液体)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e06ff42fe0cf43f78902fdb0c809bbbb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=405670&amp;e=png&amp;b=290e04\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: low key product photography of a gourmet dark chocolate bar, glossy surface, exquisite packaging --ar 16:9 (美食黑巧克力棒的低调产品摄影，光亮的表面，精致的包装)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/38545eedd5344e5fa88280507b5ea53d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=530946&amp;e=png&amp;b=0a0909\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>极简</h3>\\n<p>极简主义强调简洁，利用大量负空间将焦点吸引到主体上。这种风格突出了产品的设计、外形和功能，减少任何干扰因素。</p>\\n<blockquote>\\n<p>Prompt: minimalist product photography of a contemporary, chrome-plated kitchen faucet with clean lines (线条简洁的现代镀铬厨房龙头的简约产品摄影)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e1d3f2a703e1471786b3c1d866b81662~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=240481&amp;e=png&amp;b=526f8c\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: minimalist product photography of a sleek, stainless steel water filter pitcher (时尚不锈钢滤水壶的简约产品摄影)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/921435d236174c5ab865a5a4cbf31b40~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=325641&amp;e=png&amp;b=221815\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: minimalist product photography of a pair of wooden headphones with clean, unadorned ear cups (一副木质耳机的简约产品摄影，耳罩简洁、不加修饰)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/540cdc6f0f6b41db997817916620a079~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=320827&amp;e=png&amp;b=8c694b\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>生活化 (Lifestyle)</h3>\\n<p>这种风格是以生活化的方式展现产品在现实生活中的情景或设置，强调产品如何融入日常生活。这样是为了说明产品如何日常工作、活动或环境完美融合，引起消费者的共鸣。你可以用这样的模板来写提示词：A close-up shot of [product] being held by a [person] in an indoor/outdoor setting。</p>\\n<blockquote>\\n<p>Prompt: a close-up shot of a video game controller being held by a gamer in a gaming den (一个游戏玩家在游戏室里手持电子游戏控制器的特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ca672329ed6404ba70b653f4d199ae8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=404577&amp;e=png&amp;b=241a22\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a close-up shot of a DSLR camera being held by a professional photographer in a photography studio (专业摄影师在摄影棚内手持数码单反相机的特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/64475674982b4567912653abd05f4220~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=509201&amp;e=png&amp;b=111012\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a close-up shot of a luxury handbag being held by a fashionable woman in a high-end boutique (高端精品店中时尚女性手持奢侈品手袋的特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7490512f4c524225b1d2d7e9bad52bac~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=407855&amp;e=png&amp;b=14171a\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a close-up shot of a portable speaker being held by a group of friends at an outdoor music festival (户外音乐节上一群朋友手持便携式扬声器的特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/824a9a7b52574bcfa70e24de49c5d9ab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=370234&amp;e=png&amp;b=faede2\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a close-up shot of a Chanel perfume bottle being held by a glamorous actress on the red carpet (红地毯上魅力四射的女演员手持香奈儿香水瓶的特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9de49e4444fc4124836af9e039afad73~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=455315&amp;e=png&amp;b=2b1e16\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a close-up shot of a watch being held by a pilot in a private jet cabin (私人飞机机舱内飞行员手持手表的特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f1ce0d659089436a977e5cc8d7c2c935~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=397063&amp;e=png&amp;b=0a0c15\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>微距</h3>\\n<p>这种风格用于展示肉眼无法看到的复杂细节、纹理和特征，可以产生独特的视觉冲击感。</p>\\n<blockquote>\\n<p>Prompt: dew-covered beer bottle, beverage macro, outdoor environment, cool and refreshing lighting, commercial style (露水覆盖的啤酒瓶，饮料微距，户外环境，清凉爽口的灯光，商业风格)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40d2476f53254a11be8ee6e5e1737282~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=458402&amp;e=png&amp;b=0a2a04\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: close-up shot of a intricate silver necklace, soft natural light, elegant, minimalist, pastel color (精致银项链特写镜头，柔和的自然光，优雅、简约，粉色系)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c57063dfb03446e96ca1c2b57dfe5b7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=468942&amp;e=png&amp;b=ebdfd0\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: extreme close up shot of a futuristic smartwatch, high-tech studio lighting, sleek and metallic colors, futuristic (未来派智能手表的极致特写镜头，高科技摄影棚照明，光滑的金属色，未来派风格)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bbbb2f8bdde142c0832dc0f5357fa453~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=386980&amp;e=png&amp;b=0f0b15\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>平铺 (Flat Lay)</h3>\\n<p>这种风格是指从自上而下的角度展现产品，通常是整齐地排列在平面上。它以美观的排列方式展示多个物品，强调产品之间的关系或展示一个系列。</p>\\n<blockquote>\\n<p>Prompt: flat lay photography of a colorful assortment of makeup products, soft diffused light, playful and girly, pastel color palette --ar 16:9 (平铺摄影，色彩缤纷的化妆产品，柔和的散射光，俏皮而少女，柔和的色调)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4da47eff596b4a5e94a797659fcbd906~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=586595&amp;e=png&amp;b=ebcdec\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: flat lay photography of a tropical cocktail assortment, minimalist (热带鸡尾酒品种的平面摄影，简约风格)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5090ef832e9d48309dc1bf1e82d32d97~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=469287&amp;e=png&amp;b=e6eaed\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: flat lay photography of a collection of artisan pottery, diffused natural light --ar 16:9 (flat lay photography of a collection of artisan pottery, diffused natural light)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9b973e9e70f749ada280e6b3682e6f87~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=789995&amp;e=png&amp;b=252328\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>运动模糊</h3>\\n<p>这种风格可以展现产品的动态，通过动态模糊来创造视觉吸引力或强调产品的动态性能。</p>\\n<blockquote>\\n<p>Prompt: speed and motion captured in a spinning fidget spinner, studio setup, neon colors, exciting and playful, top-down perspective, long exposure (在旋转的小玩意儿中捕捉速度和运动，摄影棚设置，霓虹色彩，令人兴奋和俏皮，俯视视角，长时间曝光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4e787aebfecb4f0ca2b78895e5ed0f76~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=268919&amp;e=png&amp;b=04142c\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: dynamic and action-packed remote-controlled car race, outdoor track, golden hour lighting, vibrant and competitive, panning motion blur --ar 16:9 (动感十足的遥控赛车比赛，室外赛道，黄金时间照明，充满活力和竞争性，平移运动模糊)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/37932aea8feb4b0ca0e73a7338c80d83~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=354992&amp;e=png&amp;b=e39228\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a sleek smartphone in motion blur, bustling city backdrop, neon city lights, futuristic and high-tech, close-up shot, speed and motion (运动模糊中的时尚智能手机、繁华城市背景、城市霓虹灯、未来主义和高科技、特写镜头、速度与运动)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/924e10bcf62040f883cb067adee96ea1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=396705&amp;e=png&amp;b=432343\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: dynamic and action-packed fitness tracker in motion blur, gym environment, active lifestyle, health and wellness, close-up shot, speed and motion (动态模糊中的动态健身追踪器、健身房环境、积极的生活方式、健康与保健、特写镜头、速度与运动)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4e85717313a648be838ba024c958f60d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=360310&amp;e=png&amp;b=34161d\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: speed lines of a mechanical gaming chair in motion blur, gaming setup background, ergonomic design, immersive gaming experience, close-up shot (运动模糊中机械游戏椅的速度线，游戏设置背景，人体工学设计，身临其境的游戏体验，特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2fed5fcbf4b94e2b9012bb9891fe5ea9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=376301&amp;e=png&amp;b=19222f\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>剪影</h3>\\n<p>这种风格利用产品背后的背光来突出产品的外观或形状而非其细节特征。可以营造神秘感和戏剧效果。</p>\\n<blockquote>\\n<p>Prompt: close up shot of Scooter&#39;s sleek silhouette gracefully glides over the ancient, weathered cobblestone street, kissed by the soft, warm embrace of a nostalgic, golden sunset, nikon d850 --ar 16:9 (近距离拍摄 Scooter 光滑的身影在古老、饱经风霜的鹅卵石街道上优雅地滑行，被怀旧的金色夕阳温柔、温暖的拥抱所亲吻，尼康 D850)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46d118706a49435e8e26b15b1c882560~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=620636&amp;e=png&amp;b=381e0f\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: close up shot, silhouette of a highly-reviewed gaming console with dynamic LED accents, set against a futuristic backdrop --ar 16:9 (特写镜头，一款备受好评的游戏机的剪影，动态 LED 点缀，以未来主义为背景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7f29ac4fbf754b1ba4f551e3af1aa51b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=409814&amp;e=png&amp;b=321834\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a sleek sports car silhouette against a fiery sunset, highlighting its aerodynamic curves, passion and power, low-angle shot, dramatic lighting, warm color palette, nikon d 850 --ar 16:9 (在火红的夕阳下，一辆流线型跑车的轮廓凸显其空气动力学曲线、激情和力量，低角度拍摄，戏剧性照明，暖色调，尼康 D 850)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/30477400faf543a5abc918aaf646d317~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=480502&amp;e=png&amp;b=e8743a\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>微缩</h3>\\n<p>这种风格可以在场景中放置一个微小的人物模型，让场景看上去像是一个微缩世界，从而突出产品的有趣。</p>\\n<blockquote>\\n<p>Prompt: macro product photography of micro tiny sized chef climbing a giant vibrant red chili pepper (微距产品摄影，微小的厨师攀爬上一个巨大的充满活力的红辣椒)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1e9760e696b54e05bb1b341a79b62b63~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=368496&amp;e=png&amp;b=6e757e\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: macro product photography of micro tiny sized fashionista leaning on a giant designer handbag (微距产品摄影，微小身材的时尚达人倚靠在巨大的名牌手提包上)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/23fd8259b8924037970efad519aaa9a7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=360142&amp;e=png&amp;b=ea8644\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: macro product photography of micro tiny sized kid climbing on a pink doughnut (微距产品摄影，微小的孩子爬上粉色甜甜圈)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c84ed84ad3c4bc0b5038065eeec076a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=466682&amp;e=png&amp;b=f7cbe0\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>泼溅 (Splashing)</h3>\\n<p>这种产品的动态侧重于捕捉液体（通常是水）与拍摄对象相互作用的瞬间，可以用来强调产品的新鲜、纯净或防水特性。</p>\\n<blockquote>\\n<p>Prompt: a sleek silver sports car tire immersed in a puddle of water, with dynamic water droplets frozen in mid-air (光滑的银色跑车轮胎浸泡在水坑中，动态水珠凝固在半空中)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46c3bd75f60b480791395f0a8fdb2a44~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=525501&amp;e=png&amp;b=6a6f79\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: close up shot a bottle of premium olive oil, with a golden splash revealing crushed olives (特写镜头：一瓶优质橄榄油，金色的油花透露出橄榄果的碎屑)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b60300bc3ac24f5fa57638dba04dfedb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=471836&amp;e=png&amp;b=0d0a0d\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: an icy cold cola can being opened, with a refreshing explosion of cola splashing out (冰凉的可乐罐被打开，可乐喷涌而出，沁人心脾)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/501a8aa52b91464c948b9d9f2d49c358~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=468836&amp;e=png&amp;b=1d1c21\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: running shoes making contact with a puddle, water splashing on impact (跑鞋与水坑接触，撞击时水花四溅)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/84d8d0ff7afa4a01a38fb3d7f033d8c6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=520460&amp;e=png&amp;b=4b718a\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>悬浮</h3>\\n<p>在提示词中添加悬浮可以让产品处于半空中，给人以漂浮或悬浮的错觉。这种技术可以在产品周围营造出一种失重感、运动感或魔幻感。</p>\\n<blockquote>\\n<p>Prompt: close-up shot of a pair of sleek athletic sneakers suspending in mid-air, gently hovering above a billowing, vibrant cloud of multicolored powder, dreamlike, ethereal lighting (特写镜头，一双光滑的运动鞋悬浮在半空中，轻轻地盘旋在一朵波光粼粼、充满活力的五彩粉云之上，灯光如梦似幻，虚无缥缈)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/21bd7f426ebd4dc2b869063399f93381~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=389972&amp;e=png&amp;b=1a1e32\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a pair of premium noise-canceling headphones hovering gracefully amidst sound waves, immersive and serene, soft and ambient lighting, close-up shot (一副高级降噪耳机在声波中优雅地盘旋，身临其境，宁静安详，柔和的环境光，特写镜头)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/08b9c78b49594fbaa8faed3ab58dbe78~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=419581&amp;e=png&amp;b=40253f\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: close up shot of video game console suspended in mid-air, surrounded by digital graphics, gaming paradise, high-speed sync (视频游戏机悬浮在半空中的特写镜头，周围是数字图形，游戏天堂，高速同步)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3c38b536e2c046d3ba7ff299c4152045~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=442403&amp;e=png&amp;b=161f2f\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>烟雾</h3>\\n<p>在提示词中加入烟雾，可以增加戏剧性、神秘感，或象征产品的某些属性，如热度、新鲜度或强度。</p>\\n<blockquote>\\n<p>Prompt: luminous smoke enveloping a premium transparent tequila bottle (烟雾缭绕的优质透明龙舌兰酒瓶)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7e9f1793ba544ec4b17a790f6e072db5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=435016&amp;e=png&amp;b=021523\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: spirals of smoke dance in the air, sleek athletic shoe, vibrant and surreal, neon lighting (袅袅烟雾在空中飞舞，光滑的运动鞋，充满活力和超现实感，霓虹灯照明)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff8934eb7a9147cca822c66af9433970~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=362351&amp;e=png&amp;b=070408\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: rolling waves of mint green smoke highlighting a luxury skincare product (滚滚薄荷绿烟雾彰显奢华护肤品的魅力)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0ca13a3d29c14df2a8a769fecbf8f621~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=365839&amp;e=png&amp;b=8ecdb2\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>构图</h2>\\n<p>产品图片不仅仅是捕捉产品的外形和质感，更是为产品赋予情感和故事。通过不同的构图手法，我们可以呈现出产品不同的特性和魅力。下面为大家介绍如何在Midjourney 实现五种独特且有趣的构图方法，希望可以给你带来一些灵感。</p>\\n<h3>倒影</h3>\\n<p>倒影可以用来强调产品周围的环境，增加深度，让画面生动有趣。</p>\\n<blockquote>\\n<p>Prompt: a sleek silver wristwatch with luminous markings, casting a pristine reflection beneath (光滑的银色腕表上有夜光刻度，在下方投射出纯净的反光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6d54a05f2bc24913b859057a32cea4dd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=413345&amp;e=png&amp;b=1f252e\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a sleek black smartphone with a curved glass screen, its reflection mirroring the cityscape in the background (时尚的黑色智能手机，弧形玻璃屏幕，其倒影与背景中的城市景观相映成趣)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c3c9050f96d04587b98d7514924ce1bd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=334130&amp;e=png&amp;b=b5c4e5\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a sleek silver laptop with a backlit keyboard, its reflection capturing the modernity of a minimalist office (时尚的银色笔记本电脑，配有背光键盘，其反光效果体现了简约办公室的现代感)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/68cfde5b11a64b64bf456d501af5a2b5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=375701&amp;e=png&amp;b=352f39\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a pair of stylish sunglasses with mirrored lenses, capturing the vibrant colors of a tropical beach scene (一副带有镜面镜片的时尚太阳镜，捕捉热带海滩场景的鲜艳色彩)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/82c54822b9554866aecdd9c01f0516ba~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=394190&amp;e=png&amp;b=ddc5b1\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a row of artisanal perfumes in elegant glass bottles, their reflections showcasing the intricate labels (一排排手工制作的香水，装在优雅的玻璃瓶中，瓶身的倒影展示着精致的标签)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a52e510ce8342739769e267d7958945~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=458968&amp;e=png&amp;b=e0d3d0\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a sleek DSLR camera with a wide-angle lens, its reflection showcasing a breathtaking mountain landscape (一台配有广角镜头的时尚数码单反相机，其倒影展示了壮丽的山景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7721d6d26d3b41d28568af38b4aec537~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=492452&amp;e=png&amp;b=c3ced8\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>水面之下</h3>\\n<p>现实世界中要获得这种构图首先要装备一个防水相机。但是在 Midjourney 中只需要通过提示词来实现。</p>\\n<p>这种构图可以展示产品的防水功能，或营造产品的宁静、神秘或沉浸感。你可以用这个模板来写提示词：[product] plunging into [water]。当然，这个模板不是对所有的产品都有效。使用之前，请思考下什么产品放到水下才是合理的。</p>\\n<blockquote>\\n<p>Prompt: a bottle of beer plunging into clear blue water, intricate label ( 一瓶啤酒坠入清澈的碧水中，错综复杂的标签)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eb4c1c27ac764e3e9371077a2ddf5467~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=507068&amp;e=png&amp;b=06324b\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: waterproof sports camera plunging into the deep blue sea (跃入深蓝大海的防水运动相机)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/73bf543237a74cfda2a9d14fede869f0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=494917&amp;e=png&amp;b=0d5083\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a bottle of luxury perfume plunging into deep blue sea, intricate label (一瓶奢华香水坠入深蓝色的大海，复杂的标签)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6f5a953dccaa45d99b70d5d1af5c3450~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=515912&amp;e=png&amp;b=041029\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: Garmin sports watch plunging into crystalline water (Garmin 运动手表坠入清澈的水中)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5fe9d19374dc470e98da6f78215b7b90~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=484318&amp;e=png&amp;b=0a5c75\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>屏幕上</h3>\\n<p>把产品展现到屏幕上可以为产品创造一个虚拟空间，增加趣味。</p>\\n<p>你可以通过这个提示词模板来实现：front view of laptop screen displaying a high-resolution product image of [product], minimalist。</p>\\n<blockquote>\\n<p>Prompt: front view of laptop screen displaying a high-resolution product image of a luxurious perfume, minimalist --ar 16:9 (笔记本电脑屏幕正视图，显示高分辨率的奢华香水产品图像，简约风格)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f643b53e15e24193ba009963e2bcd83a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=255602&amp;e=png&amp;b=16120f\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: front view of laptop screen displaying a high-resolution product image of exquisite porcelain dinnerware --ar 16:9 (笔记本电脑屏幕正视图，显示精致瓷器餐具的高分辨率产品图像)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9c6530a8aa014cc4b1f6512917a8a7ee~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=370754&amp;e=png&amp;b=17110d\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: front view of laptop screen:: a high-resolution product image of limited edition guitar:: --ar 16:9 (笔记本电脑屏幕正视图，限量版吉他的高分辨率产品图像)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2deb90ca702c41f2b3203d2b1a1b7f11~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=434528&amp;e=png&amp;b=000203\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>几何构图</h3>\\n<p>把产品按照几何形状进行排列，可以美观地陈列产品，给观众带来秩序感和安全感。</p>\\n<blockquote>\\n<p>Prompt: bird&#39;s eye view of high-end watches neatly arranged on a sleek black glass background with a grid-like geometric pattern, soft diffused lighting, minimalist and modern, symmetrical composition (鸟瞰整齐排列在光滑黑色玻璃背景上的高档手表，网格状几何图案，柔和的散射光线，简约现代的对称构图)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/84a67a93d91a4bf5a9f8da50cf243241~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=597301&amp;e=png&amp;b=182932\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: bird&#39;s eye view of colorful macarons neatly arranged on a rustic wooden table with a circular layout, warm golden hour lighting, flat lay perspective, pastel color palette (鸟瞰五颜六色的马卡龙整齐地摆放在古朴的木桌上，采用圆形布局，暖金色灯光，平铺视角，粉色调)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3219001716cb4ddcad1cfe576796d4a1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=564243&amp;e=png&amp;b=36221c\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: bird&#39;s eye view of handcrafted leather wallets neatly arranged on a vintage map with a linear formation (手工制作的皮夹鸟瞰图，整齐地排列在复古地图上，呈线形排列)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ea0b7819c5c1446dac31469f8da8f2f9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=496312&amp;e=png&amp;b=d0c8b2\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: bird&#39;s eye view of sushi rolls neatly arranged on a black slate platter with a concentric circle layout (同心圆布局的黑色石板盘上整齐摆放的寿司卷鸟瞰图)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/db82e235c7774903895e3d02182f16a2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=582521&amp;e=png&amp;b=161619\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>Knolling</h3>\\n<p>Knolling 以美观和有条理的方式在一个平面上展现产品。在 Midjourney 中它是一个神奇的关键词，用在单个物体上有时可以把它的各个组成部分展现出来。也可以平铺一系列产品或组件。</p>\\n<blockquote>\\n<p>Prompt: knolling sewing machine, soft daylight (针织机，软日光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40980492c7f949948dded79f04725098~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=467486&amp;e=png&amp;b=e3eded\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: knolling stationery collection on a rustic wooden desk --ar 16:9 (针织机，软日光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5910bc2fea4d4a359fdababbb5887d29~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=554202&amp;e=png&amp;b=7f5842\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: knolling pet grooming essentials with a dog in the center, diffused window light --ar 16:9 (以狗为中心的宠物美容用品，窗外散射光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e0acb5fda58746d7bb139cadc252ccba~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=789608&amp;e=png&amp;b=7f593b\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>背景</h2>\\n<p>背景不仅仅是产品的衬托，它还可以增强产品的特点，引导观众的视线，甚至是传达品牌信息的媒介。下面为大家介绍一些在 Midjourney 中可以实现的背景。每种背景都有其独特的魅力和功能，为产品注入不同的情感和氛围。</p>\\n<h3>展示台</h3>\\n<p>把产品放在展示台上可以突出其重要性，使之成为视觉的焦点。下面的提示词只是用来生成展示台的图片。你随后可以把产品合成到这些背景图上。我还通过 Midjourney 的局部重绘功能在图片上添加了产品展现效果。</p>\\n<blockquote>\\n<p>Prompt: product podium with a wooden-textured platform, minimalist --ar 16:9 (产品平台，木质纹理平台，简约风格)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2bfe5a232cda456187e80e408e686571~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=442093&amp;e=png&amp;b=e6d7c4\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1bef42ee427a45b28fb2a572053d976c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=530862&amp;e=png&amp;b=e0d1be\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: warm yellow backdrop with an product podium --ar 16:9 (暖黄色背景墙与产品讲台)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3508ec4b9eb4494eaba05175603a0c8a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=350057&amp;e=png&amp;b=efb01c\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/27bcc78c75304f9188809a76a88756d4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=354730&amp;e=png&amp;b=f8bf22\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: solitary thin stone product podium against a seamless blue background --ar 16:9 (无缝蓝色背景下的孤形薄石产品平台)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1f7bd9c5ea364474b73166708efd70e5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=452088&amp;e=png&amp;b=346789\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b6d85c820f0243f19212cbbf2a497716~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=497673&amp;e=png&amp;b=74abc6\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: sleek wooden surface ready to hold a product, set against the soft blur of a modern workspace bustling in the background --ar 16:9 (光滑的木质表面，可随时放置产品，背景是现代工作空间熙熙攘攘的柔和模糊景象)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1714a496f63741088799e9130efd5546~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=368618&amp;e=png&amp;b=e4cdb6\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e801dad791c249e4863c9c7c379dfe11~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=411334&amp;e=png&amp;b=e1ceba\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: sleek metallic podium with a neon halo, set against a gradient of deep blue to purple --ar 16:9 (光滑的金属讲台，霓虹灯光晕，与深蓝到紫色的渐变色相映成趣)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7a62014e2f7f4d888ca9e02446ca21ee~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=289111&amp;e=png&amp;b=16093e\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/75802a0e65cb4fc6a9efbeb6872f66d1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=320820&amp;e=png&amp;b=120a3d\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: an illuminated round podium against a soft pink gradient --ar 16:9 (在柔和的粉色渐变背景下，一个圆形照明讲台)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ce672470481e45c5a5c1ccfd964cb8a5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=299753&amp;e=png&amp;b=ef94af\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b778d974269d44c3ac9c84bb5ef6f460~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=433881&amp;e=png&amp;b=e27da2\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a minimalist pastel yellow setup with muted gray archways, framing a white circular podium --ar 16:9 (简约的粉黄色布景，灰色拱门，衬托着白色的圆形平台)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4a5622aae41c49ac83464a26579b4279~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=463999&amp;e=png&amp;b=d6c1a2\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c018a2b65c794868acf1fc7a732a1075~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=457207&amp;e=png&amp;b=d5c2a6\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>镜头炫光</h3>\\n<p>镜头眩光是指非成像光线进入镜头，然后照射到相机的胶片或数码传感器上。这会产生明显的条纹光或一系列圆圈。在提示词中添加镜头炫光可以增加画面的戏剧感、温暖感或复古感。你可以用这个模板：[product], [color] background with lens flare。</p>\\n<blockquote>\\n<p>Prompt: shimmering diamond ring on a velvet cushion, royal blue background with lens flare (闪闪发光的钻戒，镶嵌在天鹅绒垫子上，皇家蓝背景，镜头炫光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be63d0ce387a4bc4b56a8af47b7d9e0b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=415827&amp;e=png&amp;b=050d29\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: glistening bottle of luxury perfume with golden accents, black background with lens flare (闪闪发光的豪华香水瓶，金色点缀，黑色背景，镜头炫光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/45541914b6f24c8083eea618cefd941b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=467880&amp;e=png&amp;b=01060b\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a set of gourmet chocolates arranged in a decorative box, warm brown background with lens flare --ar 16:9 (一套美食巧克力摆放在装饰盒中，暖棕色背景，带镜头光晕)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/534fb42a06df4dbf9ff6ac46870646bc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=449647&amp;e=png&amp;b=33120a\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a sleek and modern desktop computer, home office background with lens flare --ar 16:9 (时尚现代的台式电脑，带有镜头光晕的家庭办公背景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c5a74bf13ee648e8a93d5d5b3eee5d65~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=390181&amp;e=png&amp;b=ebdcc9\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: sparkling silver necklace with a heart-shaped pendant, celestial background with lens flare (闪闪发光的银项链，心形吊坠，带有镜头光晕的天体背景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2d9c0760db96447c9a9b4d8dd052e136~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=337785&amp;e=png&amp;b=271924\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>渐变粉彩</h3>\\n<p>这种风格的背景可以唤起平静、柔美或现代的感觉，为产品提供令人愉悦的现代背景。你可以用这个模板来实现词效果：[product] against a gradient pastels background。</p>\\n<blockquote>\\n<p>Prompt: a pair of stylish sunglasses, transparent lenses, contemporary frame, against a gradient pastels background --ar 16:9 (一副时尚太阳镜，透明镜片，现代镜框，渐变粉色背景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dbf6edcef0a94d8f9169d28dcb509aa6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=338495&amp;e=png&amp;b=fdc49e\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: shiny new silver smartphone, reflective surface, sleek design, against a gradient pastels background (银光闪闪的新款智能手机，反光表面，流线型设计，背景为渐变的柔和色彩)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7b876af6c6fd4cc390b80084a3ab48db~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=291282&amp;e=png&amp;b=ecbdea\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>大理石纹理</h3>\\n<p>大理石纹理的背景可以赋予产品优雅和精致的感觉</p>\\n<blockquote>\\n<p>Prompt: luxury wristwatch, intricate dial wiht sunburst motif, leather strap, on marble with texture (奢华腕表，带有太阳纹饰的复杂表盘，皮革表带，大理石纹理表盘)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b1a3c9eb00f343679628f66910c8a3f3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=520624&amp;e=png&amp;b=86796e\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: crystal-clear whiskey decanter, cut glass, aged whiskey, against background with marble texture, soft lighting (晶莹剔透的威士忌醒酒器，切割玻璃，陈年威士忌，背景为大理石纹理，灯光柔和)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3469f7129ebb450ba50b0b6e3fc701cf~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=522095&amp;e=png&amp;b=0c1117\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>飘渺虚化 (Ethereal Bokeh)</h3>\\n<p>这种风格的背景可以增添一种神奇的、童话般的氛围，让产品显得更加迷人。</p>\\n<blockquote>\\n<p>Prompt: hand-blown glass vase, soft diffused daylight, translucent pastel shades, ethereal bokeh background (手吹玻璃花瓶、柔和的散射日光、半透明的粉彩色调、空灵的虚化背景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/66ee524ad1e4401caa4689902e3169ce~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=391887&amp;e=png&amp;b=e3cec0\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: delicate porcelain teacup, garden setting, soft pastel colors,dreamlike ethereal bokeh background (精致的瓷茶杯，花园环境，柔和的粉彩，梦幻般空灵的虚化背景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7d84f4709acc4ebf80180216e068fc66~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=424401&amp;e=png&amp;b=ceb5c6\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>激光秀</h3>\\n<p>激光灯光秀可以突出产品的未来感或高科技特质，也可以简单地营造一种充满活力的派对氛围。</p>\\n<blockquote>\\n<p>Prompt: a sports car&#39;s sleek chrome grille gleaming amidst the laser light show --ar 16:9 (跑车光滑的镀铬格栅在激光灯光秀中熠熠生辉)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/04544401806840c5874175a1764c32f5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=439236&amp;e=png&amp;b=1f0511\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a high-end bicycle frame bathed in dynamic laser lights --ar 16:9 (沐浴在动态镭射灯下的高端自行车车架)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1e814638cb5f4058bbfa8c30dff59700~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=396478&amp;e=png&amp;b=000710\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a sleek VR headset shining amidst the chaotic colors of a laser light show (光滑的 VR 头显在激光灯光秀的混沌色彩中闪闪发光)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/362da67a966d483d811236e6cf405b7e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=447467&amp;e=png&amp;b=180c18\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>无缝霓虹灯网格</h3>\\n<p>在 Midjourney的提示词中添加无缝霓虹灯网格，可以同时添加霓虹灯的灯光效果和网格状的背景。</p>\\n<blockquote>\\n<p>Prompt: a shiny, chrome-plated retro microphone, placed elegantly on a seamless neon grid background, close-up (一个闪亮的镀铬复古麦克风，优雅地放置在无缝霓虹网格背景上，特写)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/355bfcd8fe3744baaa3ff4d2724d6019~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=328051&amp;e=png&amp;b=7a097e\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: a series of premium home audio speakers, each with its unique design, against a seamless neon grid background --ar 16:9</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fa087cb6f3e54ff39903280209abbce2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1456&amp;h=816&amp;s=404583&amp;e=png&amp;b=260819\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>梦幻云景</h3>\\n<p>梦幻云景的背景强调的是蓬松、梦幻般的云朵，充满想象力和幻想，给产品带来轻松、梦幻或神奇的氛围。</p>\\n<blockquote>\\n<p>Prompt: a pair of elegant high heels, elegantly placed against a whimsical cloudscape, soft, romantic lighting (一双优雅的高跟鞋，优雅地摆放在奇异的云景中，灯光柔和浪漫)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7f65e00595d54385ba96979bf7c0aeb7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=428133&amp;e=png&amp;b=748394\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>Prompt: vibrant red lipstick tube, whimsical cloudscape background (鲜艳的红色唇膏管，奇异的云景背景)</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/873647afabac40799d4a637ea1338388~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1024&amp;h=1024&amp;s=343965&amp;e=png&amp;b=bfd1da\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>总结</h2>\\n<p>在这个充满无限可能性的创意世界中，Midjourney 不仅是一个工具，更是一个桥梁，连接文本与图像，连接想象与现实。你是否准备好让你的产品在数百万种产品中脱颖而出？收藏本文并付诸实践，让你的产品与众不同，震撼市场！</p>\\n<blockquote>\\n<p>分享完毕，感谢阅读🤗 欢迎点赞，收藏，评论</p>\\n<p>更多免费原创教程，🚀关注公众号：我的AI力量</p>\\n</blockquote>', 'author': '我的AI力量', 'reading_time': 31, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 30, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'ce7c7cd93a2c2f836dc82be974bb25866d293240b8182d147a31d2efe22f8136', 'title': 'ChatGPT多模态升级，支持图片和语音，体验如何？', 'url': 'https://juejin.cn/post/7284608063913345043', 'published_at': '2023-10-02T14:49:34Z', 'created_at': '2023-10-04T14:11:40.335553Z', 'changed_at': '2023-10-04T14:11:40.335553Z', 'content': '<h1>一、前言</h1>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/121eff60ba0948c3b667d6a664af2115~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1586&amp;h=688&amp;s=173624&amp;e=png&amp;b=370637\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>9 月 25 日，ChatGPT 多模态增加了新的<strong>语音功能</strong>和<strong>图像功能</strong>。这些功能提供了一种新的、更直观的界面，允许我们与 ChatGPT 进行语音对话或展示我们正在谈论的内容。</p>\\n</blockquote>\\n<p>ChatGPT 现在可以看、听、和说话了，而不单单是一个文本驱动的工具了。</p>\\n<p>正值十一小长假，很多人选择出去旅行。ChatGPT 新推出的这项功能能否用在旅行中呢？一起来看看。</p>\\n<h1>二、图像交互功能</h1>\\n<p>ChatGPT 在 9 月 25 日新推出的这个版本中，添加了图片交互能力。</p>\\n<p>图像理解由多模态 GPT-3.5 和 GPT-4 提供支持。这些模型将其语言推理技能应用于各种图像，例如照片、屏幕截图以及包含文本和图像的文档。</p>\\n<h2>如何使用呢？</h2>\\n<p>以网页版的 ChatGPT 为例。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/18fa5d45f40444068f0e6ebdeef0f7de~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2362&amp;h=1632&amp;s=252227&amp;e=png&amp;b=353641\" alt=\"\" loading=\"lazy\"/></p>\\n<p>点输入框左侧的「图片」按钮，就可以上传图片。可以上传一张或多张图片。</p>\\n<p>上传后，针对图片中的内容，输入我们想问的问题即可。例如就像下面这样。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1b5fc7353f7f47499f9cc4c23cc8354b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1608&amp;h=298&amp;s=82463&amp;e=png&amp;b=3d3e4a\" alt=\"\" loading=\"lazy\"/></p>\\n<p>看看 ChatGPT 的回答。回答得还不错，看来 ChatGPT 对图像内容的理解还是可以的。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00335424ae834ac3b0bf1a0c1380e2ab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1454&amp;h=836&amp;s=1079916&amp;e=png&amp;b=383a45\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>在生活中有哪些应用呢？</h2>\\n<p>比如刚刚上面举的这个例子：<strong>景点识别</strong>。</p>\\n<p>现在正值十一小长假，很多人外出旅行。</p>\\n<p>当我们看到一个不知名但很有趣的景点或地标时，可以拍摄照片并展示给 ChatGPT，它可以帮助我们识别并提供相关信息。</p>\\n<p>如果我们在旅行中遇到技术问题，如相机设置等，也可以向 ChatGPT 展示问题所在，获取技术支持和建议。</p>\\n<p>再来看几个官方的示例。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9b22db99dca54c97a1a3bff4627f52a3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=770&amp;h=1482&amp;s=607193&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>这个例子中上传了两张图，一张是使用说明，另一个是工具箱。然后向 ChatGPT 提问「工具箱中是否有正确的工具」。</p>\\n<p>另外，如果要聚焦图像的特定部分，还可以使用手机版 ChatGPT 中的绘图工具，圈出某个位置。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b722fe2e75b04209a5057783f8ea1a51~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=770&amp;h=1578&amp;s=1374238&amp;e=png&amp;b=020002\" alt=\"\" loading=\"lazy\"/></p>\\n<p>这个例子是询问「如何降低自行车座」。同时使用了 ChatGPT 中的绘图工具，圈出某个位置，然后再进行提问。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/532bf04995f14139bf8d370a745a29e4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=776&amp;h=1582&amp;s=389193&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2f894522e3f446ca9eae63bf1cf3cff1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=776&amp;h=1432&amp;s=519374&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>再比如，在家中拍摄冰箱和食品储藏室的照片，来确定晚餐菜单，并询问详细的食谱。</p>\\n<p>也可以拍摄数学问题的照片、或者工作中数据的复杂图表，圈出问题，并与 ChatGPT 共同探讨解决方法。</p>\\n<p>此时脑海中飘过电视上之前经常播的一个广告：哪里不会点哪里~</p>\\n<h1>三、语音交互功能</h1>\\n<p>ChatGPT 的多模态，除了新增了「图像交互功能」外，还增加了「语音交互功能」。</p>\\n<p>用户可以使用语音与 ChatGPT 进行双向对话。</p>\\n<p>其实以前也是支持语音的，只不过是仅限于「听」</p>\\n<p>新的语音功能由新的「text-to-speech」模型驱动，能够从简短的样本语音中生成类似人类的音频。</p>\\n<p>但语音交互功能目前只在手机端可以使用。</p>\\n<h2>如何使用呢？</h2>\\n<p>在手机端应用中点「设置」→「新功能」，选择加入语音对话。</p>\\n<p>点击位于主屏幕右上角的耳机按钮，然后从 5 种不同的声音中选择喜欢的声音，就可以开始进行对话了。</p>\\n<h2>语音功能可以用来做什么呢？</h2>\\n<p>比如练习外语口语，让它充当你的外教老师。</p>\\n<p>再比旅途中与它交谈，让它充当我们的随身导游。</p>\\n<p>或者也可以用它来为家里小孩讲睡前故事，等等。</p>\\n<h1>四、模型的局限性和风险</h1>\\n<ul>\\n<li>\\n<p>用户可能依赖 ChatGPT 处理专业话题，例如研究领域。不鼓励在没有适当验证的情况下使用高风险用例。</p>\\n</li>\\n<li>\\n<p>模型擅长转录英文文本，但在处理某些其他语言时表现不佳。</p>\\n</li>\\n</ul>\\n<h1>五、目前可用范围</h1>\\n<ul>\\n<li>\\n<p>目前 ChatGPT 多模态新增的图像和语音功能，只在 <strong>Plus 用户</strong>和<strong>企业用户</strong>下可以使用。开发者和其他用户群体将在此后不久获得这些功能。</p>\\n</li>\\n<li>\\n<p>另外，语音功能只在 iOS 和 Android 上可以使用，而图像功能在网页和手机上均可使用。</p>\\n</li>\\n</ul>\\n<h1>六、结束语</h1>\\n<p>随着技术的不断进步，我们正见证着人工智能领域的一次次重大突破。ChatGPT 的多模态功能，不仅为我们提供了与机器交流的新方式，还为我们打开了无尽的可能性。</p>\\n<p>通过语音、图像和文字的结合，我们可以更加直观、高效地与 AI 助手互动，从而更好地满足我们的日常需求。</p>\\n<p>正如 OpenAI 所展示的，未来的 AI 将不仅仅是文字和代码，而是一个能「看见」、「听到」并「说话」的实体。</p>\\n<p>关于 ChatGPT 多模态的图片功能和语音功能，感兴趣的可以移步官方进一步了解：</p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fopenai.com%2Fblog%2Fchatgpt-can-now-see-hear-and-speak\" title=\"https://openai.com/blog/chatgpt-can-now-see-hear-and-speak\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">openai.com/blog/chatgp…</a></p>', 'author': '程序员X小鹿', 'reading_time': 4, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 29, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'efa1dfc2b58671275e1f3a2c12cbf7926dba692e86a53eaeb1dc1a87eedeeacc', 'title': '进化算法中的遗传规划算法（Genetic Programming）', 'url': 'https://juejin.cn/post/7284433532075294775', 'published_at': '2023-10-02T13:54:57Z', 'created_at': '2023-10-04T14:11:40.332599Z', 'changed_at': '2023-10-04T14:11:40.332599Z', 'content': '<p> </p>  <p>  </p> <h1>\\n            进化算法中的遗传规划算法（Genetic Programming）\\n             </h1> <a href=\"https://juejin.cn/user/3081288211767927/posts\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">\\n    皮牙子抓饭\\n      </a> <time datetime=\"2023-10-02T13:54:57.000Z\">\\n                    2023-10-02\\n                  </time>  \\n                    2\\n                   \\n                    阅读1分钟\\n                    <p></p>     <h2>进化算法中的遗传规划算法（Genetic Programming）</h2>\\n<h3>引言</h3>\\n<p>进化算法是一类基于生物进化理论的优化算法，通过模拟生物进化的过程，通过选择、交叉和变异等操作，不断优化解决问题。遗传规划算法（Genetic Programming，简称GP）作为进化算法的一种，通过演化生成程序或模型来解决问题。本文将重点介绍遗传规划算法在进化算法中的应用。</p>\\n<h3>遗传规划算法的基本原理</h3>\\n<p>遗传规划算法是通过对程序或模型的组合、变异和选择来进行优化的。它将程序或模型表示为一棵树结构，每个节点代表一个函数或操作符，每个叶子节点代表一个变量或常数。遗传规划算法通过遗传操作对这些树进行操作，不断生成新的解，并通过适应度评估来选择优秀的解。 遗传规划算法的基本步骤如下：</p>\\n<ol>\\n<li>初始化种群：随机生成一组初始树结构，作为种群的初始解。</li>\\n<li>评估适应度：对每个个体（树结构）进行适应度评估，评估其解决问题的能力。</li>\\n<li>选择操作：根据适应度值选择一些个体作为父代，用于后续的交叉和变异操作。</li>\\n<li>交叉操作：选择的父代个体进行交叉操作，生成新的子代个体。</li>\\n<li>变异操作：对子代个体进行变异操作，引入新的变量或操作符，增加解空间的多样性。</li>\\n<li>更新种群：将父代和子代个体结合，形成新的种群。</li>\\n<li>终止条件判断：根据预设的终止条件（例如达到最大迭代次数或找到满意的解等），判断是否终止算法。</li>\\n<li>返回结果：返回最优解或满足条件的解。</li>\\n</ol>\\n<p>以下是一个简单的遗传规划算法的示例代码，用于求解一个简单的数学函数的最大值问题。</p>\\n<pre><code>import random\\n\\nPOPULATION_SIZE = 100\\nGENERATION_COUNT = 50\\nCROSSOVER_RATE = 0.8\\nMUTATION_RATE = 0.1\\n\\nX_MIN = -10\\nX_MAX = 10\\ndef target_function(x):\\n    return x ** 2 - 2 * x + 1\\n\\nclass Individual:\\n    def __init__(self, chromosome):\\n        self.chromosome = chromosome\\n        self.fitness = self.calculate_fitness()\\n    def calculate_fitness(self):\\n        x = self.decode_chromosome()\\n        return target_function(x)\\n    def decode_chromosome(self):\\n        binary_x = &#34;&#34;.join(str(bit) for bit in self.chromosome)\\n        x = int(binary_x, 2) * (X_MAX - X_MIN) / (2 ** len(self.chromosome) - 1) + X_MIN\\n        return x\\n\\ndef initialize_population():\\n    population = []\\n    for _ in range(POPULATION_SIZE):\\n        chromosome = [random.randint(0, 1) for _ in range(10)]  \\n        individual = Individual(chromosome)\\n        population.append(individual)\\n    return population\\n\\ndef selection(population):\\n    \\n    total_fitness = sum(individual.fitness for individual in population)\\n    probabilities = [individual.fitness / total_fitness for individual in population]\\n    selected_individuals = random.choices(population, probabilities, k=POPULATION_SIZE)\\n    return selected_individuals\\n\\ndef crossover(parent1, parent2):\\n    if random.random() &lt; CROSSOVER_RATE:\\n        crossover_point = random.randint(1, len(parent1.chromosome) - 1)\\n        child1_chromosome = parent1.chromosome[:crossover_point] + parent2.chromosome[crossover_point:]\\n        child2_chromosome = parent2.chromosome[:crossover_point] + parent1.chromosome[crossover_point:]\\n        child1 = Individual(child1_chromosome)\\n        child2 = Individual(child2_chromosome)\\n        return child1, child2\\n    else:\\n        return parent1, parent2\\n\\ndef mutation(individual):\\n    mutated_chromosome = individual.chromosome.copy()\\n    for i in range(len(mutated_chromosome)):\\n        if random.random() &lt; MUTATION_RATE:\\n            mutated_chromosome[i] = 1 - mutated_chromosome[i]\\n    return Individual(mutated_chromosome)\\n\\ndef main():\\n    population = initialize_population()\\n    best_fitness = float(&#39;-inf&#39;)\\n    best_individual = None\\n    for generation in range(GENERATION_COUNT):\\n        selected_individuals = selection(population)\\n        new_population = []\\n        while len(new_population) &lt; POPULATION_SIZE:\\n            parent1, parent2 = random.sample(selected_individuals, 2)\\n            child1, child2 = crossover(parent1, parent2)\\n            child1 = mutation(child1)\\n            child2 = mutation(child2)\\n            new_population.extend([child1, child2])\\n        population = new_population\\n        \\n        for individual in population:\\n            if individual.fitness &gt; best_fitness:\\n                best_fitness = individual.fitness\\n                best_individual = individual\\n        print(&#34;Generation:&#34;, generation + 1)\\n        print(&#34;Best Individual:&#34;, best_individual.chromosome)\\n        print(&#34;Best Fitness:&#34;, best_fitness)\\n        print()\\n    \\n    x = best_individual.decode_chromosome()\\n    print(&#34;Optimal Solution:&#34;)\\n    print(&#34;x =&#34;, x)\\n    print(&#34;f(x) =&#34;, target_function(x))\\nif __name__ == &#34;__main__&#34;:\\n    main()\\n</code></pre>\\n<p>请注意，以上代码只是一个简单示例，实际应用中，遗传规划算法的参数和操作可能需要根据具体问题做适当的调整。此外，示例代码中的函数表达式和变量范围也可以根据实际问题进行修改。</p>\\n<h3>遗传规划算法的优势和应用领域</h3>\\n<p>遗传规划算法具有以下优势：</p>\\n<ol>\\n<li>高度自适应：遗传规划算法可以自动调整候选解的结构和参数，适应问题的复杂性和多样性。</li>\\n<li>可解释性强：生成的程序或模型具有良好的可解释性，可以帮助我们理解问题和解决方案之间的关系。</li>\\n<li>广泛适用性：遗传规划算法可以应用于多种问题领域，如机器学习、数据挖掘、控制系统设计等。 遗传规划算法在以下领域得到了广泛的应用：</li>\\n<li>符号回归问题：通过遗传规划算法可以生成数学模型，对给定的输入和输出进行回归分析。</li>\\n<li>优化问题：遗传规划算法可以用于求解复杂的优化问题，如函数优化、组合优化等。</li>\\n<li>控制系统设计：遗传规划算法可以用于设计控制系统的参数或结构，提高系统的性能和稳定性。</li>\\n<li>人工智能：遗传规划算法可以用于生成智能代理、决策树等，解决人工智能领域的问题。</li>\\n</ol>\\n<p>以下是一个简单的遗传规划算法智能代理的示例代码，用于求解一个简单的迷宫问题。</p>\\n<pre><code>pythonCopy codeimport random\\n\\nmaze = [\\n    [1, 1, 1, 1, 1, 1, 1, 1],\\n    [1, 0, 0, 0, 0, 0, 0, 1],\\n    [1, 0, 1, 1, 1, 1, 0, 1],\\n    [1, 0, 1, 0, 0, 0, 0, 1],\\n    [1, 0, 1, 0, 1, 1, 0, 1],\\n    [1, 0, 0, 0, 1, 0, 0, 1],\\n    [1, 1, 1, 1, 1, 1, 1, 1]\\n]\\nstart = (1, 1)\\nend = (5, 6)\\n\\nPOPULATION_SIZE = 100\\nGENERATION_COUNT = 50\\nCROSSOVER_RATE = 0.8\\nMUTATION_RATE = 0.1\\n\\nclass Individual:\\n    def __init__(self, chromosome):\\n        self.chromosome = chromosome\\n        self.fitness = self.calculate_fitness()\\n    def calculate_fitness(self):\\n        x, y = start\\n        for gene in self.chromosome:\\n            if gene == 0:  \\n                x -= 1\\n            elif gene == 1:  \\n                x += 1\\n            elif gene == 2:  \\n                y -= 1\\n            elif gene == 3:  \\n                y += 1\\n            if (x, y) == end:\\n                return 1\\n            if maze[x][y] == 1:\\n                return 0\\n        return 0\\n\\ndef initialize_population():\\n    population = []\\n    for _ in range(POPULATION_SIZE):\\n        chromosome = [random.randint(0, 3) for _ in range(50)]  \\n        individual = Individual(chromosome)\\n        population.append(individual)\\n    return population\\n\\ndef selection(population):\\n    \\n    total_fitness = sum(individual.fitness for individual in population)\\n    probabilities = [individual.fitness / total_fitness for individual in population]\\n    selected_individuals = random.choices(population, probabilities, k=POPULATION_SIZE)\\n    return selected_individuals\\n\\ndef crossover(parent1, parent2):\\n    if random.random() &lt; CROSSOVER_RATE:\\n        crossover_point = random.randint(1, len(parent1.chromosome) - 1)\\n        child1_chromosome = parent1.chromosome[:crossover_point] + parent2.chromosome[crossover_point:]\\n        child2_chromosome = parent2.chromosome[:crossover_point] + parent1.chromosome[crossover_point:]\\n        child1 = Individual(child1_chromosome)\\n        child2 = Individual(child2_chromosome)\\n        return child1, child2\\n    else:\\n        return parent1, parent2\\n\\ndef mutation(individual):\\n    mutated_chromosome = individual.chromosome.copy()\\n    for i in range(len(mutated_chromosome)):\\n        if random.random() &lt; MUTATION_RATE:\\n            mutated_chromosome[i] = random.randint(0, 3)\\n    return Individual(mutated_chromosome)\\n\\ndef main():\\n    population = initialize_population()\\n    best_fitness = 0\\n    best_individual = None\\n    for generation in range(GENERATION_COUNT):\\n        selected_individuals = selection(population)\\n        new_population = []\\n        while len(new_population) &lt; POPULATION_SIZE:\\n            parent1, parent2 = random.sample(selected_individuals, 2)\\n            child1, child2 = crossover(parent1, parent2)\\n            child1 = mutation(child1)\\n            child2 = mutation(child2)\\n            new_population.extend([child1, child2])\\n        population = new_population\\n        \\n        for individual in population:\\n            if individual.fitness &gt; best_fitness:\\n                best_fitness = individual.fitness\\n                best_individual = individual\\n        print(&#34;Generation:&#34;, generation + 1)\\n        print(&#34;Best Individual:&#34;, best_individual.chromosome)\\n        print(&#34;Best Fitness:&#34;, best_fitness)\\n        print()\\n    \\n    print(&#34;Optimal Solution:&#34;)\\n    print(&#34;Chromosome:&#34;, best_individual.chromosome)\\n    print(&#34;Path:&#34;)\\n    x, y = start\\n    for gene in best_individual.chromosome:\\n        if gene == 0:  \\n            x -= 1\\n        elif gene == 1:  \\n            x += 1\\n        elif gene == 2:  \\n            y -= 1\\n        elif gene == 3:  \\n            y += 1\\n        print(&#34;(&#34;, x, &#34;,&#34;, y, &#34;)&#34;)\\n        if (x, y) == end:\\n            break\\nif __name__ == &#34;__main__&#34;:\\n    main()\\n</code></pre>\\n<p>请注意，以上代码只是一个简单示例，实际应用中，遗传规划算法的参数和操作可能需要根据具体问题做适当的调整。此外，示例代码中的迷宫地图和起点、终点也可以根据实际问题进行修改。</p>\\n<h3>总结</h3>\\n<p>遗传规划算法作为进化算法的一种重要分支，通过演化生成程序或模型来解决问题。它具有自适应性强、可解释性强和广泛适用性等优势，在多个领域得到了广泛的应用。通过不断地选择、交叉和变异，遗传规划算法可以逐步优化解决方案，找到最优解或满足条件的解。在实际应用中，我们可以根据具体问题的特点，灵活地运用遗传规划算法，提高问题的求解效率和质量。</p>', 'author': '皮牙子抓饭', 'reading_time': 17, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 28, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'da7a3a7466d270f010b849d408de17ceaf7d3958a63df90b92b2586563e8616d', 'title': '2023（2024届）计算机保研经验分享，圆梦山东大学', 'url': 'https://juejin.cn/post/7284417436752707645', 'published_at': '2023-10-02T12:58:39Z', 'created_at': '2023-10-04T14:11:40.326693Z', 'changed_at': '2023-10-04T14:11:40.326693Z', 'content': '<p>本文首发于CSDN：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fweixin_51390582%2Farticle%2Fdetails%2F133495392\" title=\"https://blog.csdn.net/weixin_51390582/article/details/133495392\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">blog.csdn.net/weixin_5139…</a></p>\\n<blockquote>\\n<p><strong><code>前言：</code></strong> Hello大家好，我是Dream，好久不见啦！在这不见的半年多时间里我一直在全身心的<strong>投入保研之中</strong>，在写下这份面经时，真的是感慨颇多，思绪万千。站在这个时间点上，回首过去的几个月，感觉就是终于从泥潭之中爬了出来，今年计算机保研比我想象的还要难很多，尤其是预推免阶段，但是好在结果是好的，终于松了一口气。在此，记录下这些日子的点滴，分享于后来者，希望你看后可以有些许收获！（<code>ps：</code>这篇面经只针对于像我一样的四非或者双非uu，如果本身背景就很强的话，可以当个故事看，看一看我的逐梦历程）</p>\\n</blockquote>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7033031251ac4fe1b6cef202f40ad8cc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1177&amp;h=801&amp;s=89451&amp;e=png&amp;b=fefefe\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<h1>一、个人介绍</h1>\\n<ul>\\n<li><strong>学校专业</strong>：某四非人工智能（卓越人才实验班）专业（<strong>虽然叫人工智能实验班，但是没有任何帮助，只有副作用，因为专业人少很多学校卡百分比进不去，例如西工大卡2%，讲个笑话我们专业第一只有3.4%，并且计算机无评级，可以说是<code>dbuff</code>拉满了...</strong>）</li>\\n<li><strong>成绩（rk）</strong>：夏令营：6/29（可以猜到我夏令营基本是被<code>拒麻了</code>）；预推免2/29。</li>\\n<li><strong>英语：</strong> CET-4：506；CET-6：506（ps：6级最好考高点，有的学校卡450和500分两个线，<strong>500分以上绝对是加分项</strong>）</li>\\n<li><strong>竞赛：</strong> 1.中国机器人及人工智能大赛全国一等奖（<strong>一作，问的最多的</strong>）；2.第十五届中国大学生计算机设计大赛全国二等奖（一作）；3.华北五省计算机应用大赛全国一等奖（一作）；4.第八届互联网+创新创业大赛省部级银奖（一作）。<code>总结起来：</code>竞赛需要有，但是面试很少会看，只需要列举三四项即可，<strong>有国一的是个加分项！</strong>。</li>\\n<li><strong>科研：</strong> 作为负责人，主持一项国家级大创：AICV方向。（事实证明，<strong>国创真的很好使，基本每次面试都会重点提问</strong>！）</li>\\n<li><strong>其余：</strong> 爱好兴趣之类的...（这个可以多写点，例如我就写了我喜欢<code>足球</code>，正好面试的985大学老师正好也喜欢足球，我俩就足球聊了十分钟，最后拿到offer...）,其余我写了在<code>CSDN博主</code>的经历和成就（也是很大的加分点）；如果是<code>党员</code>的可以强调一下（因为每个学校的招生条件第一条都是热爱祖国，如果你是党员，那你一定热爱祖国dddd）。</li>\\n<li><strong>目标：</strong> 山东大学（最好青岛，济南也可以接受）、厦门大学、华中科技大学（后两所学校是因为上一届实验室学长学姐去了，想去<code>投奔</code>他们；想去山大是因为家在山东，以后对自己发展更有利。）</li>\\n<li><strong>最终去向：山东大学信息学院人工智能专业（青岛，梦校）</strong></li>\\n<li>一句话介绍一下自己：<strong>四非低rank半科研半竞赛</strong></li>\\n</ul>\\n<h1>二、个人经验</h1>\\n<h2>1.具体流程</h2>\\n<p><strong>夏令营：</strong> 保研途径<code>1</code>，5月-8月底（有的学校只开夏令营，夏令营学硕发的多，想要学硕一定多参加夏令营），竞争非常激烈</p>\\n<p><strong>预推免：</strong> 保研途径<code>2</code>，8月底-9月底，难度比夏令营低一些</p>\\n<p><strong>九推（十推）：</strong> 保研途径<code>3</code>，929开系统之后参加面试，相当于补录，机会较少，但是抓住机会了，上岸的院校实力超乎你的想象（历年都有上岸华五的）</p>\\n<p><strong>925：</strong> 阳历9月28日，系统录取，保研学生上岸日，但是今年是929。</p>\\n<h2>2.经验解答</h2>\\n<p><strong>rk/rank：</strong> 专业排名，一般指学习成绩排名，夏令营指前五学期排名，预推免指前六学期排名</p>\\n<p><strong>title：</strong> 学校的title高不高，指学校的名气大不大，是不是985、211还是双一流呀，都不是的话那我们是好兄弟，我们有一个共同的名字：<strong>四非！</strong></p>\\n<p><strong>强com：</strong> 学院的招生办和行政单位在招生中起决定性作用，导师不能决定能否录取你，如北航，人大,山大等；</p>\\n<p><strong>弱com：</strong> 与上述概念相反，导师在招生中起决定性作用，导师愿意要你的话，就基本稳了,如厦大、西工大，西电等；</p>\\n<p><strong>oq：</strong> over qualified，你太强了，超过了投递院校的招生资格，不让你入营。如本科华五报名末流985，本科985、211报名我们学校，学校会认为该生拿到优营也不会来，索性直接拒绝该生入营。</p>\\n<p><strong>bar：</strong> 入营的门槛，入营的难度，预推免的bar一般比夏令营低</p>\\n<p><strong>优营/offer：</strong> 优秀营员，夏令营考核通过的人，一般优营就是指的offer，即学校发给你的预录取承诺，928填该校，就会录取你（信誉好的学校一定会录取你，有些学校会鸽人）</p>\\n<p><strong>候补/wl：</strong> waiting list，候补队列，如果前面有人鸽了，那么优营名额就会顺延到你，大部分学校都有wl，很多学校会将候补顺序说明，有的学校不说你候补排名，就让你等（有点恶心）</p>\\n<h2>3.材料重要性</h2>\\n<p>（个人认为）：本科背景&gt;&gt;rk&gt;六级=科研&gt;竞赛&gt;其它</p>\\n<h2>4.考查内容：</h2>\\n<p>一般外校考察内容包括但不限于，机试、笔试、面试。面试必不可少，机试、笔试可有可无。</p>\\n<p><strong>机试：</strong> C++通用（极少数只能用C），Java、python看学校，建议使用C和C++进行练习（leetcode）</p>\\n<p><strong>笔试：</strong> 主要考察408的基础知识，数据结构&gt;操作系统=计网&gt;计组。</p>\\n<p><strong>面试：</strong> 面试在每个学校都必不可少，大致分为：英语能力考察，专业知识考察，项目问答，其它问答。形式为自我介绍（包括中文、英文或者PPT），综合问答。</p>\\n<ol>\\n<li><strong>英语能力考察：</strong> 英语一般分为英文问答，文献翻译等。英文问答中可能包括：自我介绍、日常问题、竞赛经历、项目经历等。</li>\\n<li><strong>专业知识考察：</strong> 408必不可少，数据结构&gt;操作系统=计网&gt;计组，并不代表计组一定不问，只能说几乎不问。其次是线代，概率论，高数，有些学校喜欢问数学。最后是机器学习，不管你是什么专业老师都喜欢问机器学习，常见机器学习算法一定要搞清楚，问的时候要能答出个80%来。</li>\\n<li><strong>项目问答：</strong> 这个自然有paper搞懂paper，没有的话自己理一下项目是为了解决什么问题的，你们是怎么做的，结果怎么样，你做了什么工作。</li>\\n<li><strong>其它问答：</strong> 其它可能还会有智力问答，闲聊等，问问高考省份，高考分数，你喜欢什么，为什么选择我们学校之类的。</li>\\n</ol>\\n<p><strong>以上就是考察的基本内容，自我感觉我的面试能力还可以，几乎面试不会紧张（练得多），英语基本没啥问题，专业课知识除数据结构外其余的复习较少，机器学习算法复习的多，但是<code>背景关</code>也就是初审<code>是最致命的</code>。</strong></p>\\n<p><strong>其它需要准备的东西：</strong> 要提前明确自己有没有推免资格，今年的推免名额变化很大，整体来看推免名额增多，但是有些政策发生变化，可能会导致一些人无法推免至外校甚至失去推免资格，这一点要格外关注。疫情结束，很多学校夏令营会在线下举行，要提前规划好去往各个城市的交通方式，以及是否需要中转等，整理好行李，一般来说夏天各个城市的服装差别不大，准备一些比较正式的服装，<strong>在面试时提高自己的形象（很重要）</strong>。<strong>准备好简历，获奖证书，各个学校要求的报名表和成绩证明等文书材料</strong>，在报名系统需要上传电子版，少数学校需要将复印件邮寄，这个一定要提前准备好，会节约大量时间，同时报名表和一些证明材料需要学院或者学校相关部门盖章，学校遇到节假日和一些会议会找不到人，容易错过一些学校的报名系统。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fbfcf522d12c4bd895bdefec338270ae~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1707&amp;h=1280&amp;s=820307&amp;e=jpg&amp;b=d1cfb3\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<h2>5.计算机院校难度排名</h2>\\n<p>具体可以参考下面的表（<strong>只作为参考，实际比较<code>每个人自身不同</code>，并无很大的意义</strong>）：\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6302de39481645cb970354e44b4cd1dc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=540&amp;h=960&amp;s=593965&amp;e=png&amp;b=f6f6f6\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<h2>6.总结</h2>\\n<p>今年保研由于保研率增加、强基计划、国优计划、工程硕博，可以说是机遇与风险并存，但总体趋势是四非越来越看<strong>信息了解程度和个人综合能力</strong>了。<strong>绿裙1:605176069 绿裙:2：943826679</strong>（全国最大的计算机保研交流群，里面会整理各种报名信息，计算机保研的都在里面！）</p>\\n<h1>三、夏令营</h1>\\n<p><strong>四非海投不解释</strong>，只想有个学上作为保底，也想夏令营有更多的<code>参与感</code>，同时夏令营自己rank排名太低了（专业人少，百分比更没法看）。记得我报的第一个学校是<code>南京信息工程大学</code>（南京双非，只是想报着玩玩的），结果他把我拒了（有<code>后续</code>）...我一时间感觉完了，这都不要我，那我咋办啊。<strong>于是更加坚定了我<code>海投</code>的决心和信念！</strong> 我记得总共是投了60多个学校吧，可能因为线下的原因，不是很顶尖的211基本上都进了，985bar太高了，根本进不去（在这里特别感谢<code>厦门大学</code>，给了我夏令营第一个985入营的机会）。</p>\\n<h2>1、未入：</h2>\\n<p><strong><code>一些自认为有机会型：</code></strong> 东北大学，西北工业大学（卡百分比，2%专业第一也到不了啊！），吉大SE、CS、AI（三连拒，心态稍微有些小崩，心里默念，祝福你今年必穿）；山东大学前沿交叉学院。</p>\\n<p><strong><code>一些双非有机会型：</code></strong> 浙大软件、南大软件（前两所可以说是四非的顶点了，在我看来），中海洋（就他，夏令营和预推免bar高的离谱啊，海洋啊海洋，你怎么这么自信呢），山大CS、SE、网安，北师AI，中科大科学岛，哈工大威海，中南CS，湖南，北邮CS、SE、AI）；重大；华师；华科SE。</p>\\n<h2>2、进入：</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00cc7103004645509b0528771109c311~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2716&amp;h=1262&amp;s=240280&amp;e=png&amp;b=ddede0\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n这里把一些我印象深刻的的列举了一下，中科院也入了几所，这里只介绍了我参与的。在开始说之前，接着上面的<code>南信工</code>说一下后续，我记得那天是<strong>6.22端午节</strong>，我刚和朋友刚看完电影，电影很感人快把我干哭了😭。 晚上十点刚出电影院，就收到了南信工的夏令营通知，后来发现大部分人都收到了。原来还没开营就已经被鸽穿了，之前非211都不要，建议入选23保研名场面，乐😊！</p>\\n<h3>1.西南大学</h3>\\n<p>西南大学是我参加的第一个夏令营，也是最让我印象深刻的一个夏令营。第一次去南方、第一次坐飞机、第一次参加夏令营、第一次线下考核、第一次和其他学校的学生一起住...所以这里我想以叙事的方式好好详细介绍一下～\\n<code>6.29号周四</code>，今天要飞去重庆了，参加西门大学夏令营，心里其实是没底的，走前还是简单的准备了个人介绍及读研目标，这也是我第一个夏令营，希望可以有一个好的结果，更重要的是我第一次去南方，第一次坐飞机✈️，心里还是很紧张忐忑的。中午和舍友一起吃了KFC，下午就一个人走了，去了北京国际机场，第一次去机场心里还是很激动的！（ps：去往国际机场的地铁很豪华，难怪收我25元😭），附一张北京的夜景图，美极了！\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/292cb3735c6a45429b73a87e09607338~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=1536&amp;s=406796&amp;e=jpg&amp;b=131826\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>漫步重庆街头，感觉真的很安详，加上天空还下起了小雨🌧️这个地方真的好舒适😄，但是火锅真的很辣很辣，还有阿姨说的话我真的啥也听不懂（我保证我已经很尽力去听了，但是我真的听不懂重庆话，虽然我大学舍友也有一个重庆人）。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/16531d8f18e34df4b1bb12e58325ddef~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=1536&amp;s=613669&amp;e=jpg&amp;b=bfd0dc\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n不过最重要的是。谁能告诉我，为什么重庆地铁这么深，鬼知道我已经下了四个这种很长的电梯了😲真不怕挖到岩浆吗，更离谱的是，<strong>地铁出去隧道口的那一刻我才发现我竟然还在上空</strong>，太离谱了...不愧是山城。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/14043a2787424d6c9a1f9dbafefdb28d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=1536&amp;s=493069&amp;e=jpg&amp;b=807e74\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n到学校了，学校真的超级大，我记得有8000多亩地，是<strong>全国单体校区最大的学校</strong>，附一张学校大门照片：\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/165c3b8d3d9e42f18a2388930ca6a719~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2048&amp;h=1536&amp;s=692363&amp;e=jpg&amp;b=cecfc3\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n报到的时候每个人给了我们每个人<strong>一件体血衫和含有100块的校园卡</strong>。宿舍在三楼，324，宿管阿姨已经帮助我们<strong>提前安排好了被褥</strong>，不得不说这个学院夏令营的安排真的很不错！宿舍感觉有点破旧，年岁应该很久了，详细地说这个学校感觉就很自然风，仿佛是丛林中建了一座学校，到处可见的参天大树🌳。\\n见到了舍友，这里要夸一下舍友真的超级好，都是大佬😎（西南石油、山东科技、中南民族）相处融洽，晚上一起吃了饭，买了果汁喝😄，在这里提前庆祝一下，四个舍友加上我最终都拿到自己心仪的offer：<strong>天津大学、南京理工大学、国防科技大学、山东大学</strong>，也算是圆满了！\\n在西南大学的第一天讲座+破冰。上午听了讲座报告，吃到了学院准备的点心，这要重点夸一下，真心很不错，下午和学姐学长玩小游戏，学姐对我们很热情，玩得也挺开心的😄\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b411ef032bae4f989e840e1ab4ec0a55~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2660&amp;h=1280&amp;s=547846&amp;e=jpg&amp;b=d4ccba\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n回宿舍之后就开始准备第二天的面试啦，也许是背了一晚上英语的原因，晚上睡觉的时候特累，早上醒来也是舍友叫醒的我。很巧不巧今天又下雨了，也没来得及吃饭，卡里还剩余17块没消费，早知道昨晚上花掉🥹\\n一到面试地点，学姐就为我们准备了咖啡饮料和吃的，该说不说，对我们真心不错！我面试排在中间，听前面面试的同学说很压力面，然后会问英语问题，顿时感到心里一慌。\\n等到我面试的时候，<strong>一共有三个老师</strong>，看得出来中间的老师应该是负责人，看起来就比较厉害。果不其然坐在中间的老师开始先让我来了一段<strong>中文自我介绍，然后老师问我英语问题</strong>。\\n第一个问题，你将来的职业愿景是什么，我记不起来他说的英语是什么了，但是我真的没听明白，第一个问题就把我尬住了。然后我说：<strong>sorry teacher…i cannot understand the word mean…</strong> 我以为我会就这样寄了。然后我没想到他竟然耐心的给我解释这个单词的意思，然后再去回答，于是我就把昨晚上背到的句子说了上去，最后提到了一句我想做一名老师。然后他又问我为什么想要做一名老师，我就又balabala一大顿，还没等我说，他就让我停了。\\n然后旁边的老师老师问我科研和项目，问到了项目原理和边缘计算。然后那个女老师问我还有没有报其他学校，我说我还报了贵校的人工智能学院和武汉的华中师范大学和华中农业大学，因为我特别喜欢武汉和重庆两座城市。然后那个女老师和蔼的笑了笑，又问我还有什么问题吗，然后我就问了一下 <strong>西大计院和人院</strong> 哪个好，坐在中间的老师很肯定地说：<strong>我们计院好！</strong> 然后我就点头答应，就结束了。总体感觉老师挺和蔼的，至少对我不是压力面！\\n中午和舍友吃了饭，这是我们这几天吃得最香的一顿饭，辣椒真的香到爆了！西南大学之旅结束，over！\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4d3dfd0c1859479393ac9011201c5ede~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1536&amp;h=1020&amp;s=542158&amp;e=jpg&amp;b=835e13\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>晚上飞机起飞的时候延误了两个点，起飞时已经凌晨十二点半了，为这段旅途又增添了些难忘的回忆，重庆之旅到这里就结束了🔚总体来说体验感不错，就是重庆感觉太湿太热了，正好这几天下雨，真的感觉身体和被子都很潮，<strong>北方人第一次来南方确实不太适应，就写到这吧</strong>，over！\\n补充：七月五号，西南大学优营名单出来了，我们宿舍四人都获得了优秀营员，完结撒花🌸</p>\\n<h3>2.中科院神仙院</h3>\\n<p>对<code>深先院</code>印象最深的就是有钱、有钱、太有钱！一共五天的活动，直接给我们报销机票，安排五星级酒店（第一次住这么豪华的酒店，真的哭死，床好舒服，想搬走），然后给了三件夏令营衣服和两份礼物（入营、退营礼包），最后还有花不完的餐票（最后一天花不完了，去学校超市买了200块钱的水果带走了，真的太有钱了！）\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d8d140afff9e4538bbc484e525fb3867~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4032&amp;h=3024&amp;s=6096077&amp;e=jpg&amp;b=8c816f\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9d25b1f6423a4c8393dc67dfb49e74b8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4032&amp;h=3024&amp;s=6933491&amp;e=jpg&amp;b=cdc5b4\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n在这几天里面，学校对我们安排特别到位，老师们充分为我们考虑，邀请了很多学长学姐来我们交流，同时在面试结束后安排了免费的深圳一日游，不得不说这几天应该是我人生中最挥霍的时光了，过得太舒服了！很喜欢这里，但是方向是合成生物学，要做的话也是偏交叉点，自己也是抱着学习和长见识的态度来的，这一趟收获真的很大！\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b1a66d797ff4404c8be58b307acd4479~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1334&amp;h=1178&amp;s=225177&amp;e=jpg&amp;b=36362e\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<p>最后的考核方式是<code>5分钟PPT介绍，然后英文文献翻译+老师现场提问</code>。我只记得老师对我做的人工智能方向很感兴趣，一直问，我也基本都答上来了，但是不知道为啥没拿到优营，可能是自己对合成生物这方面了解太少了的原因，不过我也很开心了这几天。</p>\\n<h3>3、厦门大学</h3>\\n<p>从深圳离开后去到了厦门大学，对厦门印象最深的就是那里的天空真的太蓝了，湛蓝色像是一幅画卷一样在空中展开。学校正对大海，出校门口就可以看到大海和沙滩。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/07f5eed6480f45f8b44b6fafc2e86a69~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4032&amp;h=3024&amp;s=6172371&amp;e=jpg&amp;b=72a3e3\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ba771bef4264548a98cea66b09dac09~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4032&amp;h=3024&amp;s=5403031&amp;e=jpg&amp;b=5b7ca3\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<p>学校和宿舍是分开的，需要坐公交车去到两公里外的宿舍区入住。宿舍是一个院子一个院子的，但是不知道是不是分配的原因，给我们分配到的宿舍比较差，感觉时间很久远了。（报到的时候给了我们一个帆布袋，一件Polo衫，一个营员证还有一个校园卡，但是卡里好像只有20元，emmm...是的，吃一顿就没了ovo）\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3d3c9a7a9961460aa85ba008c3b6b385~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1731&amp;h=740&amp;s=368421&amp;e=jpg&amp;b=0384da\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n厦门大学考核方式是<strong>机试+面试</strong>。机试是<code>纯C语言机试</code>，五道题需要做出3道题以上，也就是60分以上才可以参加面试，但是我机试太差了只做出了两道题四十分多点，所以没有资格去参加面试。但是听说过机试的并不是很多，面试的时候老师问的都比较简单，基本上机试过了拿到优营的概率是很大的！所以纯C语言机试真的很重要，希望后来的学弟学妹一定要重视机试，以后都是线下面试，所以机试越来越普遍和重要了！\\n因为我没机会去参加面试了嘛，第二天就和舍友去了鼓浪屿去玩了，不得不说人真的太多了。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/59746ea0efd14ea7959e23678477b69d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4032&amp;h=3024&amp;s=8343420&amp;e=jpg&amp;b=b3bcc2\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n厦门大学旅途结束后，我的夏令营基本就告一段落了。确实挺遗憾的，自己机试没有好好准备，挺可惜的，如果拿到了厦门大学的offer，以后基本就可以摆烂了，但是只能怪自己了。经过这几次经历之后，我也感觉累了，别的夏令营就都没去，回到学校准备保研材料和专业课复习了，全力备战预推免了。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/60320a300d214247853bba6c2f53b439~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1008&amp;h=650&amp;s=207345&amp;e=jpg&amp;b=7a858d\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<h2>3、总结：</h2>\\n<p>四非加rk太低了...，夏令营也只能到这了，想着预推免rk上去了。自己还能再冲一下985！把我自己的夏令营入营和考核一些经验和教训分享给大家：</p>\\n<ul>\\n<li><strong>1.</strong> 夏令营之前也就是<strong>前五学期绩点排名</strong>尽量往前冲，你如果有一个rank1的排名是很好用的，至少在夏令营阶段你就会获得不错的offer；</li>\\n<li><strong>2.</strong> 因为我自己的rank太低了，能进这么多211的营也挺不容易了，想了想主要是靠自己的国一竞赛、国家级大创、以及非常重要的<strong>英语六级</strong>！大家如果想上一个好的学校<strong>6级一定要过</strong>，并且尽量考到<code>500分</code>以上，对你有很大的帮助！</li>\\n<li><strong>3.</strong> <strong>机试越来越重要了！</strong> <code>纯C和C++语言机试</code>真的很重要，很多好的的学校在机试的时候会要求你用纯c语言或者C++去机试，希望后来的学弟学妹一定要重视机试。也有部分学校可以用CSP去抵机试，有能力的也要去打一打csp！而且以后都是<strong>线下面试</strong>，只要线下，80%的学校都会机试，不论是保研还是考研复试，<strong>机试只会越来越普遍</strong>，没有机试能力以后的路会很难走，一定要重视，这也是我自己的痛点和教训。</li>\\n<li><strong>4.好好打磨简历</strong>，多去尝试联系老师，说不定老师喜欢你真的会帮助你很多（亲身经历，证明很有效）这里附一份我自己的简历，我觉得这份简历的模版和格式在我看来已经很完美了，但是每个人看法和侧重也不一样，仅供参考：\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f763eda3dd114eb08d62aac21e533909~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3967&amp;h=5613&amp;s=1979327&amp;e=png&amp;b=fefefe\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></li>\\n</ul>\\n<h1>四、预推免</h1>\\n<p>作为疫情过后第一年的线下预推免，很多人说今年的预推免会简单很多，上一个985应该不会太难，我自己也是这么认为，但是相反的是今年<strong>985计算机bar真的高得离谱</strong>！\\n从保研名额下发时间比去年晚一周开始，我就料到了今年保研绝对不一般，很多985学校保研本校的名额变多了，一些认为自己保不上研的酒吧舞保研边缘er突然获得了推免资格，没有参加夏令营却参加了预推免，985高校招收的外校学生变少了，所以985的bar高的离谱。然而院校背景占比严重，<strong>211的10%，也比四非2%好走的很</strong>。我预推免是就 <strong>奔着<code>985</code></strong> 去了，前前后后投了20多个985，收获惨淡，反而211却很简单，我觉得大家可能都是奔着985去了。</p>\\n<h2>1、未入：</h2>\\n<p>预推免我的rank为2/29，又觉得自己是实验班，应该会好走很多，于是我的定位就是<strong>冲<code>中次9</code>，稳<code>末9顶2</code></strong>。前期很顺利拿到了<code>西电</code>的offer，于是开始疯狂投递<code>985</code>，却遭到了狠狠的拒绝。\\n<code>一些不理解的985高校的bar</code>：</p>\\n<ul>\\n<li>中国海洋大学CS：海洋啊海洋，你的bar为什么每年都是那么高，我们学校专业第一都进不去啊！你真的能招到学生吗....</li>\\n<li>山东大学SE：山大今年CS的bar，对于我们四非而言（计算机没评级），没猜错的是只要大专业（120人以上）的rank1。高得离谱的门槛，结果不出所料929当天被鸽穿了，哎~我也很想去济南啊，为什么不给我们这次机会，非得等929那天哭呢！\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e03bff0fd6d949fa972597c2c8a9394c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=540&amp;h=362&amp;s=146447&amp;e=png&amp;b=e2dcdc\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></li>\\n<li>中国农业大学信息学院：把我拒了，看不懂的bar，蜜汁操作...</li>\\n<li>天津大学：天大不用说了，向来看不起我们学院，近几年我们学院还真没去天大的，可想而知它的bar是啥 <strong>（if Tust AI: pass）</strong> 我觉得我学长说过一句话，<code>四非请远离天大，要不你会变得不幸</code>，这句话现在再看挺对的。</li>\\n</ul>\\n<h2>2、进入：</h2>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c6044bef9dc74e638a19102ceffe86b0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2478&amp;h=1038&amp;s=217516&amp;e=png&amp;b=ddede0\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<h2>3.重点介绍</h2>\\n<p><strong>山大信息学院人工智能：</strong>\\n这个得展开讲讲，山大青岛真的一直是梦校，最后终于如愿以偿！也算是上天再一次眷顾我了。没有山大我可能也去西电了o(╥﹏╥)o。\\n山大今年预推免依旧是<strong>强com</strong>，完全凭个人实力。即使知道它是强com，我依然在报名之前联系了老师，想着可以碰一碰运气，结果真让我碰到了，遇到了一位非常好的老师。我是15号那天联系的老师，山大报名20截止，当天晚上老师就回复我了，觉得我非常优秀以后，可以跟着他的方向去学习，然后让我加了老师的联系方式。因为老师时间比较忙，一直到17号晚上才和老师有了第一次视频通话。老师跟我讲了一下实验室的研究方向，欢迎我加入实验室，但是也和我说明白了如果真的想学习发论文的话加入他们是不错的选择（这届毕业的都发了sci2区，对计算机来说已经很牛了），当然如果你想混日子的话他也会帮我推荐别的实验室，老师也对我十分客气和亲切，感觉聊天让我没感觉到是跟老师去交流，反而更像是一位慈祥又严格的父亲。那一瞬间我觉得我找对老师了！老师跟我说这次的保研竞争十分激烈，然后表明我优秀的科研竞赛经历入营是没问题的，跟我说复试要准备的内容，让我好好准备。真的十分感谢老师！\\n23号结果出来了，我顺利进入了复试，我选的<code>人工智能专业</code>复试果然很激烈，<strong>40进5</strong>，但是我也充满了信息。山大信息学院复试是26号线下复试，校区在青岛（山大计算机除了软件都在青岛校区），和很多学校都撞时间了，为此我也放弃了可能最好的去华科的机会，全身心的投入到复试中去，英文自我介绍、英文问答、专业课数据结构、机器学习和项目科研经历我整理了厚厚的一摞纸，每天都在反复背诵和总结，为的就是面试的那一天拿出自己最好的状态。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6b04694f46924d33a16d1eccc3eec762~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1707&amp;h=1280&amp;s=245182&amp;e=jpg&amp;b=bacee0\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n25号我到了青岛，报了道，就回酒店好好准备明天的考核了。我的复试顺序在中间偏后，是个很不错的复试顺序，我在门外等待了两个多小时，终于轮到我了，大概有10个老师左右坐在桌子两边，你坐在最中央，复试的内容是：</p>\\n<ol>\\n<li><strong>3分钟英文自我介绍</strong>：因为我准备了很久了，所以很流利自信的说完了。</li>\\n<li><strong>2分钟英文问答：</strong> 问我的第一个英文问题是你<code>如何看待人工智能</code>，这个问题我正好准备了，我从两方面好的一面，坏的一面去说了，以及自己对这方面感兴趣流利地说完了；第二个问题是<code>介绍一下你的家乡</code>，我说My hometown is Rizhao. It is a coastal city with....吹了一大堆（反正挨着青岛呢，体验一下呗hhh）</li>\\n<li><strong>专业课机器学习</strong>：介绍一下你对<code>机器学习算法</code>的了解。我从监督学习到无监督学习介绍了一下，然后又说了深度学习，重点介绍了SVM，这也是我重点准备的算法；然后又问了我<code>rnn和cnn的区别</code>，这个就是基础的循环神经网络和卷积神经网络的介绍，说出重点、作用和不同来我觉得就可以啦，自认为这部分答得不错。</li>\\n<li><strong>简历面试：</strong> 老师们对我的国创项目CV方向很感兴趣，加上有个老师是做硬件的，就对我的边缘计算设备很感兴趣，就聊了一堆，自己回答的也不错，大概面试了将近20分钟我就出来啦。</li>\\n</ol>\\n<p>走出教学楼的一瞬间，我绷不住了，眼泪止不住的从眼角留了出来，我念念不忘的山大啊，我高中三年的梦想，那一瞬间我仿佛觉得什么都是值得的！\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46ed0f90c9ec410e8651a9b7df282f0b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1707&amp;h=1280&amp;s=389135&amp;e=jpg&amp;b=c0cfdf\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n面试结束当天，我在朋友圈发了这么一段话：\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d237e5d5143048d7aef0a8d5709cfa3c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1170&amp;h=941&amp;s=116722&amp;e=jpg&amp;b=fefefe\" alt=\"在这里插入图片描述\" loading=\"lazy\"/>\\n终于我如愿以偿了，27号下午回到学校的路上，我收到了山东大学信息学院复试成绩，我以<strong>复试第一名</strong>的成绩顺利获得offer，老师跟我说你一定要来啊，我说我肯定来，老师您放心，山大就是我心中最好的学校！\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e88761d244ef4ec882fea31b07e8fea8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1168&amp;h=556&amp;s=367856&amp;e=png&amp;b=fbfbfb\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<p>在29号中午12点开始我就和120的小伙伴们在焦急的等待录取结果，终于在1点我收到了待录取通知，我毫不犹豫的接受了山东大学的录取通知，我上岸啦！（也许是压力太大的原因，身上的一块大石终于可以放下来了，在上岸的第二天我就阳了，身体也陪我撑到了保研结束）\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e3aa8e5153644ae8ac1ee65e4fb5166a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1579&amp;h=773&amp;s=534707&amp;e=png&amp;b=fcfafa\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>\\n<h1>最后想说的话</h1>\\n<p>高中的梦想没想到如今却变成了现实，如今在回头看过去三年的努力，一切都是值得的！在这途中，有多少次晚上的夜不能寐，焦虑的心急如焚我已经数不清了，为了保研我失去了太多东西，也经历了人生的至暗时刻。但好在我也是幸运的，我遇到了很多生命中的贵人，遇到了待我如亲人的学长学姐（特别是120的学长学姐，还在一直帮助和关注着我们），最温馨温暖的舍友（每次拖着疲惫的身体回到宿舍，宿舍里的大家庭总能使我肆无忌惮的大笑），一起并肩而行同学，无私帮助我的老师们，无条件相信我给予我经济支持的家人，以及曾陪在我身边的人，在我人生最昏暗的时候给予我前进的光亮…\\n我是一个喜欢交流却不善表达自己的人，很少主动去表露自己的情绪和感情，所以每次我都想用笔迹去记录下来自己的感情，也感谢那些曾包容我的人。不管选择哪条路都要坚持走下去，我相信<strong>只要真正付出了，那么好的结果都会在未来以某种方式去回报你</strong>，最后也希望大家都可以如愿以偿，在以后的道路上一路生花！\\n阳光万里☀️微风正好☁️我可以有更多的时间去做自己喜欢的事情啦！站在人生新的起点，耳边又响起了熟悉的旋律“Tell them that I’m the one…”我的故事讲完啦，我们下次再见咯！\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be8669d373944edebe3cf6d2c8ea69f0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4032&amp;h=3024&amp;s=4093880&amp;e=jpg&amp;b=c8c7c0\" alt=\"在这里插入图片描述\" loading=\"lazy\"/></p>', 'author': '是Dream呀', 'reading_time': 20, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 27, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '9af29eb2383c044f2904c5a7ffbbaf47a123fa770c32e04ecddd30dd0f01c749', 'title': '探索AGI系列 | 03-01. 认知架构ACT-R理论与源码阅读 - Basic Utilities', 'url': 'https://juejin.cn/post/7283873796977508404', 'published_at': '2023-10-02T09:05:35Z', 'created_at': '2023-10-04T14:11:40.317765Z', 'changed_at': '2023-10-04T14:11:40.317765Z', 'content': '<p> </p>  <p>  </p> <h1>\\n            探索AGI系列 | 03-01. 认知架构ACT-R理论与源码阅读 - Basic Utilities\\n             </h1> <a href=\"https://juejin.cn/user/431473434562056/posts\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">\\n    MetaUniTech\\n      </a> <time datetime=\"2023-10-02T09:05:35.000Z\">\\n                    2023-10-02\\n                  </time>  \\n                    2\\n                   \\n                    阅读28分钟\\n                    <p></p>     <h2>关注公众号：MetaUniTech</h2>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9d824f04ba1a4ada9722524d14aa0e0f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=900&amp;h=383&amp;s=214367&amp;e=png&amp;b=265cd9\" alt=\"slogan.png\" loading=\"lazy\"/></p>\\n<h2>系列前情提要</h2>\\n<ul>\\n<li><a href=\"https://juejin.cn/post/7268533072986210343\" title=\"https://juejin.cn/post/7268533072986210343\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">探索AGI系列 | 01. LLM不等于通用人工智能望周知</a></li>\\n<li><a href=\"https://juejin.cn/post/7274856158174445622\" title=\"https://juejin.cn/post/7274856158174445622\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">探索AGI系列 | 02. 智能体方法论：Agent智能体 &amp; 认知架构（Cognitive Architecture）</a></li>\\n</ul>\\n<h2>前言</h2>\\n<p>读者朋友们好，上一篇《探索AGI系列 | 02. 智能体方法论：Agent智能体 &amp; 认知架构（Cognitive\\nArchitecture》在各个平台都受到了比较大的反响，一方面体现出大家对于本系列的兴趣，另一方面也给了笔者更大的创作动力。因此笔者在忙完工作之余马不停蹄的为大家更新接下来的内容。再上一章末尾，笔者为大家介绍了一下认知模型ACT-R的基础构成，在这一章，我们将更深入的对ACT-R进行庖丁解牛，从理论和代码实现角度剖析ACT-R的思路。理论方面，笔者借鉴了论文：An Integrated Theory of the Mind.\\n代码方面笔者阅读的是其python实现--PyACTR。原版实现应当是基于LISP的实现，Python版本与原版略有区别，但不影响我们的目的。</p>\\n<h2>Recap：ACT-R总览</h2>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/73d4ff3296cd463ba8f30f14b1e6d5dd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=979&amp;h=573&amp;s=108316&amp;e=png&amp;b=f1f1f1\" alt=\"actr.png\" loading=\"lazy\"/>\\nACT-R（Adaptive Control of Thought—Rational）是一种认知架构，用于建模人类认知过程和行为。它旨在模拟人类思维和决策的过程，以便研究和理解认知心理学现象。ACT-R包括多个模块，每个模块都有不同的功能，用于模拟特定认知功能。</p>\\n<ol>\\n<li>意图模块 (Intentional Module / Goal Module)\\n意图模块负责规划和控制行为。它确定当前任务的目标，协调其他模块的活动，以实现这些目标。意图模块是ACT-R中的执行控制中心。</li>\\n<li>陈述性模块 (Declarative Module)\\n陈述性模块用于存储事实、规则和概念的知识库。这允许模型访问和检索长期记忆中的信息，支持决策和问题解决过程。</li>\\n<li>视觉模块 (Visual Module)\\n视觉模块处理感觉输入，模拟人类的视觉处理过程。它负责感知和理解视觉信息，如物体、场景和符号。</li>\\n<li>手动模块 (Manual Module)\\n手动模块允许模型执行手部动作，例如移动、抓取物体等。它控制模型的运动和互动，模拟身体动作的执行。</li>\\n<li>生产模块 (Production Module / Procedural System)\\n生产模块是ACT-R的核心组成部分之一，用于表示知识和决策。它包括生产规则，描述了条件-动作对。当特定条件满足时，生产规则会触发执行相应的动作，模拟认知任务的决策过程。</li>\\n</ol>\\n<p>ACT-R的相关开发人员定义其为Hybrid模式架构，其Production Module采用了Symbolic思想，简单来说就是一系列If-then语句，不涉及对大脑的模拟。Declarative\\nModule模块采用了Connectionist的思想，定义了Activation Score来作为记忆是否获取的标准。因而ACT-R整体应该是属于Hybrid。</p>\\n<h2>模块详解</h2>\\n<p>在ACT-R认知架构中，有三个关键的&#34;基础建设&#34;单元，它们是：Buffer、Chunk和Productions。这些单元在支持认知任务的执行和模拟中起到了至关重要的角色。</p>\\n<ol>\\n<li>\\n<p>Buffer：\\nBuffer 是ACT-R中用于处理和暂存信息的关键组件。它类似于人类认知中的工作记忆，充当了信息的暂存区域。不同类型的 Buffer 用于处理不同类型的信息，如视觉输入、听觉输入、语音产出等。Buffer 负责接收、处理和传递信息，以支持模型执行感知、记忆、决策和执行等认知功能。</p>\\n</li>\\n<li>\\n<p>Chunk：\\nChunk 是ACT-R中的数据结构，用于表示和存储信息。每个 Chunk 包含了一个或多个属性（slots）以及这些属性相关联的值。块用于模拟和存储认知任务中的知识和信息。块可以看作是记忆的基本单元，它们允许模型存储和检索先前的知识，并在决策过程中使用它们。</p>\\n</li>\\n<li>\\n<p>Productions：\\nProductions 是ACT-R中的规则或指导模型行为的逻辑。它们表示了模型如何根据当前的情境和目标来进行决策和行动。Productions 定义了一系列条件和操作，当满足条件时，模型会执行相关操作。这使得模型能够在不同的情境下做出不同的决策和行动，从而模拟认知过程中的灵活性和适应性。</p>\\n</li>\\n</ol>\\n<p>这三个基础建设单元（Buffer、Chunk和Productions）共同构建了ACT-R认知架构的基础。Buffers 用于接收和传递信息，Chunks 用于存储知识和信息，而Productions 则用于规范模型的行为。它们协同工作，使ACT-R模型能够模拟和执行各种认知任务。</p>\\n<p>接下来我们将从代码实现角度去理解他们。</p>\\n<h3>Chunk</h3>\\n<p>Chunk是ACT-R中的信息单位，它是一个包含多个属性-值对的数据结构。每个属性-值对描述了信息的一部分，例如一个Chunk可以表示一个单词，它的属性可以包括&#34;词汇&#34;、&#34;语义&#34;、&#34;频率&#34;等等。Chunk用于表示和存储知识、事实、规则和任务状态。</p>\\n<p>在ACT-R中，Chunk可以存储在不同的Buffer中，以进行处理和传递。例如，视觉Buffer可以包含一个Chunk，表示一个模型看到的物体，听觉Buffer可以包含一个Chunk，表示听到的话语。Chunk的内容可以被提取、比较和操作，以支持认知任务的执行。</p>\\n<pre><code>class Chunk(Sequence):\\n    &#34;&#34;&#34;\\n    ACT-R chunks. Based on namedtuple (tuple with dictionary-like properties).\\n\\n    For example:\\n    &gt;&gt;&gt; Chunk(&#39;chunktype_example0&#39;, value=&#39;one&#39;)\\n    chunktype_example0(value= one)\\n    &#34;&#34;&#34;\\n\\n    class EmptyValue(object):\\n        &#34;&#34;&#34;\\n        Empty values used in chunks. These are None values.\\n        &#34;&#34;&#34;\\n        pass\\n    \\n    _chunktypes = {}\\n    _undefinedchunktypecounter = 0\\n    _chunks = {}\\n\\n    __emptyvalue = EmptyValue()\\n\\n    _similarities = {} \\n\\n    def __init__(self, typename, **dictionary):\\n        self.typename = typename\\n        self.boundvars = {} \\n\\n        kwargs = {}\\n        for key in dictionary:\\n\\n            \\n            if isinstance(dictionary[key], Chunk):\\n                dictionary[key] = utilities.VarvalClass(variables=None, values=dictionary[key], negvariables=(), negvalues=())\\n\\n            elif isinstance(dictionary[key], utilities.VarvalClass):\\n                for x in dictionary[key]._fields:\\n                    if x in {&#34;values&#34;, &#34;variables&#34;} and not isinstance(getattr(dictionary[key], x), str) and getattr(dictionary[key], x) != self.__emptyvalue and not isinstance(getattr(dictionary[key], x), Chunk):\\n                        raise TypeError(&#34;Values and variables must be strings, chunks or empty (None)&#34;)\\n\\n                    elif x in {&#34;negvariables&#34;, &#34;negvalues&#34;} and (not isinstance(getattr(dictionary[key], x), collections.abc.Sequence) or isinstance(getattr(dictionary[key], x), collections.abc.MutableSequence)):\\n                        raise TypeError(&#34;Negvalues and negvariables must be tuples&#34;)\\n\\n            elif (isinstance(dictionary[key], collections.abc.Iterable) and not isinstance(dictionary[key], str)) or not isinstance(dictionary[key], collections.abc.Hashable):\\n                raise ValueError(&#34;The value of a chunk slot must be hashable and not iterable; you are using an illegal type for the value of the chunk slot %s, namely %s&#34; % (key, type(dictionary[key])))\\n\\n            else:\\n                \\n                try:\\n                    temp_dict = utilities.stringsplitting(str(dictionary[key]))\\n                except utilities.ACTRError as e:\\n                    raise utilities.ACTRError(&#34;The chunk %s is not defined correctly; %s&#34; %(dictionary[key], e))\\n                loop_dict = temp_dict.copy()\\n                for x in loop_dict:\\n                    if x == &#34;negvariables&#34; or x == &#34;negvalues&#34;:\\n                        val = tuple(temp_dict[x])\\n                    else:\\n                        try:\\n                            val = temp_dict[x].pop()\\n                        except KeyError:\\n                            val = None\\n                    temp_dict[x] = val\\n                dictionary[key] = utilities.VarvalClass(**temp_dict)\\n\\n            \\n            kwargs[key+&#34;_&#34;] = dictionary[key]\\n        try:\\n            for elem in self._chunktypes[typename]._fields:\\n\\n                if elem not in kwargs:\\n\\n                    kwargs[elem] = self.__emptyvalue \\n                    dictionary[elem[:-1]] = self.__emptyvalue \\n\\n            if set(self._chunktypes[typename]._fields) != set(kwargs.keys()):\\n\\n                chunktype(typename, dictionary.keys())  \\n                warnings.warn(&#34;Chunk type %s is extended with new attributes&#34; % typename)\\n\\n        except KeyError:\\n\\n            chunktype(typename, dictionary.keys())  \\n            warnings.warn(&#34;Chunk type %s was not defined; added automatically&#34; % typename)\\n\\n        finally:\\n            self.actrchunk = self._chunktypes[typename](**kwargs)\\n\\n        self.__empty = None \\n        self.__unused = None \\n        self.__hash = None, self.boundvars.copy() \\n      \\n    def _asdict(self):\\n        &#34;&#34;&#34;\\n        Create a dictionary out of chunk.\\n        &#34;&#34;&#34;\\n        ...\\n\\n    def __eq__(self, otherchunk):\\n        if hash(self) == hash(otherchunk):\\n            return True\\n        else:\\n            return False\\n\\n    def __getattr__(self, name):\\n        if hasattr(self.actrchunk, name + &#34;_&#34;):\\n            return getattr(self.actrchunk, name + &#34;_&#34;)\\n        else:\\n            raise AttributeError(&#34;Chunk has no such attribute&#34;)\\n\\n    def __getitem__(self, pos):\\n        return re.sub(&#34;_$&#34;, &#34;&#34;, self.actrchunk._fields[pos]), self.actrchunk[pos]\\n\\n    def __hash__(self):\\n        ...\\n\\n    def __iter__(self):\\n        for x, y in zip(self.actrchunk._fields, self.actrchunk):\\n            yield re.sub(&#34;_$&#34;, &#34;&#34;, x), y\\n\\n    def __len__(self):\\n        return len(self.actrchunk)\\n\\n    def __repr__(self):\\n         ...\\n\\n    def __lt__(self, otherchunk):\\n        &#34;&#34;&#34;\\n        Check whether one chunk is proper part of another (given bound variables in boundvars).\\n        &#34;&#34;&#34;\\n        return not self == otherchunk and self.match(otherchunk, partialmatching=False)\\n\\n    def __le__(self, otherchunk):\\n        &#34;&#34;&#34;\\n        Check whether one chunk is part of another (given boundvariables in boundvars).\\n        &#34;&#34;&#34;\\n        return self == otherchunk or self.match(otherchunk, partialmatching=False) \\n\\n    def match(self, otherchunk, partialmatching, mismatch_penalty=1):\\n        &#34;&#34;&#34;\\n        Check partial match (given bound variables in boundvars).\\n        &#34;&#34;&#34;\\n        ...\\n      \\n    def removeempty(self):\\n        &#34;&#34;&#34;\\n        Remove slot-value pairs that have the value __emptyvalue, that is, None and &#39;None&#39;.\\n      \\n        Be careful! This returns a tuple with slot-value pairs.\\n        &#34;&#34;&#34;\\n        ...\\n\\n    def removeunused(self):\\n        &#34;&#34;&#34;\\n        Remove values that were only added to fill in empty slots, using None. \\n      \\n        Be careful! This returns a generator with slot-value pairs.\\n        &#34;&#34;&#34;\\n        ...\\n</code></pre>\\n<p>上面附上Chunk实现的部分代码，其中个别模块笔者忽略了具体的实现。从class的定义来看，Chunk类继承于Sequence类，并且在后面的代码中重载了一些魔法函数（一些软件工程上的优化例如防键值冲突等，与架构关系不大）。从Chunk的定义来看，chunk被定义为type和对应的slots，这点在代码中也有体现，Chunk类在__init__时，输入为typename和一些键值对。Chunk的实例化在代码实现中使用的是一个函数makechunk, 在实例化chunk之前首先需要定义chunktype，这两个函数在下面的实操示例代码中包含了。</p>\\n<pre><code>\\nfrom pyactr.chunks import chunktype, makechunk, chunkstring\\n\\nimport pyactr as actr\\n\\naddition = actr.ACTRModel()\\n\\nactr.chunktype(&#34;countOrder&#34;, (&#34;first&#34;, &#34;second&#34;)) \\n\\nactr.chunktype(&#34;add&#34;, (&#34;arg1&#34;, &#34;arg2&#34;, &#34;sum&#34;, &#34;count&#34;))\\n\\ndm = addition.decmem\\n\\nfor i in range(0, 11):\\n    dm.add(actr.makechunk(&#34;chunk&#34;+str(i), &#34;countOrder&#34;, first=i, second=i+1)) \\n...\\n\\ndef chunktype(cls_name, field_names, defaults=None):\\n    &#34;&#34;&#34;\\n    Creates type chunk. Works like namedtuple.\\n\\n    For example:\\n    &gt;&gt;&gt; chunktype(&#39;chunktype_example0&#39;, &#39;value&#39;)\\n\\n    :param field_names: an iterable or a string of slot names separated by spaces\\n    :param defaults: default values for the slots, given as an iterable, counting from the last element\\n    &#34;&#34;&#34;\\n    if cls_name in utilities.SPECIALCHUNKTYPES and field_names != utilities.SPECIALCHUNKTYPES[cls_name]:\\n        raise ACTRError(&#34;You cannot redefine slots of the chunk type &#39;%s&#39;; you can only use the slots &#39;%s&#39;&#34; % (cls_name, utilities.SPECIALCHUNKTYPES[cls_name]))\\n\\n    try:\\n        field_names = field_names.replace(&#39;,&#39;, &#39; &#39;).split()\\n    except AttributeError:  \\n        pass  \\n    field_names = tuple(sorted(name + &#34;_&#34; for name in field_names))\\n    for each in field_names:\\n        if each == &#34;ISA&#34; or each == &#34;isa&#34;:\\n            raise ACTRError(&#34;You cannot use the slot &#39;isa&#39; in your chunk. That slot is used to define chunktypes.&#34;)\\n    try:\\n        Chunk._chunktypes.update({cls_name:collections.namedtuple(cls_name, field_names, defaults=defaults)}) \\n    except TypeError:\\n        Chunk._chunktypes.update({cls_name:collections.namedtuple(cls_name, field_names)}) \\n\\n\\ndef makechunk(nameofchunk=&#34;&#34;, typename=&#34;&#34;, **dictionary):\\n    &#34;&#34;&#34;\\n    Create a chunk.\\n\\n    Three values can be specified:\\n    \\n    (i) the name of the chunk (the name could be used if the chunk appears as a value of other chunks or production rules)\\n    (ii) its type\\n    (ii) slot-value pairs.\\n\\n    For example:\\n    &gt;&gt;&gt; makechunk(nameofchunk=&#39;example0&#39;, typename=&#39;chunktype_example0&#39;, value=&#39;one&#39;)\\n    chunktype_example0(value= one)\\n\\n    This creates a chunk of type chunk1, which has one slot (value) and the value of that slot is one.\\n    &#34;&#34;&#34;\\n    if not nameofchunk:\\n        nameofchunk = &#34;unnamedchunk&#34;\\n    if not typename:\\n        typename = &#34;undefined&#34; + str(Chunk._undefinedchunktypecounter)\\n        Chunk._undefinedchunktypecounter += 1\\n    for key in dictionary:\\n        if isinstance(dictionary[key], Chunk):\\n            pass\\n        elif isinstance(dictionary[key], utilities.VarvalClass):\\n            pass\\n        else:\\n            try:\\n                temp_dict = utilities.stringsplitting(str(dictionary[key]))\\n            except utilities.ACTRError as e:\\n                raise utilities.ACTRError(&#34;The chunk value %s is not defined correctly; %s&#34; %(dictionary[key], e))\\n            loop_dict = temp_dict.copy()\\n            for x in loop_dict:\\n                if x == &#34;negvariables&#34; or x == &#34;negvalues&#34;:\\n                    val = tuple(temp_dict[x])\\n                else:\\n                    try:\\n                        val = temp_dict[x].pop()\\n                    except KeyError:\\n                        val = None\\n                temp_dict[x] = val\\n            dictionary[key] = utilities.VarvalClass(**temp_dict)\\n\\n    created_chunk = Chunk(typename, **dictionary)\\n    created_chunk._chunks[nameofchunk] = created_chunk\\n    return created_chunk\\n</code></pre>\\n<p>上述代码中体现了一些<code>工厂模式</code>的特征。在工厂模式中，有一个方法（或函数），它创建对象（在这种情况下是“chunks”），而不需要指定将创建的确切对象类别。chunktype根据输入参数cls_name、field_names和defaults创建并返回不同类型的块。被创建的对象（在这种情况下是块）通常被称为“产品”。在上述代码中，产品是collections.namedtuple类的实例，表示块类型。虽然代码不是严格遵循工厂方法模式（因为它不涉及继承或子类），但它展现了一种类似的概念，即根据输入参数创建对象（块类型），而客户端代码无需知道其构建细节。它封装了根据输入参数创建块类型的过程，类似于工厂方法模式封装了对象的创建。综合来看，Chunk 类在代码中既是块类型（chunk type）的定义者，也是块的实例化者。块类型的定义包括了块应该具有的属性，而块的实例则是具体的数据对象，具有这些属性的具体值。这使得代码能够有效地创建和操作不同类型的块，并在模拟 ACT-R 模型时使用它们。</p>\\n<p>值得注意的一点：一个Chunk块中可以包含<code>多个</code>type-slots键值对。根据这个特质，笔者很容易联想到图数据结构中的node attributes。事实上，很多基于ACT-R的实现都采用了图结构。</p>\\n<h3>Buffer</h3>\\n<p>Buffer是ACT-R中的信息处理单元，类似于短期工作记忆。每个Buffer都有特定的名称，表示不同的认知任务或处理阶段。以下是一些常见的Buffer：</p>\\n<ul>\\n<li>\\n<p>视觉缓冲区 (Visual Buffer)：用于处理视觉输入，例如从眼睛接收到的图像信息。它允许模型暂时存储和处理视觉信息，以支持感知和理解。</p>\\n</li>\\n<li>\\n<p>听觉缓冲区 (Auditory Buffer)：用于处理听觉输入，例如听到的声音和语音。它允许模型存储和操作听觉信息，支持语言理解和音频处理。</p>\\n</li>\\n<li>\\n<p>手动缓冲区 (Manual Buffer)：用于处理手动操作，例如按下按钮或移动鼠标。它允许模型暂时存储手动操作的信息，以支持互动和执行动作。</p>\\n</li>\\n<li>\\n<p>陈述性记忆缓冲区 (Declarative Memory Buffer)：陈述性记忆缓冲区用于与长期记忆中的知识进行交互。它允许模型从长期记忆中检索和存储信息，支持记忆和知识的使用。</p>\\n</li>\\n</ul>\\n<p>每个Buffer都可以包含一个或多个Chunk，这些Chunk是信息的基本单位。通过Buffer，ACT-R模型可以在不同的认知任务之间传递、存储和操作信息，从而模拟人类认知过程中的信息处理和转换。</p>\\n<pre><code>class Buffer(collections.abc.MutableSet):\\n    &#34;&#34;&#34;\\n    Buffer module.\\n    &#34;&#34;&#34;\\n\\n    _BUSY = utilities._BUSY\\n    _FREE = utilities._FREE\\n    _ERROR = utilities._ERROR\\n\\n    def __init__(self, dm=None, data=None):\\n        self.dm = dm\\n        self.state = self._FREE \\n        if data == None:\\n            self._data = set([])\\n        else:\\n            self._data = data\\n        assert len(self) &lt;= 1, &#34;Buffer can carry at most one element&#34;\\n\\n    @property\\n    def dm(self):\\n        &#34;&#34;&#34;\\n        Default harvest of goal buffer.\\n        &#34;&#34;&#34;\\n        return self.__dm\\n\\n    @dm.setter\\n    def dm(self, value):\\n        if isinstance(value, collections.abc.MutableMapping) or not value:\\n            self.__dm = value\\n        else:\\n            raise ValueError(&#39;The attempted dm value cannot be set; it is not a possible declarative memory&#39;)\\n\\n    def __contains__(self, elem):\\n        return elem in self._data\\n\\n    def __iter__(self):\\n        for elem in self._data:\\n            yield elem\\n\\n    def __len__(self):\\n        return len(self._data)\\n\\n    def __repr__(self):\\n        return repr(self._data)\\n\\n    def add(self, elem):\\n        &#34;&#34;&#34;\\n        Add a chunk into the buffer.\\n\\n        elem must be a chunk.\\n        &#34;&#34;&#34;\\n        ...\\n\\n    def discard(self, elem):\\n        &#34;&#34;&#34;\\n        Discard an element without clearing it into a memory.\\n        &#34;&#34;&#34;\\n        self._data.discard(elem)\\n    \\n    def show(self, attr):\\n        &#34;&#34;&#34;\\n        Print the content of the buffer.\\n        &#34;&#34;&#34;\\n        ...\\n\\n    def test_buffer(self, inquiry):\\n        &#34;&#34;&#34;\\n        Is buffer full/empty?\\n        &#34;&#34;&#34;\\n        ...\\n\\n    def modify(self, otherchunk, actrvariables=None):\\n        &#34;&#34;&#34;\\n        Modify the chunk in Buffer according to the info in otherchunk.\\n        &#34;&#34;&#34;\\n        ...\\n</code></pre>\\n<p>如上是基类Buffer的实现，继承于抽象基类MutableSet，这意味着在Buffer中的元素是会进行去重的。但这不代表Buffer可以存储多个元素，我们通过__init__的最后一行可以知道self._data的长度至多为1，也就是说从Memory中获取多个元素是可以的，如果这多个元素去重后是同一个，则Buffer不会报错。这也是为什么使用Set实现的原因。</p>\\n<p>同时，Buffer还定义了状态，每个Buffer实例只能进行原子操作，操作过程中实力状态会改成BUSY。</p>\\n<pre><code>class Buffer(collections.abc.MutableSet):\\n    ...\\n    def add(self, elem):\\n        &#34;&#34;&#34;\\n        Add a chunk into the buffer.\\n   \\n        elem must be a chunk.\\n        &#34;&#34;&#34;\\n        self._data = set()\\n        \\n        if isinstance(elem, chunks.Chunk):\\n            self._data.add(elem)\\n        else:\\n            raise TypeError(&#34;Only chunks can be added to Buffer&#34;)\\n</code></pre>\\n<p>通过重载的add函数我们可以知道，Buffer中的元素仅可为Chunk, 同时，在向Buffer中添加新元素的时候，会清空之前的值，因为每次调用Buffer.add()时会将self._data重新赋值为新的空集合。</p>\\n<pre><code>class Buffer(collections.abc.MutableSet):\\n    ...\\n    def modify(self, otherchunk, actrvariables=None):\\n        &#34;&#34;&#34;\\n        Modify the chunk in Buffer according to the info in otherchunk.\\n        &#34;&#34;&#34;\\n        if actrvariables == None:\\n            actrvariables = {}\\n        elem = self._data.pop()\\n        try:\\n            mod_attr_val = {x[0]: utilities.check_bound_vars(actrvariables, x[1]) for x in otherchunk.removeunused()} \\n        except utilities.ACTRError as arg:\\n            raise utilities.ACTRError(&#34;The modification by the chunk &#39;%s is impossible; %s&#34; % (otherchunk, arg))\\n        elem_attr_val = {x[0]: x[1] for x in elem}\\n        elem_attr_val.update(mod_attr_val) \\n        mod_chunk = chunks.Chunk(otherchunk.typename, **elem_attr_val) \\n\\n        self._data.add(mod_chunk) \\n        \\n    def show(self, attr):\\n        &#34;&#34;&#34;\\n        Print the content of the buffer.\\n        &#34;&#34;&#34;\\n        if self._data:\\n            chunk = self._data.copy().pop()\\n        else:\\n            chunk = None\\n        try:\\n            print(&#34; &#34;.join([str(attr), str(getattr(chunk, attr))]))\\n        except AttributeError:\\n            print(attr)\\n\\n</code></pre>\\n<p>上述代码实现的是对Buffer进行<code>修改/查看</code>的逻辑。我们可以看到每次对于Buffer中chunk的修改都是对该<code>chunk的副本</code>进行操作。查看时，复制一份原本的数据并pop出来；修改时对副本进行修改然后推回原本的集合。</p>\\n<p>每个模块都有其对应的Buffer，该模块Buffer类都继承于Buffer，例如Visual Module的buffer为Visual(buffers.Buffer)。<code>正常情况</code>下每个module仅有一个Buffer，每个module可以有好几个，例如Declarative Memory可以有多个，在存储的字典中，DM名若有重复，key被定义为<code>dm_&lt;idx&gt;</code>.</p>\\n<p>总结一下Buffer的特点：</p>\\n<ul>\\n<li>\\n<p><strong>接口和通信</strong>：Buffer充当了不同模块之间的接口，用于通信和数据传递。</p>\\n</li>\\n<li>\\n<p><strong>唯一性</strong>：每个Buffer有一个唯一的名称，通过该名称引用，如目标缓冲区和检索缓冲区。</p>\\n</li>\\n<li>\\n<p><strong>请求和响应</strong>：Buffer用于向其关联模块发送请求以执行操作，并可以查询其模块的状态。模块通过Buffer回应请求，通常生成事件来执行操作并将结果放入Buffer中。</p>\\n</li>\\n<li>\\n<p><strong>一次只有一个Chunk</strong>：每个Buffer一次只能容纳一个块（Chunk），这个块对所有其他模块可见。</p>\\n</li>\\n<li>\\n<p><strong>块的副本</strong>：当将块放入Buffer时，Buffer会制作该块的副本，任何对Buffer中块的更改都不会影响原始块。</p>\\n</li>\\n</ul>\\n<h3>Productions</h3>\\n<p><strong>Productions（产生式）</strong> 是 ACT-R 中的重要概念。每个产生式由以下三个主要组成部分构成：</p>\\n<p><strong>rule（规则）</strong>：</p>\\n<ul>\\n<li>条件部分：一系列模式的逻辑连接，用于匹配当前缓冲区的内容和缓冲区与模块的状态。</li>\\n</ul>\\n<p><strong>utility（效用）</strong>：</p>\\n<ul>\\n<li>表示产生式执行的效用，通常用于衡量执行该产生式的价值或重要性。</li>\\n</ul>\\n<p><strong>reward（奖励）</strong>：</p>\\n<ul>\\n<li>表示执行产生式后的奖励，用于指示执行结果的价值。</li>\\n</ul>\\n<p>当产生式的条件部分满足所有约束时，它可能会被选择执行。一旦执行，将按照定义的操作来执行，并且可以考虑产生的奖励。在ACT-R教程中，您可以找到有关如何指定和使用Productions的更多详细信息。</p>\\n<p>接下来，我们将深入代码层面，以更好地理解 <code>Production</code> 相关类的工作原理。</p>\\n<pre><code>class Production(collections.UserDict):\\n    &#34;&#34;&#34;\\n    Production rule.\\n    &#34;&#34;&#34;\\n\\n    def __init__(self, rule, utility, reward):\\n        self.rule = {}\\n        self.rule[&#39;rule&#39;] = rule\\n        self.rule[&#39;utility&#39;] = utility\\n        self.rule[&#39;reward&#39;] = reward\\n\\n        self.utility = utility\\n        self.reward = reward\\n\\n    def __contains__(self, elem):\\n        return elem in self.rule\\n\\n    def __iter__(self):\\n        for elem in self.rule:\\n            yield elem\\n\\n    def __len__(self):\\n        return len(self.rule)\\n\\n    def __getitem__(self, key):\\n        return self.rule[key]\\n    \\n    def __delitem__(self, key):\\n        del self.rule[key]\\n\\n    def __repr__(self):\\n        txt = &#39;&#39;\\n        production = self[&#34;rule&#34;]()\\n        utility = self[&#34;utility&#34;]\\n        reward = self[&#34;reward&#34;]\\n        txt += &#39;{}\\\\n==&gt;\\\\n{}&#39;.format(next(production), next(production))\\n        if utility:\\n            txt += &#34;\\\\nUtility: {}\\\\n&#34;.format(utility)\\n        if reward:\\n            txt += &#34;Reward: {}\\\\n&#34;.format(reward)\\n        return txt\\n\\n    def __setitem__(self, key, value):\\n        assert key in {&#34;rule&#34;, &#34;utility&#34;, &#34;reward&#34;}, &#34;The production can set only one of four values -- rule, utility, reward; you are using &#39;%s&#39;&#34; %key\\n        self.rule[key] = value\\n</code></pre>\\n<p>单条Production继承于collections.UserDict。为什么不使用namedtuple，笔者认为Production需要实现更加自定义化的键值关系，例如：  <code>__repr__</code>（Production的复现逻辑）。接下来我们来看多条Productions的类实现：</p>\\n<pre><code>class Productions(collections.UserDict):\\n    &#34;&#34;&#34;\\n    Production rules.\\n    &#34;&#34;&#34;\\n    \\n    _undefinedrulecounter = 0\\n\\n    __DFT_UTILITY = 0\\n    __DFT_REWARD = None\\n        \\n    def __init__(self, *rules):\\n        self.rules = {}\\n        for rule in rules:\\n            try:    \\n                utility_position = len(inspect.getargspec(rule).args)-inspect.getargspec(rule).args.index(&#39;utility&#39;)\\n            except ValueError:\\n                utility = self.__DFT_UTILITY\\n            else:\\n                utility = inspect.getargspec(rule).defaults[0-utility_position]\\n            try:    \\n                reward_position = len(inspect.getargspec(rule).args)-inspect.getargspec(rule).args.index(&#39;reward&#39;)\\n            except ValueError:\\n                reward = self.__DFT_REWARD\\n            else:\\n                reward = inspect.getargspec(rule).defaults[0-reward_position]\\n            self.update({rule.__name__: {&#39;rule&#39;: rule, &#39;utility&#39;: utility, &#39;reward&#39;: reward}})\\n\\n        self.used_rulenames = {} \\n\\n    def __contains__(self, elem):\\n        return elem in self.rules\\n\\n    def __iter__(self):\\n        for elem in self.rules:\\n            yield elem\\n\\n    def __len__(self):\\n        return len(self.rules)\\n\\n    def __getitem__(self, key):\\n        return self.rules.get(key)\\n    \\n    def __delitem__(self, key):\\n        del self.rules[key]\\n\\n    def __repr__(self):\\n        txt = &#39;&#39;\\n        for rulename in self.rules:\\n            txt += rulename + &#39;{}\\\\n&#39;.format(self.rules[rulename])\\n        return txt\\n\\n    def __setitem__(self, key, value):\\n        if isinstance(value, collections.abc.MutableMapping):\\n            self.rules[key] = Production(**value)\\n        else:\\n            self.rules[key] = Production(rule=value[&#34;rule&#34;], utility=self.__DFT_UTILITY, reward=self.__DFT_REWARD)\\n\\n    def __collapse__(self, rule1, rule2, slotvals, retrieval):\\n        &#34;&#34;&#34;\\n        Collapses 2 rules into 1.\\n        &#34;&#34;&#34;\\n        ...\\n        return Production(rule=func, utility=self.__DFT_UTILITY, reward=self.__DFT_REWARD)\\n\\n    def __rename__(self, name, variables):\\n        &#34;&#34;&#34;\\n        Rename production, so that variable names do not clash. name is used to change the variable name to minimize clash. Returns the production with the new name.\\n        &#34;&#34;&#34;\\n        ... \\n        return Production(rule=func, utility=self.__DFT_UTILITY, reward=self.__DFT_REWARD) \\n\\n    def __substitute__(self, rule, variable_dict, val_dict):\\n        &#34;&#34;&#34;\\n        Substitutes variables in rule1 and rule2 according to the information in the dictionary variable_dict.\\n        &#34;&#34;&#34;\\n        ...\\n        return Production(rule=func, utility=rule[&#39;utility&#39;], reward=rule[&#39;reward&#39;])\\n\\n    def __check_valid_compilation__(self, rule_name1, rule_name2, buffers):\\n        &#34;&#34;&#34;\\n        Check that production compilation is valid. There are several cases in which compilation is blocked because it would result in an unsafe rule (a rule that might differ in its output compared to the original rule1 and rule2 firing one after the other). The function returns True if the production compilation is unsafe and should be stopped.\\n        &#34;&#34;&#34;\\n        ...\\n\\n    def compile_rules(self, rule_name1, rule_name2, slotvals, buffers, model_parameters):\\n        &#34;&#34;&#34;\\n        Rule compilation.\\n        &#34;&#34;&#34;\\n        ...\\n        return new_name, re_created\\n</code></pre>\\n<p>在Productions初始化阶段代码定义了一个名为 <code>Productions</code> 的自定义类，用于表示一组产生式规则。以下是代码段的流程解释：</p>\\n<ol>\\n<li>\\n<p><code>Productions</code> 类继承自 <code>collections.UserDict</code>，这意味着它具有字典类的特性，但可以进行自定义扩展。</p>\\n</li>\\n<li>\\n<p><code>_undefinedrulecounter</code> 是一个类属性，用于跟踪未定义的规则数量，初始化为0。</p>\\n</li>\\n<li>\\n<p><code>__DFT_UTILITY</code> 和 <code>__DFT_REWARD</code> 是类属性，分别表示默认的效用（utility）和奖励（reward）值。这些默认值将用于规则中未明确指定效用和奖励的情况。</p>\\n</li>\\n<li>\\n<p>在 <code>__init__</code> 方法中，接受了一个可变数量的参数 <code>*rules</code>，这表示可以传递多个规则函数作为参数。</p>\\n</li>\\n<li>\\n<p>在 <code>__init__</code> 方法中，通过迭代传递的规则函数（<code>*rules</code>）来创建规则字典（<code>self.rules</code>）。对于每个规则函数，以下步骤会重复执行：</p>\\n<ul>\\n<li>使用 <code>inspect.getargspec</code> 函数来检查规则函数的参数和默认值。</li>\\n<li>通过查找参数中的 <code>&#39;utility&#39;</code> 和 <code>&#39;reward&#39;</code> 来确定效用和奖励的位置。</li>\\n<li>如果规则函数中未找到 <code>&#39;utility&#39;</code>，则使用默认效用值 <code>__DFT_UTILITY</code>。</li>\\n<li>如果规则函数中未找到 <code>&#39;reward&#39;</code>，则使用默认奖励值 <code>__DFT_REWARD</code>。</li>\\n<li>使用规则函数的名称作为键，将规则函数、效用和奖励的信息存储为字典值，将其添加到 <code>self.rules</code> 字典中。</li>\\n</ul>\\n</li>\\n<li>\\n<p>最后，<code>self.used_rulenames</code> 初始化为空字典，用于跟踪已使用的规则名称，这在效用学习方面可能会用到。</p>\\n</li>\\n</ol>\\n<p>Productions定义了 <code>规则编译</code> 的逻辑。在ACT-R中，<strong>规则编译（Rule Compilation）</strong> 是一种优化技术，用于将高级认知规则翻译成底层的操作指令，以便在模拟中更高效地执行这些规则。这项技术的主要作用如下：</p>\\n<ol>\\n<li>\\n<p><strong>提高模拟效率</strong>：ACT-R模型通常包括许多高级的认知规则，这些规则用于描述认知过程，例如决策、记忆检索等。将这些高级规则翻译成底层操作指令可以显著提高模拟的效率，使模型能够更快速地执行任务。</p>\\n</li>\\n<li>\\n<p><strong>精确模拟</strong>：规则编译可以确保模拟的行为与实际认知过程更加一致。通过将高级规则转化为底层操作，可以更准确地模拟认知过程中的每个步骤和决策。</p>\\n</li>\\n<li>\\n<p><strong>支持实验设计</strong>：在ACT-R中进行实验设计时，规则编译可以确保模型在执行任务时遵循与实验一致的步骤和时间表。这有助于研究人员更好地理解和解释实验结果。</p>\\n</li>\\n<li>\\n<p><strong>提高模型的可解释性</strong>：规则编译将高级规则转化为可执行的操作指令，使模型的行为更容易理解和解释。研究人员可以更清晰地查看模型是如何执行任务的，从而更好地分析模型的行为和决策。</p>\\n</li>\\n</ol>\\n<pre><code>class Productions(collections.UserDict):\\n    \\n    ...\\n    \\n    def compile_rules(self, rule_name1, rule_name2, slotvals, buffers, model_parameters):\\n        &#34;&#34;&#34;\\n        Rule compilation.\\n        &#34;&#34;&#34;\\n        slotvals = slotvals.copy()\\n\\n        stop = self.__check_valid_compilation__(rule_name1, rule_name2, buffers)\\n        if stop:\\n            return False, False\\n\\n        \\n        modified_actrvariables = set()\\n\\n        \\n        retrieval = None\\n        for buff in slotvals:\\n            if slotvals[buff]:\\n                if isinstance(slotvals[buff], collections.abc.MutableSequence):\\n                    retrieval = buff \\n                    \\n                    if buffers[buff]:\\n                        return False, False\\n                else:\\n                    for slot in slotvals[buff]:\\n                        if slotvals[buff][slot] != None:\\n                            var = slotvals[buff][slot].variables\\n                            if var != None:\\n                                modified_actrvariables.add(var)\\n\\n        new_2rule = self.__rename__(rule_name2, modified_actrvariables) \\n\\n        production2 = new_2rule[&#39;rule&#39;]()\\n        \\n        pro2 = next(production2)\\n\\n        matched, valued = utilities.match(pro2, slotvals, rule_name1, rule_name2)\\n\\n        new_1rule = self.__substitute__(self[rule_name1], matched, valued)\\n        \\n        new_2rule = self.__substitute__(new_2rule, matched, valued)\\n\\n        for buff in slotvals:\\n            mod_attr_val = {}\\n            if slotvals[buff]:\\n                for elem in slotvals[buff]:\\n                    varval = utilities.make_chunkparts_with_new_vars(slotvals[buff][elem], matched, valued)\\n                    mod_attr_val[elem] = varval\\n                slotvals[buff] = mod_attr_val\\n\\n        new_rule = self.__collapse__(new_1rule, new_2rule, slotvals, retrieval)\\n        \\n        idx = 0\\n        re_created = &#34;CREATED&#34;\\n        while True:\\n            if idx &gt; 0:\\n                new_name = &#34; &#34;.join([str(rule_name1), &#34;and&#34;, str(rule_name2), str(idx)])\\n            else:\\n                new_name = &#34; &#34;.join([str(rule_name1), &#34;and&#34;, str(rule_name2)])\\n            if self.__getitem__(new_name):\\n                pr1 = self[new_name][&#34;rule&#34;]()\\n                pr2 = new_rule[&#34;rule&#34;]()\\n                if next(pr1) == next(pr2) and next(pr1) == next(pr2):\\n                    re_created = &#34;RE-CREATED&#34;\\n                    if model_parameters[&#34;utility_learning&#34;]:\\n                        self[new_name][&#34;utility&#34;] = round(self[new_name][&#34;utility&#34;] + model_parameters[&#34;utility_alpha&#34;]*(self[rule_name1][&#34;utility&#34;]-self[new_name][&#34;utility&#34;]), 4)\\n                    break\\n                else:\\n                    idx += 1\\n            else:\\n                self[new_name] = new_rule\\n                break\\n\\n        return new_name, re_created\\n</code></pre>\\n<ol>\\n<li>\\n<p><code>compile_rules</code> 方法接受多个参数，其中包括 <code>rule_name1</code> 和 <code>rule_name2</code>，这是要编译的两个规则的名称，以及其他参数如 <code>slotvals</code>、<code>buffers</code> 和 <code>model_parameters</code>。</p>\\n</li>\\n<li>\\n<p>在开始编译之前，首先复制了 <code>slotvals</code>，以便进行修改而不影响原始数据。</p>\\n</li>\\n<li>\\n<p>通过调用 <code>__check_valid_compilation__</code> 方法来检查编译是否有效，如果不是有效的编译，则返回 <code>False</code>。这是一个用于检查编译合法性的内部方法。</p>\\n</li>\\n<li>\\n<p>在编译之前，代码会处理 <code>slotvals</code> 中的变量，确保它们不会与其他变量发生冲突。这包括获取那些不能与其他变量冲突的变量，并检查缓冲区中是否存在与检索相关的信息。如果存在与检索相关的信息，并且在 <code>rule_name2</code> 结束时未丢弃，则不进行编译，返回 <code>False</code>。</p>\\n</li>\\n<li>\\n<p>接下来，代码会对 <code>rule_name2</code> 中的所有变量进行重命名，以避免变量之间的冲突。</p>\\n</li>\\n<li>\\n<p>创建 <code>rule_name2</code> 的产生式实例，并进行匹配操作，将 <code>rule_name2</code> 中的模式与 <code>slotvals</code> 进行匹配，以获取匹配和值的信息。</p>\\n</li>\\n<li>\\n<p>使用匹配和值的信息，替换 <code>rule_name1</code> 和 <code>rule_name2</code> 中的变量，生成新的规则。</p>\\n</li>\\n<li>\\n<p>对于 <code>slotvals</code> 中的每个缓冲区，将变量值修改为匹配和值后的新值。</p>\\n</li>\\n<li>\\n<p>最后，将新生成的规则与现有的规则进行比较，以确定是否需要重新创建规则。如果已经存在具有相同规则的规则，则根据模型参数进行效用学习的更新。如果没有相同规则的规则，则将新规则添加到规则集中，并返回新规则的名称。</p>\\n</li>\\n</ol>\\n<p>这段代码实现了规则编译的过程，<strong>用于将两个规则合并成一个规则</strong>，以提高模型的效率和精确性。这个过程包括匹配、替换、重命名和效用学习等步骤，以确保生成的新规则是有效的。</p>\\n<pre><code>\\nclass ProductionRules(object):\\n    &#34;&#34;&#34;\\n    Production knowledge.\\n    &#34;&#34;&#34;\\n\\n    _UNKNOWN = utilities._UNKNOWN\\n    _PROCEDURAL = utilities._PROCEDURAL\\n    _EMPTY = utilities._EMPTY\\n    _RHSCONVENTIONS = utilities._RHSCONVENTIONS\\n    _LHSCONVENTIONS = utilities._LHSCONVENTIONS\\n    _INTERRUPTIBLE = utilities._INTERRUPTIBLE\\n\\n    def __init__(self, rules, buffers, dm, model_parameters):\\n        self.__actrvariables = {} \\n        self.rules = rules\\n        self.ordered_rulenames = sorted(rules.keys(), key=lambda x: rules[x][&#39;utility&#39;], reverse=True) \\n\\n        self.last_rule = None \\n        self.last_rule_slotvals = {key: None for key in buffers} \\n        self.current_slotvals = {key: None for key in buffers} \\n        self.compile = [] \\n\\n        self.buffers = buffers \\n\\n        self.procs = [] \\n\\n        self.dm = dm \\n\\n        self.env_interaction = set() \\n\\n        self.model_parameters = model_parameters\\n\\n    def procedural_process(self, start_time=0):\\n        ...\\n\\n    def compile_rules(self):\\n        ...\\n\\n    def update(self, RHSdictionary, time):\\n        ...\\n\\n    def extra_test(self, name, tested, test, temp_actrvariables, time):\\n        ...\\n\\n    def clear(self, name, cleared, optional, temp_actrvariables, time, freeing=True):\\n        ...\\n\\n    def execute(self, name, executed, executecommand, temp_actrvariables, time):\\n        ...\\n\\n    def modify(self, name, modified, otherchunk, temp_actrvariables, time):\\n        ...\\n\\n    def modify_request(self, name, modified, otherchunk, temp_actrvariables, time):\\n        ...\\n\\n    def overwrite(self, name, updated, otherchunk, temp_actrvariables, time):\\n        ...\\n\\n    def visualencode(self, name, visualbuffer, chunk, temp_actrvariables, time, extra_time, site):\\n        ...\\n\\n    def retrieveorset(self, name, updated, otherchunk, temp_actrvariables, time):\\n        ...\\n\\n    def retrieve(self, name, retrieval, otherchunk, temp_actrvariables, time):\\n        ...\\n\\n    def automatic_search(self, name, visualbuffer, stim, time):\\n        ...\\n\\n    def automatic_buffering(self, name, visualbuffer, stim, time):\\n        ...\\n\\n    def visualshift(self, name, visualbuffer, otherchunk, temp_actrvariables, time):\\n        ...\\n\\n    def visualcontinue(self, name, visualbuffer, otherchunk, temp_actrvariables, time, extra_time, site):\\n        ...\\n\\n    def motorset(self, name, motorbuffer, otherchunk, temp_actrvariables, time):\\n        ...\\n\\n    def motorcontinue(self, name, motorbuffer, otherchunk, temp_actrvariables, time, time_presses):\\n        ...\\n\\n    def LHStest(self, dictionary, actrvariables, update=False):\\n        ...\\n\\n    def test(self, submodule_name, tested, testchunk, temp_actrvariables):\\n        ...\\n\\n    def query(self, submodule_name, tested, testdict, temp_actrvariables):\\n        ...\\n\\n</code></pre>\\n<p><code>ProductionRules</code> 类是一种用于处理和管理产生式规则的 Python 类。这个类的主要作用是在一个认知体系架构中实现产生式系统的操作和规则处理。<code>ProductionRules</code> 类可以通过多种方式来管理 <code>Productions</code>。</p>\\n<ol>\\n<li>\\n<p><strong>添加和删除规则</strong>：<code>ProductionRules</code> 类可以提供方法，允许用户添加新的产生式规则或删除现有的规则。这样，用户可以根据需要灵活地管理规则库。例如，可以实现 <code>add_rule</code> 和 <code>remove_rule</code> 方法来添加和删除规则。</p>\\n</li>\\n<li>\\n<p><strong>规则执行</strong>：<code>ProductionRules</code> 类可以提供方法，用于执行匹配的产生式规则。这包括在系统状态满足规则条件时触发规则的执行。执行规则时，可以调用 <code>Productions</code> 类的相关方法来执行规则中定义的操作。</p>\\n</li>\\n<li>\\n<p><strong>规则选择</strong>：<code>ProductionRules</code> 类可以维护规则的排序列表，以便系统可以根据规则的效用（utility）选择要执行的规则。这种选择可以基于规则的条件和其他上下文信息。选择规则后，可以调用 <code>Productions</code> 类的方法来执行规则。</p>\\n</li>\\n<li>\\n<p><strong>规则编译</strong>：如果需要，<code>ProductionRules</code> 类可以提供编译规则的功能，将规则转化为更高效的内部表示形式。编译后的规则可以更快地执行。这种编译可以涉及将规则中的条件和动作转化为适合系统执行的形式。这一过程可以在规则执行之前进行。</p>\\n</li>\\n<li>\\n<p><strong>规则检索</strong>：<code>ProductionRules</code> 类可以支持按名称或其他属性检索规则的功能。这使得用户可以根据需要检索和查看规则，以了解规则库的内容。</p>\\n</li>\\n<li>\\n<p><strong>规则管理</strong>：除了添加和删除规则外，<code>ProductionRules</code> 类还可以支持规则的编辑和更新。这允许用户对现有规则进行修改，以适应系统的需求。</p>\\n</li>\\n</ol>\\n<p><code>ProductionRules</code> 类可以充当规则库的中央管理器，负责规则的管理、执行和编译。通常情况下被用于对<code>已经使用过的Productions</code>进行管理，一般在Simulation阶段进行实例化。Productions是在Model初始化阶段就会被实例化成为一个class member.</p>\\n<p>这里附上一段Model的代码：</p>\\n<pre><code>class ACTRModel(object):\\n    &#34;&#34;&#34;\\n    ACT-R model, running ACT-R simulations.\\n\\n    model_parameters and their default values are:\\n    {&#34;subsymbolic&#34;: False,\\n    &#34;rule_firing&#34;: 0.05,\\n    &#34;latency_factor&#34;: 0.1,\\n    &#34;latency_exponent&#34;: 1.0,\\n    &#34;decay&#34;: 0.5,\\n    &#34;baselevel_learning&#34;: True,\\n    &#34;optimized_learning&#34;: False,\\n    &#34;instantaneous_noise&#34; : 0,\\n    &#34;retrieval_threshold&#34; : 0,\\n    &#34;buffer_spreading_activation&#34; : {},\\n    &#34;spreading_activation_restricted&#34; : False,\\n    &#34;strength_of_association&#34;: 0,\\n    &#34;association_only_from_chunks&#34;: True,\\n    &#34;partial_matching&#34;: False,\\n    &#34;mismatch_penalty&#34;: 1,\\n    &#34;activation_trace&#34;: False,\\n    &#34;utility_noise&#34;: 0,\\n    &#34;utility_learning&#34;: False,\\n    &#34;utility_alpha&#34;: 0.2,\\n    &#34;motor_prepared&#34;: False,\\n    &#34;strict_harvesting&#34;: False,\\n    &#34;production_compilation&#34;: False,\\n    &#34;automatic_visual_search&#34;: True,\\n    &#34;emma&#34;: True,\\n    &#34;emma_noise&#34;: True,\\n    &#34;emma_landing_site_noise&#34;: False,\\n    &#34;eye_mvt_angle_parameter&#34;: 1,\\n    &#34;eye_mvt_scaling_parameter&#34;: 0.01\\n    }\\n\\n    environment has to be an instantiation of the class Environment.\\n    &#34;&#34;&#34;\\n\\n    MODEL_PARAMETERS = {&#34;subsymbolic&#34;: False,\\n                &#34;rule_firing&#34;: 0.05,\\n                &#34;latency_factor&#34;: 0.1,\\n                &#34;latency_exponent&#34;: 1.0,\\n                &#34;decay&#34;: 0.5,\\n                &#34;baselevel_learning&#34;: True,\\n                &#34;optimized_learning&#34;: False,\\n                &#34;instantaneous_noise&#34; : 0,\\n                &#34;retrieval_threshold&#34; : 0,\\n                &#34;buffer_spreading_activation&#34; : {},\\n                &#34;spreading_activation_restricted&#34; : False,\\n                &#34;strength_of_association&#34;: 0,\\n                &#34;association_only_from_chunks&#34;: True,\\n                &#34;partial_matching&#34;: False,\\n                &#34;mismatch_penalty&#34;: 1,\\n                &#34;activation_trace&#34;: False,\\n                &#34;utility_noise&#34;: 0,\\n                &#34;utility_learning&#34;: False,\\n                &#34;utility_alpha&#34;: 0.2,\\n                &#34;motor_prepared&#34;: False,\\n                &#34;strict_harvesting&#34;: False,\\n                &#34;production_compilation&#34;: False,\\n                &#34;automatic_visual_search&#34;: True,\\n                &#34;emma&#34;: True,\\n                &#34;emma_noise&#34;: True,\\n                &#34;emma_landing_site_noise&#34;: False,\\n                &#34;eye_mvt_angle_parameter&#34;: 1, \\n                &#34;eye_mvt_scaling_parameter&#34;: 0.01, \\n                }\\n\\n    def __init__(self, environment=None, **model_parameters):\\n\\n        self.chunktype = chunks.chunktype\\n        self.chunkstring = chunks.chunkstring\\n\\n        self.visbuffers = {}\\n\\n        start_goal = goals.Goal()\\n        self.goals = {&#34;g&#34;: start_goal}\\n\\n        self.__buffers = {&#34;g&#34;: start_goal}\\n\\n        start_retrieval = declarative.DecMemBuffer()\\n        self.retrievals = {&#34;retrieval&#34;: start_retrieval}\\n        \\n        self.__buffers[&#34;retrieval&#34;] = start_retrieval\\n        \\n        start_dm = declarative.DecMem()\\n        self.decmems = {&#34;decmem&#34;: start_dm}\\n        &#39;&#39;&#39;\\n        这里是Productions的初始化\\n        &#39;&#39;&#39;\\n        self.productions = productions.Productions() \\n        self.__similarities = {}\\n\\n        self.model_parameters = self.MODEL_PARAMETERS.copy()\\n\\n        try:\\n            if not set(model_parameters.keys()).issubset(set(self.MODEL_PARAMETERS.keys())):\\n                raise(utilities.ACTRError(&#34;Incorrect model parameter(s) %s. The only possible model parameters are: &#39;%s&#39;&#34; % (set(model_parameters.keys()).difference(set(self.MODEL_PARAMETERS.keys())), set(self.MODEL_PARAMETERS.keys()))))\\n            self.model_parameters.update(model_parameters)\\n        except TypeError:\\n            pass\\n\\n        self.__env = environment\\n\\n   ...\\n   \\n   def simulation(self, realtime=False, trace=True, gui=True, initial_time=0, environment_process=None, **kwargs):\\n   &#34;&#34;&#34;\\n   Prepare simulation of the model\\n   \\n   This does not run the simulation, it only returns the simulation object. The object can then be run using run(max_time) command.\\n   \\n   realtime: should the simulation be run in real time or not?\\n   trace: should the trace of the simulation be printed?\\n   gui: should the environment appear on a separate screen? (This requires tkinter.)\\n   initial_time: what is the starting time point of the simulation?\\n   environment_process: what environment process should the simulation use?\\n   The environment_process argument should be supplied with the method environment_process of the environment used in the model.\\n   kwargs are arguments that environment_process will be supplied with.\\n   &#34;&#34;&#34;\\n   \\n   if len(self.decmems) == 1:\\n      for key in self.__buffers:\\n          self.__buffers[key].dm = self.decmem \\n   elif len([x for x in self.decmems.values() if x]) == 1:\\n      for key in self.__buffers:\\n          if not self.__buffers[key].dm:\\n              self.__buffers[key].dm = self.decmem \\n   \\n   decmem = {name: self.__buffers[name].dm for name in self.__buffers\\\\\\n          if self.__buffers[name].dm != None} \\n   \\n   self.__buffers[&#34;manual&#34;] = motor.Motor() \\n   \\n   if self.__env:\\n      self.__env.initial_time = initial_time \\n      if self.visbuffers:\\n          self.__buffers.update(self.visbuffers)\\n      else:\\n          dm = list(decmem.values())[0]\\n          self.__buffers[&#34;visual&#34;] = vision.Visual(self.__env, dm) \\n          self.__buffers[&#34;visual_location&#34;] = vision.VisualLocation(self.__env, dm) \\n   \\n   self.productions.used_rulenames = {} \\n\\n   &#39;&#39;&#39;\\n   这里是ProductionRules的初始化\\n   &#39;&#39;&#39;\\n   used_productions = productions.ProductionRules(self.productions, self.__buffers, decmem, self.model_parameters) \\n   \\n   chunks.Chunk._similarities = self.__similarities\\n   \\n   return simulation.Simulation(self.__env, realtime, trace, gui, self.__buffers, used_productions, initial_time, environment_process, **kwargs)\\n</code></pre>\\n<h2>本章小结</h2>\\n<p>本章开启了对于ACT-R结构上的详细解剖，笔者结合代码实现为大家讲解了ACT-R中的三个基础模块Chunk, Buffer, Production。\\n下一章节开始我们将剖析五大主体模块：<code>意图模块, 陈述性模块, 视觉模块, 手动模块, 生产模块</code>。其中，生产模块在本章介绍Production时已经略微提到。\\n这篇文章代码浓度可能比较高，如果阅读起来有点吃力的话可以先看一下ACT-R的综述性论文，笔者把链接放在这：</p>\\n<p><a href=\"https://link.juejin.cn?target=http%3A%2F%2Fact-r.psy.cmu.edu%2Fwordpress%2Fwp-content%2Fuploads%2F2012%2F12%2F526FSQUERY.pdf\" title=\"http://act-r.psy.cmu.edu/wordpress/wp-content/uploads/2012/12/526FSQUERY.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">act-r.psy.cmu.edu/wordpress/w…</a></p>\\n<p>希望文章能给大家带来收获，如果通过阅读本文有收获的话，还请<code>点赞关注收藏</code>一波，您的支持就是我的动力！我们下期再见！（假期应该能更新比较快把）</p>', 'author': 'MetaUniTech', 'reading_time': 73, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 26, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '779ffaf25743804ba6d4dc9360c3740ade96d8e67588b39d877b5cfa38195e0b', 'title': '《特征工程训练营》——自然语言处理：社交媒体情感分类', 'url': 'https://juejin.cn/post/7284196027149893672', 'published_at': '2023-10-02T05:24:51Z', 'created_at': '2023-10-04T14:11:40.315038Z', 'changed_at': '2023-10-04T14:11:40.315038Z', 'content': '<p>我们过去的两个案例研究专注于完全不同的领域，但有一个共同的重要组成部分：我们处理的是结构化表格数据。在接下来的两个案例研究中，我们将看到特殊情况，需要采用特定的特征工程技术来实现机器学习。在这个案例研究中，我们将探讨自然语言处理（NLP）领域的技术，这是机器学习的一个分支，专注于处理原始文本数据。</p>\\n<p>正如在前几章中讨论的那样，非结构化数据非常普遍，数据科学家通常需要在非结构化数据（如文本和图像）上执行机器学习任务。常见的NLP任务之一是执行文本分类或文本回归，其中的目标是仅基于原始文本执行分类或回归。</p>\\n<p>图5.1包含了三种不同的文本分类示例。第一个示例——“我喜欢这家餐厅！”——是常见的情感分析，其目标是预测文本是积极的还是消极的。第二个示例——LIMITED OFFER, BIG WINNER, OPEN NOW!!!11!!——是一个垃圾邮件分类任务，通常用于电子邮件主题行。最后一个示例展示了家庭自动化系统如何根据语音命令转换为文本来决定用户要求执行的任务。</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b3f4dcb2acbb47578e6f8cd645c41fa7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=720&amp;h=474&amp;s=31759&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在深入研究自然语言处理和该领域的特征工程技术之前，我们应该先了解一些基本术语。在本章中，我将把文档称为长度可变的文本片段。文档可以是电影评论、推文、研究论文——实际上可以是任何东西！文档是我们NLP模型的主要输入。当我们有一组文档时，我们称这个集合为语料库（图5.2）。</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2fba043965d24642a90d817ec34f60e5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=533&amp;h=448&amp;s=16959&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>有无限多的自然语言处理问题，因为作为人类，我们通过语言进行自然交流。我们期望我们的机器学习驱动系统能够解析我们的语言并根据需要执行任务。问题在于机器学习模型无法处理和学习来自长度可变的原始字符串。机器学习模型期望以观察到的形式接收具有固定长度特征向量的数据。为了进行任何形式的自然语言处理，我们需要将文本转换为特征向量的形式，如图5.3所示。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/580ad85e1a9f489c9e8337ee2f083460~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=905&amp;h=280&amp;s=26200&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>让我们来看一下我们案例研究的数据集。</p>\\n<p>警告：本章包含一些运行时间较长的代码示例，特别是在后面涉及到自动编码器和BERT时。请注意，一些代码示例可能在本书的最低要求上运行超过一小时。</p>\\n<h1>推文情感数据集</h1>\\n<p>我们在这个案例研究中使用的数据集来自Kaggle竞赛，名为Twitter美国航空公司情感（<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.kaggle.com%2Fcrowdflower%2Ftwitter-airline-sentiment\" title=\"https://www.kaggle.com/crowdflower/twitter-airline-sentiment\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.kaggle.com/crowdflower…</a>）。我们稍微修改了数据，以使不同情感类别更加平衡，如下所示。</p>\\n<pre><code>import pandas as pd                                           ❶\\nimport numpy as np                                            ❶\\ntweet_df = pd.read_csv(&#39;../data/cleaned_airline_tweets.csv&#39;)  ❷\\ntweet_df.head()  \\n</code></pre>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c23ad85143594752b1c62e494cc7ba35~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=674&amp;h=295&amp;s=67669&amp;e=png&amp;b=f9f9f9\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>和其他章节一样，我们将假设我们对预测模型几乎没有控制权。事实上，每当我们尝试新的特征工程技术时，我们都会将该技术与逻辑回归进行对比，并对单个逻辑回归参数进行网格搜索。提醒自己为什么要这样做，我们的目标是找到将文本表示为结构化数据的最佳方法，我们要确保如果我们的ML管道性能提高，这是由于我们的特征工程工作，而不是我们依赖ML模型的学习能力。</p>\\n<p>虽然我们的数据集没有太多可以探索的内容，但查看一下数据是个好主意，以了解文本列和响应标签。为此，让我们介绍一个名为pandas profiling的新包。pandas profiling包提供了一个报告，用于快速描述和探索数据，以加快ML工作的分析阶段。它可以为我们提供每列的描述，包括定量和定性数据，以及其他信息，如缺失数据的报告、文本长度的直方图等等。让我们看一下探查工具提供的报告，如下所示。</p>\\n<pre><code>from pandas_profiling import ProfileReport\\nprofile = ProfileReport(tweet_df, title=&#34;Tweets Report&#34;, explorative=True)\\n \\nprofile\\n</code></pre>\\n<p>运行此代码将生成我们数据的报告，其中包含一些关键信息。例如，在“Categories”选项卡中的“text”列的“Toggle Details”下，我们有一个文本长度的直方图，显示了文本长度大致呈正态分布，长度在140个字符左右有一个峰值（图5.5）。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ca14ad9410074225b83b6feaa7c8294f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=734&amp;h=511&amp;s=23752&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>我们还可以看到响应标签情感的分布。它显示了我们的数据在情感方面相当平衡，而且我们的空准确度（分类的基准指标，通过猜测最常见的类别可以达到的准确度）仅占数据的34.9%（图5.6）。这应该很容易超越。我们的目标是创建一个机器学习系统，可以接收一条推文并预测其中的三个类别之一。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2b2bbc15c16549a9ac768031029c0899~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=643&amp;s=55365&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>在继续之前，让我们将数据集分成训练集和测试集，这样我们可以使用它们来自信地比较我们的特征工程工作。</p>\\n<p>注意：我们将始终在训练集上训练我们的特征工程系统，然后将它们应用于测试集，就好像测试集是尚未见过的全新数据一样。</p>\\n<p>作为提醒，我们将按照几乎每个案例研究中的方式将数据拆分为80/20的训练/测试拆分。我们还将为可复现性设置一个random_state（以便获得相同的拆分），并在我们的类标签sentiment上进行分层，以便训练集和测试集具有与整体数据相同的类标签分布。让我们在以下代码中看到这一切。</p>\\n<pre><code>from sklearn.model_selection import train_test_split\\n \\ntrain, test = train_test_split(\\n    tweet_df, test_size=0.2, random_state=0,\\n    stratify=tweet_df[‘sentiment’]\\n)\\n \\nprint(f&#39;Count of tweets in training set: {train.shape[0]:,}&#39;)\\nprint(f&#39;Count of tweets in testing set: {test.shape[0]:,}&#39;)\\n \\nCount of tweets in training set: 3,088\\nCount of tweets in testing set: 772\\n</code></pre>\\n<p>现在我们有了训练集和测试集，现在是时候讨论如何通过一种称为向量化的过程将我们的文本转换为ML算法可以处理的形式了。</p>\\n<h2>问题陈述和定义成功</h2>\\n<p>我们在这里进行了另一项分类工作。我们的模型目标可以总结为以下问题：给定一条推文的文本，我们是否可以找到将文本表示为特征并准确分类推文情感的方法？</p>\\n<p>本案例研究的目标是找到不同的方法将我们的推文转化为可供机器理解的特征，并使用这些特征来训练一个模型。为了更好地确定哪些特征工程技术对我们最有帮助，我们将坚持使用简单的逻辑回归分类器。通过这样做，我们可以更有信心地认为我们管道性能的提升主要是由于我们的特征工程工作。</p>', 'author': '数据智能老司机', 'reading_time': 6, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 25, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'ffc41618ed82cd79997b4174b019bffc67384c93ff2239621d5aacd25fcb8061', 'title': '[深度学习|风格迁移|迁移学习]基础教程学习', 'url': 'https://juejin.cn/post/7284221961621422135', 'published_at': '2023-10-01T10:15:32Z', 'created_at': '2023-10-04T14:11:40.309394Z', 'changed_at': '2023-10-04T14:11:40.309394Z', 'content': '<p> </p>  <p>  </p> <h1>\\n            [深度学习|风格迁移|迁移学习]基础教程学习\\n             </h1> <a href=\"https://juejin.cn/user/2342385241820414/posts\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">\\n    proanimer\\n      </a> <time datetime=\"2023-10-01T10:15:32.000Z\">\\n                    2023-10-01\\n                  </time>  \\n                    1\\n                   \\n                    阅读1分钟\\n                    <p></p>     <p>将一个图像中的风格应用在另一图像之上，即<em>风格迁移</em>（style transfer）这里我们需要两张输入图像：一张是<em>内容图像</em>，另一张是<em>风格图像</em>。 我们将使用神经网络修改内容图像，使其在风格上接近风格图像。</p>\\n<h2>使用预训练模型进行迁移</h2>\\n<p><img src=\"https://zh.d2l.ai/_images/neural-style.svg\" alt=\"风格迁移\" loading=\"lazy\"/></p>\\n<p>首先，我们初始化合成图像，例如将其初始化为内容图像。</p>\\n<p>该合成图像是风格迁移过程中唯一需要更新的变量，即风格迁移所需迭代的模型参数。 然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。 这个深度卷积神经网络凭借多个层逐级抽取图像的特征，我们可以选择其中某些层的输出作为内容特征或风格特征。</p>\\n<p>接下来，我们通过前向传播（实线箭头方向）计算风格迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。 风格迁移常用的损失函数由3部分组成：</p>\\n<ol>\\n<li><em>内容损失</em>使合成图像与内容图像在内容特征上接近；</li>\\n<li><em>风格损失</em>使合成图像与风格图像在风格特征上接近；</li>\\n<li><em>全变分损失</em>则有助于减少合成图像中的噪点。</li>\\n</ol>\\n<p>最后，当模型训练结束时，我们输出风格迁移的模型参数，即得到最终的合成图像。</p>\\n<p>使用一个预训练模型,比如VGG提取内容与特征.</p>\\n<pre><code>pretrained_net = torchvision.models.vgg19(pretrained=True)\\n</code></pre>\\n<p>为了抽取图像的内容特征和风格特征，我们可以选择VGG网络中某些层的输出。</p>\\n<p>一般来说，<strong>越靠近输入层，越容易抽取图像的细节信息；反之，则越容易抽取图像的全局信息。</strong> <strong>为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，即<em>内容层</em>，来输出图像的内容特征</strong>。</p>\\n<p>我们从VGG中选择不同层的输出来匹配局部和全局的风格，这些图层也称为<em>风格层</em>。 VGG网络使用了5个卷积块。可以选择第四卷积块的最后一个卷积层作为内容层，选择每个卷积块的第一个卷积层作为风格层。</p>\\n<pre><code>style_layers, content_layers = [0, 5, 10, 19, 28], [25]\\nnet = nn.Sequential(*[pretrained_net.features[i] for i in\\n                      range(max(content_layers + style_layers) + 1)])\\n</code></pre>\\n<pre><code>def extract_features(X, content_layers, style_layers):\\n    contents = []\\n    styles = []\\n    for i in range(len(net)):\\n        X = net[i](X)\\n        \\n        if i in style_layers:\\n            styles.append(X)\\n        if i in content_layers:\\n            contents.append(X)\\n    return contents, styles\\n</code></pre>\\n<pre><code>def get_contents(image_shape, device):\\n    content_X = preprocess(content_img, image_shape).to(device)\\n    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\\n    return content_X, contents_Y\\n\\ndef get_styles(image_shape, device):\\n    style_X = preprocess(style_img, image_shape).to(device) \\n    _, styles_Y = extract_features(style_X, content_layers, style_layers)\\n    return style_X, styles_Y\\n</code></pre>\\n<p><code>get_contents</code>函数对内容图像抽取内容特征； <code>get_styles</code>函数对风格图像抽取风格特征。 因为在训练时无须改变预训练的VGG的模型参数，所以我们可以在训练开始之前就提取出内容特征和风格特征。</p>\\n<p>由于合成图像是风格迁移所需迭代的模型参数，我们只能在训练过程中通过调用<code>extract_features</code>函数来抽取合成图像的内容特征和风格特征.</p>\\n<pre><code>rgb_mean = torch.tensor([0.485, 0.456, 0.406])\\nrgb_std = torch.tensor([0.229, 0.224, 0.225])\\n\\ndef preprocess(img, image_shape):\\n    transforms = torchvision.transforms.Compose([\\n        torchvision.transforms.Resize(image_shape),\\n        torchvision.transforms.ToTensor(),\\n        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\\n    return transforms(img).unsqueeze(0)\\n\\ndef postprocess(img):\\n    img = img[0].to(rgb_std.device)\\n    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\\n    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))\\n</code></pre>\\n<p>preprocess与postprocess分别将PIL数据转为tensor,tensor转为PIL数据.</p>\\n<h3>损失函数</h3>\\n<p><strong>内容损失</strong></p>\\n<p>容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。 平方误差函数的两个输入均为<code>extract_features</code>函数计算所得到的内容层的输出</p>\\n<pre><code>def content_loss(Y_hat, Y):\\n    \\n    \\n    return torch.square(Y_hat - Y.detach()).mean()\\n</code></pre>\\n<p><strong>风格损失</strong></p>\\n<p>风格损失与内容损失类似，也通过平方误差函数衡量合成图像与风格图像在风格上的差异。 为了表达风格层输出的风格，我们先通过<code>extract_features</code>函数计算风格层的输出。 假设该输出的样本数为1，通道数为c，高和宽分别为h和w，我们可以将此输出转换为矩阵X，其有c行和hw列。 这个矩阵可以被看作由c个长度为hw的向量x1,…,xc组合而成的。其中向量xi代表了通道i上的风格特征。</p>\\n<pre><code>def gram(X):\\n    num_channels, n = X.shape[1], X.numel() // X.shape[1]\\n    X = X.reshape((num_channels, n))\\n    return torch.matmul(X, X.T) / (num_channels * n)\\n</code></pre>\\n<p>风格损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与风格图像的风格层输出。这里假设基于风格图像的格拉姆矩阵<code>gram_Y</code>已经预先计算好了</p>\\n<pre><code>def style_loss(Y_hat, gram_Y):\\n    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()\\n</code></pre>\\n<p><strong>全变分损失</strong></p>\\n<p>有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。 一种常见的去噪方法是<em>全变分去噪</em>（total variation denoising）： 假设xi,j表示坐标(i,j)处的像素值，降低全变分损失<code>∑i,j|xi,j−xi+1,j|+|xi,j−xi,j+1|</code></p>\\n<p>能够尽可能使邻近的像素值相似。</p>\\n<pre><code>def tv_loss(Y_hat):\\n    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())\\n</code></pre>\\n<p><strong>损失函数</strong></p>\\n<pre><code>content_weight, style_weight, tv_weight = 1, 1e3, 10\\ndef compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\\n    \\n    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(contents_Y_hat, contents_Y)]\\n    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(styles_Y_hat, styles_Y_gram)]\\n    tv_l = tv_loss(X) * tv_weight\\n    \\n    l = sum(10 * styles_l + contents_l + [tv_l])\\n    return contents_l, styles_l, tv_l, l\\n</code></pre>\\n<p>contents_l表示内容损失,</p>\\n<h3>初始化合成图像</h3>\\n<pre><code>def get_inits(X, device, lr, styles_Y):\\n    gen_img = SynthesizedImage(X.shape).to(device)\\n    gen_img.weight.data.copy_(X.data)\\n    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\\n    styles_Y_gram = [gram(Y) for Y in styles_Y]\\n    return gen_img(), styles_Y_gram, trainer \\n</code></pre>\\n<pre><code>class SynthesizedImage(nn.Module):\\n    def __init__(self, img_shape, **kwargs):\\n        super(SynthesizedImage, self).__init__(**kwargs)\\n        self.weight = nn.Parameter(torch.rand(*img_shape))\\n\\n    def forward(self):\\n        return self.weight\\n</code></pre>\\n<h3>训练模型</h3>\\n<pre><code>def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\\n    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y) \\n    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\\n    animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;,\\n                            xlim=[10, num_epochs],\\n                            legend=[&#39;content&#39;, &#39;style&#39;, &#39;TV&#39;],\\n                            ncols=2, figsize=(7, 2.5))\\n    for epoch in range(num_epochs):\\n        trainer.zero_grad()\\n        contents_Y_hat, styles_Y_hat = extract_features(\\n            X, content_layers, style_layers) \\n        contents_l, styles_l, tv_l, l = compute_loss(\\n            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\\n        l.backward()\\n        trainer.step()\\n        scheduler.step()\\n        if (epoch + 1) % 10 == 0:\\n            animator.axes[1].imshow(postprocess(X))\\n            animator.add(epoch + 1, [float(sum(contents_l)),\\n                                     float(sum(styles_l)), float(tv_l)])\\n    return X\\n</code></pre>\\n<pre><code>device, image_shape = d2l.try_gpu(), (300, 450)\\nnet = net.to(device)\\ncontent_X, contents_Y = get_contents(image_shape, device)\\n_, styles_Y = get_styles(image_shape, device) \\noutput = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50) \\n</code></pre>\\n<p>合成图像保留了内容图像的风景和物体，并同时迁移了风格图像的色彩。例如，合成图像具有与风格图像中一样的色彩块，其中一些甚至具有画笔笔触的细微纹理</p>\\n<h3>结果展示</h3>\\n<p><img src=\"https://img1.imgtp.com/2023/09/26/fqReGmQh.png\" alt=\"image-20230926225405384\" loading=\"lazy\"/></p>\\n<p><img src=\"https://img1.imgtp.com/2023/09/26/spmizDnH.png\" alt=\"image-20230926225430234\" loading=\"lazy\"/></p>\\n<p><img src=\"https://img1.imgtp.com/2023/09/26/RlUlfrPS.png\" alt=\"image-20230926225439855\" loading=\"lazy\"/></p>\\n<p>可以尝试改动style_weight,看看风格变换.比如style_weight增大,发现style loss太小了,而且会影响content loss</p>\\n<p><img src=\"https://img1.imgtp.com/2023/09/26/zHqMS03J.png\" alt=\"image-20230926231717608\" loading=\"lazy\"/></p>\\n<ul>\\n<li>风格迁移常用的损失函数由3部分组成：（1）<strong>内容损失使合成图像与内容图像在内容特征上接近</strong>；（2）<strong>风格损失令合成图像与风格图像在风格特征上接近</strong>；（3）<strong>全变分损失则有助于减少合成图像中的噪点</strong>。</li>\\n<li>我们<strong>可以通过预训练的卷积神经网络来抽取图像的特征</strong>，并通过最小化损失函数来不断更新合成图像来作为模型参数。</li>\\n<li>我们使用<strong>gram矩阵表达风格层输出的风格</strong></li>\\n</ul>\\n<h2>使用GAN进行风格迁移</h2>\\n<p>GANs是生成艺术图像的好方法。另一种有趣的技术是所谓的风格转换，它获取一个内容图像，然后用不同的风格重新绘制，从风格图像中应用过滤器。</p>\\n<p>工作方式如下:</p>\\n<p>我们从随机噪声图像开始（或从内容图像开始，但为了理解起见，从随机噪声开始更容易）</p>\\n<p>我们的目标是创建这样一个图像，<strong>它将接近内容图像和风格图像</strong>。这将由两个损失函数确定：基于CNN在<strong>当前图像和内容图像的某些层提取的特征来计算内容损失</strong>,<strong>使用Gram矩阵巧妙地计算当前图像和风格图像之间的风格损失</strong></p>\\n<p>为了使图像更平滑并去除噪声，我们还引入了全变分损失，它计算相邻像素之间的平均距离</p>\\n<p>优化方式使用梯度下降（或一些其他优化算法）调整当前图像，以最小化总损失，总损失是所有三个损失的加权和。</p>\\n<p>在代码上与之前的差别是使用高斯分布采样得到的噪音作为需要优化的参数,</p>\\n<pre><code>img_style = load_image(&#39;images/style.jpg&#39;)\\nimg_content = load_image(&#39;images/image.jpg&#39;)\\n\\nimg_result = np.random.uniform(size=(img_size,img_size,3))\\nvgg = tf.keras.applications.VGG16(include_top=False, weights=&#39;imagenet&#39;)\\nvgg.trainable = False\\ndef layer_extractor(layers):\\n    outputs = [vgg.get_layer(x).output for x in layers]\\n    model = tf.keras.Model([vgg.input],outputs)\\n    return model \\ncontent_layers = [&#39;block4_conv2&#39;] \\ncontent_extractor = layer_extractor(content_layers)\\n\\ncontent_target = content_extractor(preprocess_input(tf.expand_dims(img_content,axis=0)))\\n\\ndef content_loss(img):\\n    z = content_extractor(preprocess_input(tf.expand_dims(255*img,axis=0))) \\n    return 0.5*tf.reduce_sum((z-content_target)**2)\\n\\ndef gram_matrix(x):\\n  result = tf.linalg.einsum(&#39;bijc,bijd-&gt;bcd&#39;, x, x)\\n  input_shape = tf.shape(x)\\n  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\\n  return result/(num_locations)\\n\\nstyle_layers = [&#39;block1_conv1&#39;,&#39;block2_conv1&#39;,&#39;block3_conv1&#39;,&#39;block4_conv1&#39;]\\n\\ndef style_extractor(img):\\n    return [gram_matrix(x) for x in layer_extractor(style_layers)(img)]\\n\\nstyle_target = style_extractor(preprocess_input(tf.expand_dims(img_style,axis=0)))\\n\\ndef style_loss(img):\\n    z = style_extractor(preprocess_input(tf.expand_dims(255*img,axis=0)))\\n    loss = tf.add_n([tf.reduce_mean((x-target)**2) \\n                           for x,target in zip(z,style_target)])\\n    return loss / len(style_layers)\\ndef variation_loss(img):\\n  img = tf.cast(img,tf.float32)\\n  x_var = img[ :, 1:, :] - img[ :, :-1, :]\\n  y_var = img[ 1:, :, :] - img[ :-1, :, :]\\n  return tf.reduce_sum(tf.abs(x_var)) + tf.reduce_sum(tf.abs(y_var))\\n  \\ndef total_loss_var(img):\\n    return content_loss(img)+150*style_loss(img)+30*variation_loss(img)\\n\\nimg.assign(clip(np.random.normal(-0.3,0.3,size=img_content.shape)+img_content/255.0))\\n\\ntrain(img,loss_fn=total_loss_var)\\n</code></pre>\\n<p>注意,提取风格的层数一般选择每个特征块的前面几层,而提取内容的层数一般选择特征块的后面几块.</p>\\n<p>风格迁移与迁移学习存在不可区分的关系,因为我们将一些知识从一个神经网络模型转移到另一个。在迁移学习中，我们通常从预先训练的模型开始，该模型已经在一些大型图像数据集（如ImageNet）上进行了训练。</p>\\n<p>预训练模型比如:</p>\\n<ul>\\n<li>VGG-16/VGG-19，它们是相对简单的模型，仍然提供良好的精度。经常将VGG作为第一次尝试是了解迁移学习如何运作的好选择。</li>\\n<li>ResNet是微软研究院于2015年提出的一系列模型。它们有更多的层，因此占用更多的资源。</li>\\n<li>MobileNet是一系列尺寸较小的型号，适用于移动设备。如果你缺乏资源，并且可能会牺牲一点准确性，那么就使用它们。</li>\\n</ul>\\n<p>使用pytorch加载预训练模型.</p>\\n<pre><code>vgg = torchvision.models.vgg16(pretrained=True)\\nvgg\\n</code></pre>\\n<p>结构如下,可以看见有<code>features</code>,<code>avgpool</code>以及<code>classifier</code></p>\\n<pre><code>VGG(\\n  (features): Sequential(\\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (1): ReLU(inplace=True)\\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (3): ReLU(inplace=True)\\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (6): ReLU(inplace=True)\\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (8): ReLU(inplace=True)\\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (11): ReLU(inplace=True)\\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (13): ReLU(inplace=True)\\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (15): ReLU(inplace=True)\\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (18): ReLU(inplace=True)\\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (20): ReLU(inplace=True)\\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (22): ReLU(inplace=True)\\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (25): ReLU(inplace=True)\\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (27): ReLU(inplace=True)\\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n    (29): ReLU(inplace=True)\\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  )\\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\\n  (classifier): Sequential(\\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\\n    (1): ReLU(inplace=True)\\n    (2): Dropout(p=0.5, inplace=False)\\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\\n    (4): ReLU(inplace=True)\\n    (5): Dropout(p=0.5, inplace=False)\\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\\n  )\\n)\\n</code></pre>\\n<p><strong>查看每一层后的结果</strong></p>\\n<p>利用<code>torchinfo</code>库查看</p>\\n<pre><code>summary(vgg,input_size=(1,3,224,224))\\n</code></pre>\\n<img src=\"https://img1.imgtp.com/2023/10/01/MiSavdBT.png\" alt=\"image-20231001161905086\" loading=\"lazy\"/>\\n<p>可以看到输入一张3通道224的图像特征层输出是512通道的宽高为7的特征</p>\\n<p><strong>提取图像特征</strong></p>\\n<pre><code>res = vgg.features(sample_image).cpu()\\nplt.figure(figsize=(15,3))\\nplt.imshow(res.detach().view(512,-1).T)\\nprint(res.size())\\n</code></pre>\\n<p>利用feature层提取特征,然后利用预训练模型提取的特征,直接拿一个简单的Linear层作为分类器进行训练,比如:</p>\\n<pre><code>vgg_dataset = torch.utils.data.TensorDataset(feature_tensor,label_tensor.to(torch.long))\\ntrain_ds, test_ds = torch.utils.data.random_split(vgg_dataset,[700,100])\\ntrain_loader = torch.utils.data.DataLoader(train_ds,batch_size=32)\\ntest_loader = torch.utils.data.DataLoader(test_ds,batch_size=32)\\n\\nnet = torch.nn.Sequential(torch.nn.Linear(512*7*7,2),torch.nn.LogSoftmax()).to(device)\\n\\nhistory = train(net,train_loader,test_loader)\\n</code></pre>\\n<p>net就是简单的线性层加一个激活函数</p>\\n<p><strong>常用的加载数据流程</strong></p>\\n<pre><code>std_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\\n                          std=[0.229, 0.224, 0.225])\\ntrans = transforms.Compose([\\n        transforms.Resize(256),\\n        transforms.CenterCrop(224),\\n        transforms.ToTensor(), \\n        std_normalize])\\ndataset = torchvision.datasets.ImageFolder(&#39;data/PetImages&#39;,transform=trans)\\ntrainset, testset = torch.utils.data.random_split(dataset,[20000,len(dataset)-20000])\\n</code></pre>\\n<p>如果图像在一个文件夹中,利用<code>ImageFolder</code>得到dataset.然后使用<code>dl = torch.utils.data.DataLoader(dataset,batch_size=bs,shuffle=True)</code>用于循环每个batch处理.</p>\\n<p>可以在训练过程中使用原始VGG-16网络作为一个整体来避免手动预计算特征.如下</p>\\n<ul>\\n<li><strong>将最终分类器替换为将产生所需数量的类的分类</strong>器。</li>\\n<li><strong>冻结卷积特征提取器的权重，使得它们不被训练</strong>。建议最初进行这种冻结，因为否则未经训练的分类器层可能会破坏卷积提取器的原始预训练权重。冻结权重可以通过将所有参数的requires_grad属性设置为False来实现</li>\\n</ul>\\n<pre><code>for x in vgg.features.parameters():\\n    x.requires_grad = False\\n</code></pre>\\n<p>如果您的对象在视觉上与普通的ImageNet图像不同，则这种功能组合可能无法发挥最佳效果。因此，开始训练卷积层也是有意义的。 为此，我们可以解冻之前冻结的卷积滤波器参数。不过一般都会采用一些微调方法,比如LoRA等.</p>\\n<h3>其他方向</h3>\\n<p>domain knowledge,domain adaption或者是transfer learning,本质上都是像提取一些特征,这种特征能在多个domain上使用.我们可以考虑利用这种特征进行可视化或者对抗攻击等.</p>\\n<p>比如利用预训练模型作为分类器,尝试从下从正态分布采样得到噪声图,然后作为输入,优化这个输入使其被分类为想要的分类.这样的图像虽然被正确分类了但人眼还是能明显看出差别.</p>\\n<p><img src=\"https://img1.imgtp.com/2023/10/01/Mu9sCSMV.png\" alt=\"image-20231001174507317\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>这种噪音对我们来说没有多大意义，但很可能它包含了很多需要的类别(比如猫)特有的低级别过滤器。然而，由于有很多方法可以优化输入以获得理想的结果，因此优化算法没有动机找到视觉上可理解的模式.</p>\\n<p>为了让它看起来不那么像噪音，我们可以在损失函数中引入一个附加项——变化损失。它测量图像的相邻像素的相似程度。如果我们将这个项添加到损失函数中，它将迫使优化器找到噪声较小的解决方案，从而具有更多可识别的细节</p>\\n</blockquote>\\n<pre><code>def total_loss(target,res):\\n    return 10*tf.reduce_mean(keras.metrics.sparse_categorical_crossentropy(target,res)) + \\\\\\n           0.005*tf.image.total_variation(x,res)\\n\\noptimize(x,target,loss_fn=total_loss)\\n</code></pre>\\n<p>也就是分类的损失加上全变分损失.全变分损失目的是减小噪声,得到图像图下</p>\\n<p><img src=\"https://img1.imgtp.com/2023/10/01/v3ypktNG.png\" alt=\"image-20231001174926326\" loading=\"lazy\"/></p>\\n<p>对抗攻击就利用一张本身是狗分类也确实是狗的图片,对这张图片进行优化</p>\\n<pre><code>x = tf.Variable(np.expand_dims(img,axis=0).astype(np.float32)/255.0)\\noptimize(x,target,epochs=100)\\n</code></pre>\\n<p><img src=\"https://img1.imgtp.com/2023/10/01/xcacfJg1.png\" alt=\"image-20231001175428790\" loading=\"lazy\"/></p>\\n<p>在pytorch中使用<code>autograd</code>计算梯度.</p>\\n<p>最后推荐一下微软的Ai for beginners的课程,质量比较高,此外还有李沐的d2l,台湾李宏毅老师的深度学习课程以及fast.ai课程,都是比较好的.</p>\\n<blockquote>\\n<p>我们能够在预先训练的CNN中可视化猫（以及任何其他物体）的理想图像，<strong>使用梯度下降优化来调整输入图像而不是权重</strong>。获得有意义的图像的主要技巧是使用<strong>变化损失作为额外的损失函数</strong>，这会<strong>使图像看起来更平滑</strong>。</p>\\n</blockquote>\\n<h3>参考资料</h3>\\n<ol>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzh.d2l.ai%2Fchapter_computer-vision%2Fneural-style.html\" title=\"https://zh.d2l.ai/chapter_computer-vision/neural-style.html\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">13.12. 风格迁移 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fmicrosoft%2FAI-For-Beginners%2Ftree%2Fmain%2Flessons%2F4-ComputerVision%2F10-GANs\" title=\"https://github.com/microsoft/AI-For-Beginners/tree/main/lessons/4-ComputerVision/10-GANs\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">AI-For-Beginners/lessons/4-ComputerVision/10-GANs at main · microsoft/AI-For-Beginners (github.com)</a></li>\\n<li>[<a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fabs%2F1508.06576\" title=\"https://arxiv.org/abs/1508.06576\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">1508.06576] A Neural Algorithm of Artistic Style (arxiv.org)</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fmicrosoft%2FAI-For-Beginners%2Fblob%2Fmain%2Flessons%2F4-ComputerVision%2F08-TransferLearning%2FREADME.md\" title=\"https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/08-TransferLearning/README.md\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">AI-For-Beginners/lessons/4-ComputerVision/08-TransferLearning/README.md at main · microsoft/AI-For-Beginners (github.com)</a></li>\\n</ol>\\n<p>本文由<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fmdnice.com%2F%3Fplatform%3D2\" title=\"https://mdnice.com/?platform=2\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">mdnice</a>多平台发布</p>', 'author': 'proanimer', 'reading_time': 29, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 24, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '71ff62d51a3068cddf8a83fe99c5c151f068c9e330a9f3b6c9e4ad906a94ae7d', 'title': 'ChatGPT架构师：语言大模型的多模态能力、幻觉与研究经验', 'url': 'https://juejin.cn/post/7284185214392827965', 'published_at': '2023-10-01T09:04:17Z', 'created_at': '2023-10-04T14:11:40.301234Z', 'changed_at': '2023-10-04T14:11:40.301234Z', 'content': '<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/221cf97ed8ea4094b0bb0ed8a97c72d3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=366&amp;h=240&amp;s=133998&amp;e=png&amp;b=242423\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><strong>来源 | The Robot Brains Podcast</strong></p>\\n<p><strong>OneFlow编译</strong></p>\\n<p><strong>翻译｜宛子琳、杨婷</strong></p>\\n<p>9月26日，OpenAI宣布ChatGPT新增了图片识别和语音能力，使得ChatGPT不仅可以进行文字交流，还可以给它展示图片并进行互动，这是一次ChatGPT向多模态进化的重大升级。</p>\\n<p>OpenAI联合创始人，ChatGPT架构师John Schulman此前认为，添加多模态功能会给大模型带来极大的性能提升。“如果扩展出现边际收益递减，那么添加多模态就能让模型获得文本中无法获得的知识，并有可能掌握纯语言模型无法完成的任务。例如，通过观看与物理世界甚至是与电脑屏幕互动的视频，模型能获得巨大收益。”</p>\\n<p>在负责ChatGPT之前，Schulman是深度强化学习的早期先驱之一，他发明了广泛应用的近端策略优化算法（PPO），这实际上也是ChatGPT训练的一部分。他还发明了信任区域策略优化（TRPO），对OpenAI Gym、OpenAI Benchmark以及现代深度学习时代的许多元学习算法作出了重要贡献。</p>\\n<p>在创立OpenAI之前，加入OpenAI之前，Schulman在加州大学伯克利分校攻读博士学位，一开始主要研究机器人技术，随着深度学习兴起，转而研究强化学习，其导师正是强化学习领域的领军人物Pieter Abbeel。</p>\\n<p>近期，John Schulman与Pieter Abbeel就ChatGPT的构建过程及方法、能力、局限等模型细节展开了详细探讨，并展望了语言大模型向多模态模型进化的发展方向。此外，Schulman还分享了自己一路走来的研究历程与经验。</p>\\n<p>（以下内容经授权后由OneFlow编译发布，转载请联系授权。视频：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DnM_3d37lmcM%25EF%25BC%2589\" title=\"https://www.youtube.com/watch?v=nM_3d37lmcM%EF%BC%89\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">www.youtube.com/watch?v=nM_…</a></p>\\n<p><strong>1</strong></p>\\n<h1>ChatGPT的构建</h1>\\n<p><strong>Pieter Abbeel：ChatGPT给我留下了深刻印象。我曾向ChatGPT输入过一段关于某公司的演讲，然后要求ChatGPT用一段话来描述该公司，完成以后，我进一步要求ChatGPT以Snoop Dogg（西岸说唱巨星Snoop Lion（原名Snoop Dogg））风格来描述该公司。尽管Snoop Dogg从来没有写过关于这家公司的说唱内容，但ChatGPT很好地完成了任务。ChatGPT的能力让我震惊，我不禁想知道这种模型是如何构建的。</strong></p>\\n<p><strong>John Schulman：</strong> 训练ChatGPT有以下几个步骤。首先，从预训练的语言模型开始，训练模型模仿大量由人类撰写的文本。我们希望模型能够像人类一样使用语言，为此需要大量文本数据，我们从互联网上搜集到了大量文本，并训练模型生成类似文本。在训练过程中，模型根据之前的文本预测下一个词，通过大量训练得到一个预训练的语言模型，但它只能生成类似于互联网上的随机文本，其中有些符合要求，有些则不符合。为了训练模型生成更加一致且专业的内容，我们需要对模型进行微调。</p>\\n<p>微调有两个步骤，第一步是监督学习，在这一步，我们会使用雇佣人员撰写的高质量回应来训练模型。第二步是强化学习，以进一步改进模型。在这一步，我们训练了一个奖励模型来识别好的回应，并使用该模型进行强化学习训练。</p>\\n<p><strong>Pieter Abbeel：用奖励模型进行强化学习也就是通过奖励评估机器人生成的文本，并试图将获得的奖励最大化。那么是否有可能生成一种文本生成能力超越人类的聊天机器人呢？</strong></p>\\n<p><strong>John Schulman：</strong> 当然可以。在某些方面，模型已经具备了超越人类的能力，但在其他方面可能还有待提升。例如，如果以速度作为能力的衡量标准，这些模型在撰写诗歌或创作Snoop Dogg风格的歌词等方面比人类快得多。总之，我们不能用单一的指标来衡量模型的智能程度，它们在某些方面已经超越了人类，比如知识的广度以及写作风格的多样性等，但模型在其他方面的能力仍有待提升。</p>\\n<p><strong>Pieter Abbeel：模型在哪些方面还有待提升？</strong></p>\\n<p><strong>John Schulman：</strong> 虽然模型在某些任务上表现出色，但在数学推理等任务上，它们的表现远不及熟练的人类。例如在需要长时间运算的任务中，聊天机器人往往难以有效执行任务，即使输入的提示很详细谨慎，并告知模型可以采取多个步骤，它们也经常会在运算过程中陷入困境，无法顺利完成任务。</p>\\n<p><strong>Pieter Abbeel：说实话，人类同样需要一段时间才能熟练掌握数学，大多数人无法真正达到顶尖数学家的水平。那么，目前模型在数学等方面的应用是否只是开始，它们的表现是否会逐步提升？</strong></p>\\n<p><strong>John Schulman：</strong> 模型在数学方面的表现会越来越好。目前我们还难以确定模型所面临的根本限制，只知道确实有一些限制因素的存在，例如没有足够的执行机制，只能生成文本，无法实际执行任务等。但这些都是可以克服的表层限制，不是关键所在。当前的模型还不够智能，无法进行高质量的创造性思考，也无法进行数学和科学等方面的工作。实现这些目标还需要一定时间，目前，我们还不确定具体需要多长时间以及模型的具体发展方向。</p>\\n<p><strong>Pieter Abbeel：语言模型早就出现了，但直到ChatGPT爆火，语言模型才真正走进大众的视野，人们与ChatGPT之间的互动热度是之前的语言模型无法企及的。你在开发ChatGPT及其前身InstructGPT，并在其中引入强化学习组件时，是否预料过会出现这种情况？</strong></p>\\n<p><strong>John Schulman：</strong> 我确实认为ChatGPT的聊天界面比之前的模型更便捷易用，具有更大潜力，然而，我并没有预料到它会如此受欢迎。最初，我认为ChatGPT可能只适用于某些特定领域，可以使用InstructGPT来制作聊天机器人，我们只需向InstructGPT输入正确的提示，然后它就会像聊天机器人一样运行，并且还能得到不错的结果。</p>\\n<p>最初发布时，ChatGPT的运行结果和InstructGPT类似，但ChatGPT更具自我意识，能理解自身局限，幻觉更少。Instruct模型主要用于生成连续文本和进行写作测试，其中具有一定的幻觉成分，这是InstructGPT的特点之一。在某种程度上，ChatGPT可能比之前的模型略好，但并没有显著提升。因此，我也没有意料到它会如此成功。</p>\\n<p><strong>2</strong></p>\\n<h1>幻觉成因及解决方法</h1>\\n<p><strong>Pieter Abbeel：你提及ChatGPT可能会产生幻觉，那么幻觉到底是什么呢？它的成因是什么？如何避免模型出现幻觉？</strong></p>\\n<p><strong>John Schulman：</strong> 幻觉是指模型会编造并输出一些看似合理的文本，这些文本可能包含虚构的内容、数字或引用。我们可以这样理解模型的幻觉现象：模型具有一定程度的能动性，它更关注表达的正确或以合适的风格书写并输出内容，准确性则次之。</p>\\n<p>考虑到最大似然性目标，模型的这种选择是显而易见的，在这种情况下，模型不会太关注输出内容的正确与否，而是更看重听起来正确或看起来合理，因此，以简单方式训练出的模型常常会产生幻觉。通过微调和人类反馈，我们可以显著减少幻觉的输出，但无法完全消除。免费模型带有较多幻觉，基于GPT-4模型的幻觉输出较少，但仍偶尔出现，特别是当涉及模型未经训练、未察觉到的特定限制时。</p>\\n<p><strong>Pieter Abbeel：“模型未经训练，未察觉到的特定限制”是不是意味着我们可以训练模型识别这些限制？</strong></p>\\n<p><strong>John Schulman：</strong> 我们可以训练模型，让模型意识到特定限制的存在。例如，早期模型对自身的能力一无所知，当你问它能否给某人发送电子邮件时，它可能会回答“是的，我刚刚发送了那封电子邮件”，然后，我们再针对这种特定类型的查询继续训练，让它学会回答“不，我无法发送电子邮件”。也就是说，我们可以采用分阶段的过程，教授模型一些它无法做到的特定限制，模型会对此进行泛化。</p>\\n<p>GPT-4是一个非常智能的模型，它的泛化能力很强，如果你告诉它一些不能完成的事，它通常能就此推断出很多其他超出自己能力的事。但这种方法并不完美，例如在引用方面，模型掌握了大量特定书籍和重要的论文知识，如果要求模型提供这方面的引用，有时它能给出有用的正确答案。所以在写作过程中，我们更倾向于让模型提供答案。</p>\\n<p>因此，模型会认为自己在某种程度上能够提供引用，但它对于这些引用的自信程度并没有很好的内部感知，这就导致模型有时会编造引用。模型也许知道自己编造了引用内容，也许不知道，所以，有时你可以问它对此是否确定。<strong>我们还不太清楚模型的这些能力以及模型对自身限制的感知是如何泛化的，这是一个很有意思的研究课题</strong>。</p>\\n<p><strong>Pieter Abbeel：关于引用，在模型事先阅读过整个互联网内容的情况下，与其让模型提供引文，不如让模型在实时检索中获取相关信息。你是如何权衡检索和训练模型权重这两种方法的呢？</strong></p>\\n<p><strong>John Schulman：</strong> 两种方法都有其适用场景，<strong>目前，我们正在将两种方法进行结合</strong>。我们在ChatGPT中使用了一种浏览模型，它可以从网上查找信息（尽管最近我们暂时关闭了这一功能，但后续会恢复）。我认为，模型可以在权重中存储大量信息，包括非常详细的事实知识。如果将信息存储在权重中，模型就可以灵活使用这些信息，例如利用这些信息建立事物之间的联系，这时就算你提出一个与某件事模糊相关的问题，模型也可能会成功建立在搜索查询中难以建立的联系。因此，将信息存储在权重中可以让模型更加智能、灵活。</p>\\n<p>检索也有其自身的重要优势。首先，检索可以实时访问信息，相比将信息储存在权重中，检索可以提供更多细节。其次，检索输出更易于人工验证。在模型训练和用例测试阶段，保证模型输出的可检测性非常重要。因为模型的知识范围非常广泛，人们对模型输出结果进行评分时可能面临评分人员对该主题缺乏了解的情况，这会导致评分困难。因此，如果模型能够提供引用来源，这将大大提升监督评估的准确性。因为模型有时会输出幻觉，所以对于终端用户而言，验证语言模型的输出对于处理幻觉大有助益。</p>\\n<p><strong>Pieter Abbeel：大多数语言大模型只进行单次迭代训练（single Epoch training），也就是只对数据进行一次训练。模型是如何只进行一次遍历就记住这些具体的信息？在进行一次遍历时如何通过仅进行一次梯度更新来记住特定引文，并将其存储下来？</strong></p>\\n<p><strong>John Schulman：</strong> 语言模型对预训练数据信息的吸收程度确实让人惊讶。很多既定事实往往会多次出现在互联网的不同文档中，如果某一事实只出现在一个文档中，那么模型可能无法回想起这个事实（至少当前模型是这样的）。关于模型需要看到特定事实多少次，才能真正记住它并内化，目前还没有确切答案，可能需要几十次。</p>\\n<p><strong>3</strong></p>\\n<h1>探索LLM能力的新方法</h1>\\n<p><strong>Pieter Abbeel：显然，这些模型是在大量数据上进行训练的。最近，Sam Altman提到，随着模型规模不断扩大，继续增加训练数据、扩大模型规模已经难以继续提升模型性能，这意味着数据和规模能带来的机会不多了。对此你怎么看？</strong></p>\\n<p><strong>John Schulman：</strong> 确实，现有数据和模型规模扩展方法能带来的性能提升可能在一段时间后达到极限，算法、数据集、数据集大小以及算力能带来的提升将逐渐递减，但我们距离这种情况的出现还有一段时间，在此之前能做的还有很多。</p>\\n<p><strong>Pieter Abbeel：在未来，我们是否会继续依赖人类反馈来筛选文本数据？是否需要引入其他元素，比如通过让模型观看视频以更好地理解物理世界，或者让模型使用模拟器来体验物理世界的感觉？引入新元素是否可以将模型推向更高水平？相对于增加更多现有类型的数据，这些新元素的引入是否与模型的性能提升不太相关？</strong></p>\\n<p><strong>John Schulman：</strong> 我认为，添加多模态功能会带来极大的性能提升。如果扩展出现边际收益递减，那么添加多模态就能让模型获得文本中无法获得的知识，并有可能掌握纯语言模型无法完成的任务。例如，通过观看与物理世界甚至是与电脑屏幕互动的视频，模型能获得巨大收益。所有软件都是为人类设计的，如果模型能够观察像素并理解视频，我们就可以使用各种现有软件或帮助人们使用这些软件。为模型赋予新能力，让模型与新事物互动，将大大增强模型的实际能力。</p>\\n<p>此外，通过更加智能微调，纯语言模型还能完成很多任务。基于人类反馈的强化学习也还有很多提升空间，与其依赖于在人类数据上训练的奖励模型，不如利用模型自身来进行评估。</p>\\n<p><strong>Pieter Abbeel：这让我想起了生成对抗网络（GAN）：生成对抗网络有两个模型，一个用于训练，另一个用于生成更真实的任务。但在原始GAN中，模型通常生成的是图像，这听起来与微调非常相似，但微调可能会降低模型的泛化能力和展示知识的广度。这种情况是否属实？如果属实，这一过程是如何发生的？</strong></p>\\n<p><strong>John Schulman：</strong> 这种情况的确存在。对模型进行微调时，会减少其输出风格和内容类型的多样性，导致模式崩溃（mode collapse）或熵崩溃（entropy collapse），在某些情况下，模型会输出一组非常狭窄或单一的答案。</p>\\n<p>如果让语言模型，比如ChatGPT讲笑话，模型很可能会不断输出同一个笑话，比如“为什么科学家不相信原子？因为原子“编造”了一切（原文为“because atoms make up everything”，其中“make up”既有“构成”又有“编造”、“虚构”之意）”等老掉牙的笑话，或者一些类似的无趣笑话，模型会一直沿用下去。总之，这种模式崩溃效应确实会出现。</p>\\n<p><strong>Pieter Abbeel：在进行微调时，模型的能力可能也会有所下降。</strong></p>\\n<p><strong>John Schulman：</strong> 这是因为在预训练时使用了更大批次的数据，同时要确保在各种类型的大量输入中保留模型的全部能力。然而，在微调时，你只能看到一个小得多的数据集，因此可能丢失了一些在微调数据集中未被充分呈现的能力。同时，微调过程中可能会出现更多噪音，受噪音影响，模型性能可能会略微下降。</p>\\n<p>因此，微调的确会导致模型性能的略微下降。我们对模型进行了各种基准测试，并与预训练的基础模型进行了比较，尽量抑制模型能力的下降。不过可以肯定的是，在最新配置下，模型的性能下降并不严重。</p>\\n<p><strong>4</strong></p>\\n<h1>闭源还是开源？</h1>\\n<p><strong>Pieter Abbeel：目前在AI领域，除ChatGPT外，还有很多包括开源模型在内的其他模型。你如何看待开源和闭源模型？</strong></p>\\n<p><strong>John Schulman：</strong> \\xa0开源模型对于学术研究价值非凡，学术研究者可以对模型进行微调、更改架构，还可以通过人类反馈改进强化学习等，强大的开源模型使这类研究成为可能。目前，闭源模型在技术上则更具优势，是目前已知最好的模型。显然，在没有商业因素刺激且非闭源的情况下，我们很难激励人们开发出真正优质的模型，因此，我预计性能最佳的模型会是闭源的。但开源模型更有利于学术研究等活动，也可能会出现很多优秀的商业用例，因为人们能根据自己的数据对模型微调，这是当前商业模型供应商无法提供的。</p>\\n<p><strong>Pieter Abbeel：如果要构建开源模型，获取高质量的数据和大量计算资源可能非常困难？</strong></p>\\n<p><strong>John Schulman：</strong> 没错。开源模型无法获利，因此难以获得大量投资。所以，尽管开源模型具有极大的公益价值，但它不会是SOTA模型。另外，开源还面临着一些安全方面的顾虑，例如有人利用开源模型进行超大规模的垃圾信息传播，这种行为是API供应商所不允许的。鉴于上述问题，Meta等开源模型公司需要慎重对待开源问题，但开源模型未来的具体发展走向还有待观察。</p>\\n<p><strong>5</strong></p>\\n<h1>LLM主导地位及前景</h1>\\n<p><strong>Pieter Abbeel：现在，语言模型在对话中已经占据了主导地位，与其他领域相比，语言模型取得了多次重大突破，其快速增长的能力引发了广泛讨论。未来，其他领域是否会出现类似的重大飞跃？如果有，会出现在什么领域？</strong></p>\\n<p><strong>John Schulman：</strong> 我无法准确预测什么领域会出现重大突破。但我认为，类似于语言大模型的核心技术将成为许多领域的基石，从而在不同模态间构建关联，例如将语言大模型与视觉、视频等结合，从而实现更多功能。这正是语言模型的巨大优势，因为语言的信息密度相比其他类型的数据（例如视频）要高，包含的噪音较少。因此，未来相当长的一段时间内，语言都会是一种良好媒介，能够在有限的计算资源下承载大量智能。</p>\\n<p>然而我认为，语言与其他模态之间存在着相互渗透的趋势。我们可能会看到人们将语言与视频等多种模态相融合，甚至可能将语言与机器人技术相结合。我预测，未来的机器人技术将采用多模态模型，这些模型将在语言、视频和控制等方面进行联合训练。我相信这种类型的结合将成为未来人工智能领域的重要发展趋势之一。</p>\\n<p>还有一些领域，虽然与当前训练的模型类型无关，但可扩展的超级监督或提高监督质量在这些领域却更具重要性。其中关键在于，如何收集某些领域的数据，尤其是那些人类难以生成高质量标签或范例的领域。</p>\\n<p>那么，我们如何将人类与机器结合，从而创造出高于人类自身产出质量的数据，并在真正存在困难的领域实现对模型的监督？这是一个有趣且重要的问题，很可能会在机器学习研究领域获得更多关注。</p>\\n<p><strong>Pieter Abbeel：我脑海中浮现的例子是，一款能进行高效科学研究的AI ，它能够阅读人类难以解读的生物数据，如蛋白质序列、RNA和DNA序列，然后分析实验结果，以某种方式将它们结合起来提出新的假设，甚至得出人类难以得出的结论。</strong></p>\\n<p><strong>John Schulman：</strong> 没错。这是一个令人兴奋的研究领域，也许有一些类似于生物学的研究领域对人类来说过于复杂，但借助人工智能，即便它们并不比人类聪明，也能更高效地完成工作。因此，我们也许可以让模型在生物学中筛选大量复杂数据，并从中得出有价值的结论。</p>\\n<p><strong>6</strong></p>\\n<h1>研究生涯与心得</h1>\\n<p><strong>Pieter Abbeel：我对你的职业发展轨迹非常感兴趣，最初在我的实验室中，你主要研究模仿学习和机器人领域，后来你意识到了强化学习的发展潜力，于是你在OpenAI花了大量时间对其进行研究，并将其引入到语言模型领域。那么，从现在回溯过去，你是从何时开始关注语言模型的？原因是什么？</strong></p>\\n<p><strong>John Schulman：</strong> 大约在GPT-2时期，当时人们逐渐意识到了语言模型的巨大潜力，认为这一领域值得关注。实际上，那时我并没有立即将重心转向语言模型。我当时认为，无监督学习在一定程度上已经开始取得成效，而训练生成模型是创建具备通用能力模型的非常有效的方法，这样的通用模型可以通过微调用于各种下游任务。</p>\\n<p>当时，我对样本效率和强化学习非常感兴趣，即研究模型学习新任务的速度，这个问题在某种程度上是强化学习的核心，甚至可以说是人工智能的核心难题之一。</p>\\n<p>GPT-2发布后，它在许多任务上表现出色，比如通过少量示例理解上下文含义，同时微调也取得了许多良好结果，能够完成各种任务，如自然语言基准测试。总之，我当时认为，我们应该将强化学习领域的模型训练应用于游戏和机器人领域。</p>\\n<p>也许我们应该训练视频模型，然后通过强化学习对其进行微调，因此，我开始投入相关工作，虽然取得了一些成效，但在GPT-3时代并没有取得足够令人兴奋的成果。GPT-3的表现更令我惊讶，于是我决定将自己的工作和团队的工作重心转向强化学习。尽管我们不是OpenAI第一个自行开发语言模型的团队，但我们决定作为强化学习团队朝这个方向发展。</p>\\n<p>当时我们开展了两个项目，一个主要解决数学问题，另一个专注于检索和网络浏览，利用强化学习来更好地学习如何使用这些工具，这就是我进入语言模型领域的契机。我还记得确切的时间应该是2020年或2019年年中。</p>\\n<p><strong>Pieter Abbeel：读博期间，你曾转变过一次研究方向，由最初的模仿学习（教机器人从示例中学习）转向了强化学习。当时你在模仿学习方面已经做出了一些成绩，为什么要转向强化学习？对于大部分研究人员来说，自己正在从事的研究方向是否正确是一个关键问题，尤其是前期已经在某个课题上投入大量时间的情况下，这时候转向相关的全新领域是一项成本较高的决策，在新领域取得相同的产出效率之前，可能会经历一段适应期，这期间也许会进展缓慢。</strong></p>\\n<p><strong>John Schulman：</strong> 是的。除了最初决定进入机器学习领域以外，从机器人转向强化学习是我做过的最大的研究方向调整。转变过后，很长一段时间内我都只是做一些简单示例，例如实现拼车功能等，这种状态持续了六个月左右。很快我意识到需要下定决心，之后转变过程逐渐变得顺利，这种转变更像是一种连续的过渡，比如从一个强化学习领域转向另一个强化学习领域，从解决一个问题转向解决另一个问题。一直以来，这些转变对我来说都相当自然，我认为，能够在适当的时机将研究主题切换到强化学习是一种幸运。</p>\\n<p><strong>Pieter Abbeel：至少在深度学习真正发挥作用的当下，你参与了将规划与强化学习相结合的一些最早期工作，这些工作至今仍是许多人构建和使用的基础，包括近端策略优化（PPO），这可能仍然是至今使用最广泛的强化学习算法。</strong></p>\\n<p><strong>虽然现在已经过去了七年，但你肯定还记得自己的博士时代，现在许多博士生都在思考一个问题，那就是工业界对科研的影响，比如拥有巨额预算的OpenAI，来自微软的最新高达100亿美元的投资，这笔资金似乎主要用于计算资源，可能还包括数据治理等。如此大规模的预算显然在博士生项目中不可能达到，因此，在OpenAI工作也许能得到博士项目中不可能有的机会，也许能在AI领域大显身手。从你的角度出发，作为OpenAI的一员，你认为是否存在一些无需大规模计算和数据预算，但仍能产出令人兴奋的成果的机会？</strong></p>\\n<p><strong>John Schulman：</strong> 根据我偶然看到的一些学术论文，上述目标是可以实现的，但达成这一目标并不容易。你需要找到不会被工业界赶超且不会被某个新发布的模型所取代的方向。我认为，其实有很多符合上述条件的方向，比如进行微调、科学调查、深入了解模型的泛化方式或者研究更好的监督方法。开展高质量科学研究的机会有很多，我们需要真正深入挖掘、理解事物，工业实验室可能更关注结果和创造出更好的产品。无论如何，AI领域的博士生们需要进行精细的研究，保持好奇心，并尽量深入理解这一领域。</p>\\n<p><strong>Pieter Abbeel：你会不会在闲暇时间研究一些自己感兴趣的问题？假如我是计算机专业的学生，有没有一种流程可以用来识别这些有价值的问题？</strong></p>\\n<p><strong>John Schulman：</strong> 我认为，需要找到模型具备哪些能力会令人兴奋，但实现这些能力的方法并不完全明确。目前，我们在训练模型方面存在一些局限，也许并不需要引入新的能力，比如让模型进行外科手术等，但令人困惑的是，我们无法理解模型能力的来源在数据集中的哪个部分。</p>\\n<p>我认为，<strong>将模型行为归因于数据集是一个有趣的领域，最近在这方面也展开了一些有意思的研究</strong>。然而，目前的问题是，我们预先训练模型，然后在不同的数据集上进行微调，最终得到了一个模型，但我们不确定模型的所有行为都来自于哪里。对于这种情况，我们也许可以将感兴趣的未来能力或当前方法的缺点与解决措施相结合，并避免那些无需导师的帮助就能得到解决的问题。</p>\\n<p><strong>Pieter Abbee：回顾深度学习的起源，当初，除少数人之外没有人研究深度学习，而之后该领域取得了突破性进展，当然，其中包含很多合作者的努力。现在，每个人都在使用大型数据集训练出的大模型， 但在未来可能会是微型数据集，虽然模型不会过于微小，但可能存在一些尚未发现的方法，与我们今天的研究大相径庭。</strong></p>\\n<p><strong>John Schulman：</strong> 这很有可能成为现实，未来也许能够通过更小但更丰富的数据集取得更大的突破。人类学习的数据量并不少，我们通过眼睛接收到的数据带宽非常高。尽管与预训练数据集相比，人类婴儿接触到的数据非常有限，大部分局限于一个房间内，但婴儿看到的数据更加丰富。</p>\\n<p>总之，如果你能仅通过微型数据集学习得到非常优秀的视觉系统，这将是令人惊叹的成就。所以我认为，很可能存在一些新的架构和损失函数能实现这一目标。我们经常倾向于继续追求那些已经取得良好结果且可扩展性强的方法，但实际上仍然有很多尚未发现的内容，我们可能仍停留在某种特定的局部最优解中，很难预测下一个重大突破点在哪里。</p>\\n<p>当人们谈及Geoffrey Hinton、Yoshua Bengio和Yann Lecun（深度学习三巨头）在深度学习起源阶段的合作，可能存在幸存者偏见。他们三人可能当时正好从事着正确的领域，但还有很多人当时正从事其他鲜为人知的领域，他们工作的重要性从未得到关注，也无法获得认可。</p>\\n<p><strong>Pieter Abbeel：我个人对此不完全赞同，但在学术界确实很难做到攻读博士学位的同时，开展与工业界同样有趣的研究。若能直接进入工业界，你将立即获得更为丰富的资源，从而进行更大规模的实验，至少能够产出更为显著的成果。以你的个人经历为例，你在加州理工学院完成了本科学业，后来在伯克利攻读博士学位，假设你现在刚从加州理工学院毕业，你认为自己会选择继续攻读博士学位，还是会直接加入工业研究实验室？</strong></p>\\n<p><strong>John Schulman：</strong> 无论是攻读博士学位还是直接加入研究计划，都可能是一个不错的起点，这两种选择都有各自的优势和不足。例如，博士学位需要更长时间的投入，这意味着你可以真正成为某个领域的专家，同时，作为博士研究生，你也可以参与实习等，我认为选择攻读博士没什么不好。但如果你选择直接加入实习计划，可能自由度相对较低，在探索不同领域时可能会受到一些限制，因此在这二者之间可能存在一些权衡。我不确定自己会选择哪种方式，我认为这两种都不错。</p>\\n<p><strong>Pieter Abbeel：作为在我的实验室中显然是最成功的博士研究生之一，许多新学生都会问你是如何进行研究的。作为博士研究生，你遵循着怎样的研究方法，你的日程安排是怎样的，你在项目中的发展轨迹如何，是如何推进项目运行的？</strong></p>\\n<p><strong>John Schulman：</strong> 我当时正好具备合适的时间和地点条件，因此即便是我自己重新来过，也很难完全复现当时确切的初始条件。但我的建议是，如果要研究某个领域，我会充分阅读相关资料，比如详细阅读该领域的论文，此外我还会深入学习很多基础内容，比如优化和信息理论的相关教材。</p>\\n<p>就具体问题而言，在最初的几年里，我基本上遵循实验室的研究方向。我们创造出一些不错的成果，但并非是随意地进行研究，也尝试着采用一些看起来相对合理的方法。总的来说，我试图在其中找到一种自然或合理的平衡，我认为启发性问题的确很有价值，但在不构建产品的情况下，这些问题未必需要解决。最终，你可能并不会真正得出一个有用的产品，只是希望这些问题能够催生出一个良好的解决方案。</p>\\n<p>在最初的几年里，我努力在各个领域中实现一些令人兴奋的成果，随着深度学习的兴起，我开始产生类似中年危机的危机感，开始反思自己在机器人领域所做的一切是否足够深入，我不确定这些方法是否能取得持续性成功，因此我开始更加深入地探索，并决定转向深度强化学习研究。</p>\\n<p>这一发展过程相当自然。一开始，你会以目标为导向进行工作，这个阶段你对于方法可能并不十分了解。在持续研究一段时间后，你会逐渐认识到当前范式的局限性，这会为下一个范式或是下一个方法导向的研究提供有益思路。</p>\\n<p><strong>欢迎 Star、试用 OneFlow 最新版本：</strong></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FOneflow-Inc%2Foneflow%2F\" title=\"https://github.com/Oneflow-Inc/oneflow/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/Oneflow-Inc…</a></p>', 'author': 'OneFlow一流科技', 'reading_time': 23, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 23, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '194b9a20df0ced2aeafd82144d9d4648b90dd0e7259f301b9b4e40fe4328ff40', 'title': 'ChatGPT是如何产生心智的？ | 京东云技术团队', 'url': 'https://juejin.cn/post/7284069963123687483', 'published_at': '2023-10-01T05:15:23Z', 'created_at': '2023-10-04T14:11:40.293379Z', 'changed_at': '2023-10-04T14:11:40.293379Z', 'content': '<h1>一、前言 - ChatGPT真的产生心智了吗？</h1>\\n<p>来自斯坦福大学的最新研究结论，一经发出就造成了学术圈的轰动，“原本认为是人类独有的心智理论（Theory of Mind，ToM），已经出现在ChatGPT背后的AI模型上”。所谓心智理论，就是理解他人或自己心理状态的能力，包括同理心、情绪、意图等。这项研究中，作者发现：davinci-002版本的GPT3已经可以解决70%的心智理论任务，相当于7岁儿童。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7add26e0b0e64f6c8c2ed0339773e7fa~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1810&amp;h=360&amp;s=201614&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>2023 年，面对铺天盖地的 AI 应用，我们人类终于意识到，有一些东西被永远的改变了。但在这波 AI 热潮之中，只有一个应用是真正吓人的——ChatGPT。由于心智无法通过量化评判，但ChatGPT确实满足智能的定义，例如推理、计划、解决问题、抽象思考、理解复杂想法、快速学习。但ChatGPT本质上只做一件事情：续写。当我们给出前N个词汇的时候，如果一个模型能够告诉我们第“N+1”个词汇大概率会是什么，我们就认为模型掌握了语言的基本规律。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d7105dbde71f49feb861039698bde9b2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1414&amp;h=773&amp;s=7664550&amp;e=gif&amp;f=137&amp;b=f6f5f5\" alt=\"\" loading=\"lazy\"/></p>\\n<p>为什么光凭“续写”就可以产生智能？以下是摘自知乎的一段回答：</p>\\n<p>“为什么这么一个简单的接话茬能力让ChatGPT看起来能够解决各种各样的任务呢？因为我们人类大部分的任务都是以语言为载体的。当我们前面说了一些话，它把接下来的话接上，任务就完成了。ChatGPT作为一个大语言模型，目的就是“把话接上”，而把话接上这件事情可以在不知不觉中帮我们完成各种任务。”，这也解释了为什么ChatGPT有时候瞎胡诌，他并没有撒谎，他不知道对错，他只是想把对话顺利进行下去。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0ec56b4524ce4b5da8c4dbde13bfba36~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2316&amp;h=854&amp;s=626559&amp;e=png&amp;b=f6f6f7\" alt=\"\" loading=\"lazy\"/></p>\\n<p>如果真的像上述所说，那GPT似乎没有我们想象的神奇，看起来只是一个基于大数据和统计学的语言模型，通过它学习的海量文本预测下一个概率最高的词。就像是有一个容量巨大的“数据库”，所有的回答都是从这个数据库里拿到的。</p>\\n<p>但奇怪的是，ChatGPT又可以回答他没有学习过的问题，最具代表性的是训练集中不可能存在的六位数加法，这显然无法通过统计学的方式来预测下一个最高概率的数字是多少。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e67900cfbe994897a864c8fbed4508f4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1726&amp;h=358&amp;s=71999&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>不仅如此，GPT还学习到了在对话中临时学习的能力。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c02d802c3aca4f09b41e493e27855898~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1720&amp;h=414&amp;s=126095&amp;e=png&amp;b=f7f7f8\" alt=\"\" loading=\"lazy\"/></p>\\n<p>看起来ChatGPT除了“续写”外，还真的产生了逻辑推理能力。这些统计之外的新能力是如何出现的？</p>\\n<p>如何让机器理解语言，如何让代码存储知识？这篇文章，只是为了回答一个问题：一段代码是如何拥有心智的？</p>\\n<h1>二、Attention is all you need - 注意力机制</h1>\\n<p>搜索所有有关ChatGPT的文章，发现有一个词的出现频率特别高，Attention is all you need。ChatGPT的一切都建立在“注意力机制”之上，GPT的全称是Generative Pre-trained Transformer，而这个transformer就是一个由注意力机制构建的深度学习模型。其来源于2017年的一篇15页的论文，《Attention is all you need》[1]。再结合OpenAI对于GPT2和GPT3的两篇论文[2][3]，我们可以拆开这个大语言模型，看看他在说话的时候究竟发生了什么。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e18cc76927f34e828eccee6423f4f440~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2422&amp;h=922&amp;s=409089&amp;e=png&amp;b=fefdfd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>注意力机制的诞生来源于人脑的思维方式，例如在读这段话时，你的注意力会不断的从左往右一个字一个字的闪过，之后再把注意力放到完整的句子上，理解这些字词的关系，其中有些关键词还会投入更多的注意，这一切发生在电光火石之间。</p>\\n<p>而基于注意力机制的Transformer和GPT系列模型就是在模拟这一思维过程，通过让机器理解一句话中字词之间的关系和意义，完成下一个词的续写，然后再理解一遍，再续写一个词，最后写成一段话。要让程序模仿这件事并不容易。如何让机器计算字符，如何让代码存储知识，为什么将以上模型框架中的一个单元拆开后，全都是圆圈和线？</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6ad540a8ed6b492a875870ee2fc61b78~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1062&amp;h=930&amp;s=614060&amp;e=png&amp;b=f5f5f5\" alt=\"\" loading=\"lazy\"/></p>\\n<p>所以研究AI的第一步，是搞清楚上图中的一个圆圈究竟能够干什么。</p>\\n<h2>2.1 神经元 - 圆圈和线</h2>\\n<p>1957年的一篇论文，《感知器：大脑中信息存储和组织的概率模型》[4]中也出现了一堆圆圈和线，这就是今天各种AI模型的基本单元，我们也叫它神经网络。一个世纪前，科学家就已经知道了人脑大概的运作方式，这些圆圈模拟的是神经元，而线就是把神经元连接起来的突触，传递神经元之间的信号。</p>\\n<p>\\ufeff</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7d8662a2fe0a4f499900819575ada100~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2574&amp;h=894&amp;s=586318&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>将三个神经元连接在一起，就得到了一个开关，要么被激活输出1，要么不被激活输出0。开关可以表达是否，区分黑白，标记同类，但是归根到底都是一件事情，分类。过去几十年，无数个人类最聪明的头脑所做的，就是通过各种方式把这些圆圈连接起来，试图产生智能。</p>\\n<p>\\ufeff<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fplayground.tensorflow.org%2F\" title=\"https://playground.tensorflow.org/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">这个网站</a>可以模拟更多的神经元分裂问题。可以看到一个神经元能处理的情况还是太有限了，能分开明显是两块的数据，而内圈外圈的数据就分不开。但如果加入激活函数，再增加新的神经元，每一个新增的神经元都可以在边界上新增一两条折线，更多的折线就可以围得越来越像一个圆，最终完成这个分类。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/041ed5c1e9c24bcc9358aa2f00ea9175~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2732&amp;h=864&amp;s=600580&amp;e=png&amp;b=f8f8f8\" alt=\"\" loading=\"lazy\"/></p>\\n<p>分类可以解决很多具体问题，假如上图中的每个点的X轴和Y轴分别代表一只小狗的岁数和体重，那么只凭这两种数值就可以分出来这是两个不同品种的狗，每个点代表的信息越多，能解决的问题也就越复杂。比如一张784个像素的照片，就可以用784个数字来表示分类，这些点就能分类图片。更多的线，更多的圆圈，本质上都是为了更好的分类。这就是今天最主流的AI训练方案，基于神经网络的深度学习。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fa4f4b3dd9b04ab090e841e1883de83e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1438&amp;h=1262&amp;s=1291762&amp;e=png&amp;b=1b1b1b\" alt=\"\" loading=\"lazy\"/></p>\\n<p>学会了分类，某种程度上也就实现了创造。</p>\\n<p>这就是为什么有这么多业界学者意识到了深度学习的本质，其实是统计学，沿着圆圈和线的道路，他们终究会到达终点，成为人人都可以使用的工具。而如果拆开GPT系列模型，暴露出来的也仍然只是这些圆圈和线。但分类和统计真的能模仿人的思维吗？ 在论述之前，先了解一下成语接龙的底层原理。</p>\\n<h2>2.2 成语接龙</h2>\\n<p>在2018年第一代GPT的原始论文[5]中，我们可以看到GPT系列的模型结构。回想上文中提到的注意力机制，这一层被叫做注意力编码层，它的目标就是模仿人的注意力，抽取出话语之间的意义，把12个这样的编码层叠在一起，文字从下面进去，出来的就是GPT预测的下一个词。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b83d65898f1046c18cca855688d09c27~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2108&amp;h=874&amp;s=311116&amp;e=png&amp;b=fbf5f4\" alt=\"\" loading=\"lazy\"/></p>\\n<p>比如输入how are you之后，模型会输出下一个单词doing，为什么它会输出doing？接下来我们就得搞明白中间到底发生了什么。</p>\\n<p>输入how are you后，这三个单词会被转换为3个1024维度的向量，接着每个向量都会加上一个位置信息，表示how是第一个词，are是第二个词，以此类推之后它们会进入第一个注意力编码层，计算后变成三个不一样的1024长的向量，再来到第二层、第三层，一直经过全部的24个注意力编码层的计算处理，仍然得到三个1024长的向量，对下一个词的续写结果就藏在最后一个向量里面。关键的计算就发生在这些注意力编码层，这一层里又可以分成两个结构，先算多头注意力，再算全连接层。注意力层的任务是提取话语间的意义，而全链接层需要对这些意义做出响应，输出存储好的知识。</p>\\n<p>我们可以先用how做个例子，注意力层里有三个训练好的核心参数KQV，用于计算词语间的关联度，将它们与每个向量相乘后，就能得到how和are的关联度，再通过这种方式计算how和you， how和how的关联度，就能得到三个打分，分数越高意味着它们的关联越重要。之后再让三个分数和三个有效信息相乘再相加，就把how变成了一个新的64个格子的向量，然后对are和you做同样的操作，就得到了三个新的向量。</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8839714159fe418181049edeaa535c30~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=846&amp;h=165&amp;s=22630&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>参与这轮计算的KQV是固定的，而模型里一共有16组不同的KQV，他们分别都会做一轮刚才这样的运算，得到16组不同的输出，这叫做多头注意力，意味着对这句话的16组不同的理解。把它们拼在一起，就得到了和输入相同长度的1024个格子，再乘一个权重矩阵W就进入到了全链接层的计算。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1e76d3a15f2d4dd1a003848588af4a7a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3546&amp;h=1780&amp;s=1386403&amp;e=png&amp;b=fdfcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p>在全连接层里，就是4096个我们熟悉的神经元，它们都还是在做分类的工作。这里的计算是把被注意力层转换后的how向量和这里的每一个神经元都连接在一起，1024个格子里的每一个数字都分别和第一个神经元的连线的权重相乘再相加，这个神经元会输出一个相似度分值，与此同时，每一个神经元都在做类似的操作。只有少数神经元的输出大于零，也就意味着神经元对这个敏感，再连接1024个格子号所对应的向量，就又得到了一个新的向量。之后are和you做类似的计算，就得到了三个和初始长度一样的1024长的格子串，这就是一层注意力编码层内发生的事情。之后的每一层都按照相同的流程在上一层的基础上做进一步的计算，即便每一层都只带来了一点点理解，24层算完以后也是很多理解了，最终还是得到三个向量，每个1024长。而模型要输出的下一个词就基于这最后一个向量，也就是you变换来的向量，把它从1024恢复成0-50256范围的序号，我们就能看到这个序号向量在词表里最接近的值。到这一步就可以说模型算出了how are you之后的下一个词，最有可能是doing。</p>\\n<p>我们希望模型继续续写，就把这个doing续在how are you后面，转换成四个向量，再输入进模型，重复刚才的流程，再得到下一个词。这样一个接一个，一段话越来越长，直到结束，变成我们看到的一段话，这就是文字接龙的秘密。而ChatGPT也只是把这个续写模型改成了对话界面而已，你提的每一个问题都会像这样成为续写的起点，你们共同完成了一场文字接龙。</p>\\n<h2>2.3 “大”语言模型</h2>\\n<p>刚刚提到的每一层的计算流程长，其实还好，GPT真正吓人的地方是参数量大。GPT1的基本尺寸是768，每一层有超过700万个参数，12层就是1.15亿个参数，在他发布的2018年已经非常大了。我们刚刚拆开的GPT medium基本尺寸是10241，共有24层，每一层有1200万参数，乘起来就是3.5亿参数。而到了ChatGPT用的GPT3的版本，它的参数量是1750亿，层数增加到了96层。GPT4并没有公布它的大小，有媒体猜测它是GPT3的六倍，也就是一万亿参数。这意味着，即便把一张3090显卡的显存变大几百倍，让他能装的下级GPT4，回答一个简单问题可能仍然需要计算40分钟。</p>\\n<p>拆开这一切，就会发现没有什么惊人的秘密，只有大，文明奇观的那种大，无话可说的那种大，这就是GPT系列的真相，一个“大”语言模型。但是我们还是无法回答为什么这样的模型能够产生智能，以及现在还出现了一个新的问题，为什么参数量非得这么大？</p>\\n<p>让我们先总结一下目前的已知信息，第一，神经网络只会做一件事情，数据分类，第二，GPT模型里注意力层负责提取话语中的意义，再通过全链接层的神经元输出存储好的知识，第三，GPT说的每一个词都是把对话中的所有词在模型中跑一遍，选择输出概率最高的词。所以，GPT拥有的知识是从哪来的？我们可以在OpenAI的论文中看到ChatGPT的预训练数据集，他们是来自网站、图书、开源代码和维基百科的大约700GB的纯文本，一共是4991个token，相当于86万本西游记。而它的训练过程就是通过自动调整模型里的每一个参数，完成了这些海量文字的续写。</p>\\n<p>在这个过程中，知识就被存储在了这一个一个的神经元参数里，之后它的上千亿个参数和存储的知识就不再更新了。所以我们使用到的ChatGPT其实是完全静止的，就像一具精致的尸体，它之所以看起来能记住我们刚刚说的话，是因为每输出一个新的词，都要把前面的所有词拿出来再算一遍，所以即便是写在最开头的东西，也能够影响几百个单词之后的续写结果。但这也导致了ChatGPT每轮对话的总词汇量是有上限的，所以GPT不得不限制对话程度。就像是一条只有七秒记忆的天才金鱼。</p>\\n<p>现在回到前言中提到的问题，为什么ChatGPT可以回答他没有学习过的互联网不存在的问题，例如一个训练数据里不可能存在的六位数加法，这显然无法通过统计学的方式来预测下一个最高概率的数字是多少，这些统计之外的新能力是如何出现的？</p>\\n<p>今年5月，OpenAI的新研究给了我启发，这篇论文名为《语言模型，可以解释语言模型中的神经元》[6]。简单来说就是用GPT4来解释GPT2。给GPT2输入文本时，模型里的一部分神经元会激活，Open AI让GPT4观察这个过程，猜测这个神经元的功能，再观察更多的文本和神经元，猜测更多的神经元，这样就可以解释GPT2里面每一个神经元的功能，但是还不知道GPT4猜的准不准。验证方法是让GPT4根据这些猜想建立一个仿真模型，模仿GPT2看到文本之后的反应，再和真的GPT2的结果做对比，结果一致率越高，对这个神经元功能的猜测就越准确。OpenAI在<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fopenaipublic.blob.core.windows.net%2Fneuron-explainer%2Fneuron-viewer%2Findex.html%23%2F\" title=\"https://openaipublic.blob.core.windows.net/neuron-explainer/neuron-viewer/index.html#/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">这个网站</a>里记录了他们对于每一个神经员的分析结果。</p>\\n<p>比如我们输入30, 28，就可以看到第30层的第28个神经元的情况。GPT4认为这个神经元关注的是具体时间。下面是各种测试例句，绿色就表示神经元对这个词有反应，绿色越深，反应就越大。可以发现，即便拼写完全不同，但这些模型中间层的神经元也已经可以根据词语和上下文来理解它们的意义了。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6e5ace4109e0459da1b006f3f48337ab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3004&amp;h=1008&amp;s=367918&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<p>但OpenAI也发现，只有那些层数较低的神经元才是容易理解的。这个柱状图里的横坐标是对神经元解释的准确程度，纵坐标是神经元的数量。可以看到，对于前几层的神经元，差不多一半都能做到0.4以上的准确度。但是层数越高，得分低的神经元就越来越多了，大多数神经元还是处在一片迷雾之中。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a670de74074f44828005d77b82e5f7cc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2534&amp;h=798&amp;s=146392&amp;e=png&amp;b=fcfcfc\" alt=\"\" loading=\"lazy\"/></p>\\n<p>因为对于语言的理解本来就是难以解释的，比如这样一段对话。对于中文母语的我们来说，很快就能理解这段话的意思，但是对于一个神经网络，只靠几个对“意思”有反应的神经元显然是不够意思。</p>\\n<blockquote>\\n<p>A：“你这是什么意思？” B：“没什么意思，意思意思。” A：“你这人真有意思。” B：“其实也没有别的意思。” A：“那我就不好意思了。” B：“是我不好意思。”</p>\\n</blockquote>\\n<p>而GPT似乎理解了这些意思，它是如何做到的？</p>\\n<h2>2.4 Emergence - 涌现</h2>\\n<p>“将万事万物还原为简单基本定律的能力，并不蕴含从这些定律出发，重建整个宇宙的能力。” —— Philip Anderson.</p>\\n<p>1972年，理论物理学家Philip Anderson在Science发表了一篇名为《More is Different》[7]的论文，奠定了复杂科学的基础，安德森认为：“大量基本粒子的复杂聚集体的行为并不能依据少数粒子的性质作简单外推就能得到理解。取而代之的是在每一复杂性的发展层次之中呈现了全新的性质，从而我认为要理解这些新行为所需要作的研究，就其基础性而言，与其它相比也毫不逊色”。</p>\\n<p>回顾语言模型的结构，信息是随着注意力编码层不断往上流动的，层数越高的神经元越有能力关注那些复杂抽象的概念和难以言说的隐喻。这篇叫《在干草堆里找神经元》[8]的论文也发现了类似的情况，他们找到了一个专门用来判断语言是否为法语的神经元。如果在小模型当中屏蔽这个神经元，他对法语的理解能力马上会下降，而如果在一个大模型中屏蔽它，可能几乎没什么影响。这意味着在模型变大的过程中，一个单一功能的神经元很可能会分裂出多个适应不同情况的神经元，它们不再那么直白的判断单一问题，进而变得更难。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/775caed01abd4e8491ae9825005fa2cf~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2494&amp;h=1478&amp;s=824138&amp;e=png&amp;b=fdfdfd\" alt=\"\" loading=\"lazy\"/></p>\\n<p>能理解这就是OpenAI为什么非得把模型搞得这么大的原因，只有足够大才足够抽象，而大到了一定程度，模型甚至会开始出现从未出现过的全新能力。</p>\\n<p>在这篇名为《大语言模型的涌现能力》的论文中[9]，研究人员对于这些大小不同的语言模型完成了八项新能力的测试。可以看到，他们在变大之前一直都不太行，而一旦大到某个临界点，它突然就行了，开始变成一条上窜的直线，就像是在一瞬间顿悟了一样。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a4494d9e182749c0b82a9a439efc8989~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1576&amp;h=1058&amp;s=223109&amp;e=png&amp;b=ffffff\" alt=\"\" loading=\"lazy\"/></p>\\n<p>纵观我们的自然和宇宙，一个复杂系统的诞生往往不是线性成长，而是在复杂度积累到某个阈值之后，突然的产生一种新的特质，一种此前从未有特的全新状态，这种现象被称作涌现，Emerge。而这个上千亿参数的大语言模型，好像真的涌现出了一些数据分类之上的新东西。</p>\\n<p>最近读了《失控》这本书，里面也提到了一个概念叫涌现，可以理解为蜂群智慧。一只蜜蜂是很笨的，但是组成一个群体就可以完成很多超越个体智慧的决策。当然我不觉得AI的单个神经元是愚笨的，而是会不会这种“意识”，也会因为大量功能迭代，学习，突然涌现出来，就像人类的进化，不知怎么的就有了意识。就像这个世界的一切都是由原子构成，但如果只是计算原子之间的相互作用力，我们永远也无法理解化学，也无法理解生命。所以，如果仅仅从还原论的角度把AI看作只做二元分裂的圆圈和线，我们就永远无法理解大语言模型今天涌现出的抽象逻辑和推理能力，为此，我们需要在一个新的层级重新理解这件事。</p>\\n<h1>三、中文房间</h1>\\n<p>1980年，美国哲学教授John Searle在这篇名为《心智大脑和程序》[10]的论文中提出了一个著名的思想实验，中文房间。把一个只懂英文的人关在一个封闭的房间里，只能通过传递纸条的方式和外界对话。房间里有一本英文写的中文对话手册，每一句中文都能找到对应的回复。这样房间内的人就可以通过手册顺畅的和外界进行中文对话，看起来就像是会中文一样，但实际上他既不理解外面提出的问题，也不理解他所返回的答案。</p>\\n<p>他试图通过中文房间证明，不管一个程序有多聪明或者多像人，他都不可能让计算机拥有思想、理解和意识。真的是这样吗？在这个名为<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fiep.utm.edu%2Fchinese-room-argument%2F\" title=\"https://iep.utm.edu/chinese-room-argument/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">互联网哲学百科全书</a>的网站中，可以看到围绕中文房间的各种争论，他们都没能互相说服。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dd72e1d7e525459c84635bcfac0636e9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2382&amp;h=1004&amp;s=767521&amp;e=png&amp;b=fefefe\" alt=\"\" loading=\"lazy\"/></p>\\n<p>这些讨论都停留在思想层面，因为如果只靠一本打印出来的手册，中文房间是不可能实现的。中文对话有着无穷无尽的可能，即便是同样一句话，上下文不同，回答也不同。这意味着手册需要记录无限多的情况，要不然总有无法回答的时候。但诡异的是，ChatGPT真的实现了。作为一个只有330GB的程序，ChatGPT在有限的容量下实现了几乎无限的中文对话，这意味着他完成了对中文的无损压缩。</p>\\n<p>想象有一个这样的复读机，空间只有100MB，只能放十首歌。要听新的歌，就得删掉旧的歌。但现在我们发现了一个神奇复读机。现在只需要唱第一句，这个复读机就可以通过续写波形的方式把任何歌曲播放出来。我们应该怎么理解这个复读机？我们只能认为他学会了唱歌。</p>\\n<h1>四、Compression - 压缩即智慧</h1>\\n<p>回想GPT的学习过程，它所做的，就是通过它的1750亿个参数，实现了它所学习的这4990亿个token的压缩。到这一步，逐渐意识到，是压缩产生了智能。</p>\\n<p>Jack Ray, OpenAI大语言模型团队的核心成员，在<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdO4TPJkeaaU\" title=\"https://www.youtube.com/watch?v=dO4TPJkeaaU\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">视频讲座</a>中提到，压缩一直是我们的目标。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/926a469cb4c042ff83a9be96db959c88~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=4016&amp;h=2436&amp;s=247725&amp;e=png&amp;b=010101\" alt=\"\" loading=\"lazy\"/></p>\\n<p>接下来是我对于压缩及智能这件事的理解，假设我要给你发送这句话，“压缩即智慧”。</p>\\n<p>我们可以把GPT当做一种压缩工具，我用它压缩这句话，你收到后再用GPT解压，我们得先知道这句话的信息量有多大。在GBK这样的编码里，一个汉字需要两个字节，也就是16个0/1来表述，这可以表示2的16次方，也就是65536种可能。这句话一共5个字符，就需要一共80个0和1，也就是80比特。但实际上这句话的信息量是可以小于80比特的。它的真实信息量其实可以用一个公式计算。</p>\\n<p>\\ufeff\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9d745691c4264f9189cf73f4994f383e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=392&amp;h=92&amp;s=5405&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>这是1948年香农给出的信息熵的定义，它告诉我们信息的本质是一种概率密度。我们可以把这里的P简单理解为每个字出现的概率，它们出现的概率越低，整句话的信息量就越大。如果这句话里的每个字都是毫无规律的随机出现，那么P的概率就是1/65536，计算后的信息量就是原始的80比特。常见的传统压缩方法是找到重复的字，但几乎不重复的句子就很难压缩。更重要的是，正常的语言是有规律的，“压”后面跟着“缩”的概率远大于1/65536，这就给了信息进一步压缩的空间。而语言模型所做的就是在压缩的过程中找到语言的规律，提高每个字出现的概率。比如我们只发送“压缩”，让语言模型开始续写，预测的概率表里就会出现接下来的词，我们只需要选择“即”和“智慧”所在的位置，例如（402，350）。那这两这个数字就实现了信息的压缩，接收方基于这些信息，从相同语言模型的概率去处理，选出数字对应的选项，就完成了解压。2个最大不超过5000的数字，每个数字只要13位0/1就能表示，加上前2个字，一共也只需要发送52位0/1，信息压缩到原来的52/80大约65%。</p>\\n<p>相反，如果语言模型的预测效果很差，后续文字的词表还是会很长，无法实现很好的压缩效果。所以可以发现，压缩效果越好意味着预测效果越好，也就反映了模型对于被压缩信息的理解，而这种理解本身就是一种智能。为了把九九乘法表压缩的足够小，他需要理解数学，而如果把行星坐标压缩的足够小，他可能就理解了万有引力。今天，大语言模型已经成为了无损压缩的最佳方案，可以实现14倍的压缩率。压缩这一视角最大的意义在于，相比于神秘莫测的涌现，它给了我们一个清晰明确、可以量化机器智能的方案。即便面对中文房间这样的思想实验，我们也有办法研究这个房间的智能程度。</p>\\n<p>但是，通过压缩产生的智慧和人的心智真的是同一种东西吗？</p>\\n<h1>五、写在最后</h1>\\n<p>如果要问，现阶段GPT和人类说话方式最大的不同是什么，我认为，答案是他不会说谎。对于语言模型来说，说和想是一件事情，他只是一个字一个字的把他的思考过程和心理活动说出来了而已。GPT从不回答我不知道，因为他并不知道自己不知道，这就是AI的幻觉，看起来就像是一本正经的胡说八道，他只是想让对话继续下去，是否正确反而没那么重要。优化这个问题的方法也很简单，只需要在提问的时候多补充一句，Let&#39;s think step by step，请逐步分析，让GPT像人一样多想几步，对他来说也就是把想的过程说出来。Step by step，这种能力也被称为Chain of Thought，思维链。心理学家Daniel Kahneman把人的思维划分成了两种，系统一是直觉、快速的、没有感觉的，系统二则需要主动的运用知识、逻辑和脑力来思考。前者是快思考，就像我们可以脱口而出八九七十二，九九八十一，而后者是慢思考。就比如要回答72乘81是多少，就必须列出过程，一步步计算。思维链的存在意味着大语言模型终于有了推理能力。而为了做到这件事，我们的大脑进化了6亿年。我们可以在6亿年前的水母身上看到神经网络最古老的运行方式。水母外围的触角区域和中心的嘴部区域都有神经元。当触角感知到食物时，这里的神经元会激活，然后把信号传给中心的神经元，食物也会被这个触角卷起来送到嘴里。漫长的岁月里，我们的大脑就在神经网络的基础上一层又一层的叠加生长出来。</p>\\n<p>首先进化出来的是爬虫类脑，这部分和青蛙的脑子有点像，它控制着我们的心跳、血压、体温这些让我们不会死的东西。然后是古生物脑，它支配着我们的动物本能，饥饿、恐惧和愤怒的情绪，繁衍后代的欲望都来自边缘系统的控制。而最外侧这两毫米左右的薄薄的一层，是最近几百万年才进化出来的新结构、新皮质，我们人类引以为傲的那些部分，语言、文字、视觉、听力、运动和思考都发生在这里，但我们对新皮质还是知之甚少。目前已知的是，这里有大概200亿个神经元，每一平方厘米的新皮质中都大约有一千万个神经元和500亿个神经元之间的连接。只需要从人类大脑外侧取下一小片三平方厘米的新皮质，就已经和ChatGPT大的吓人的参数量类似了。而我们的大脑之所以需要这么多神经元，是因为GPT仅仅需要预测下一个词，而我们的神经元需要时刻预测这个世界下一秒会发生什么。</p>\\n<p>最近几十年的神经科学研究发现除了能激活神经元的突触信号，还存在大量负责预测的树突脉冲信号。一个处于预测状态的神经元如果得到足够强的突出信号，就可以比没有预测状态的神经元更早的被激活，进而抑制其他的神经元。这意味着有一个事无巨细的世界模型就存储在我们新皮质的200亿个神经元里，而我们的大脑永远不会停止预测。所以，当我们看到一个东西，其实看到的是大脑提前构建的模型，如果它符合我们的预测，无事发生。而一旦预测错误，大量的其他神经元就会被激活，让我们注意到这个错误，并及时更新模型。所以每一次错误都有它的价值。我们也正是在无数次的预测错误和更新认知中真正认识了世界。</p>\\n<p>现在我可以试着回答最初的问题，GPT或许尚未涌现心智，但他已经拥有了智能。它是一个“大”的语言模型，是几百万个圆圈和线互相连接的分类器，是通过预测下一个词实现文字接龙的聊天大师，是不断向上抽取意义的天才金鱼，是对几千亿文字无损压缩的复读机，是不论对错永远积极回应人的助手。它可能又是一场快速退潮的科技热点，也可能是人类的最后一项重要的发明。从围棋、绘画、音乐到数学、语言、代码，当AI开始在那些象征人类智力和创造力的事情上逐渐超越的时候，给人类最大的冲击不仅仅是工作被替代的恐惧，而是一种更深层的自我怀疑。人类的心智是不是要比我们想象的浅薄的多，我不这么认为。</p>\\n<p>机器可以是一个精妙准确的复读机，而人类是一个会出错的复读机。缺陷和错误定义了我们是谁。每一次不合规矩，每一次难以理解，每一次沉默、停顿和凝视，都比不假思索的回答更有价值。</p>\\n<h1>参考文献</h1>\\n<p>[1] Vaswani, Ashish, et al. &#34;Attention is all you need.&#34; <em>Advances in neural information processing systems</em> 30 (2017).</p>\\n<p>[2] Radford, Alec, et al. &#34;Language models are unsupervised multitask learners.&#34; <em>OpenAI blog</em> 1.8 (2019): 9.</p>\\n<p>[3] Brown, Tom, et al. &#34;Language models are few-shot learners.&#34; <em>Advances in neural information processing systems</em> 33 (2020): 1877-1901.</p>\\n<p>[4] Rosenblatt, F. &#34;The perceptron: A probabilistic model for information storage and organization in the brain. <em>&#34; Psychological Review, 65</em> (1958): <em>386–408.</em></p>\\n<p>[5] Radford, Alec, et al. &#34;Improving language understanding by generative pre-training.&#34; (2018).</p>\\n<p>[6] Bills, Steven, et al. &#34;Language models can explain neurons in language models.&#34; <em>URL <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fopenaipublic\" title=\"https://openaipublic\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">https://openaipublic</a>. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)</em> (2023).</p>\\n<p>[7] Anderson, Philip W. &#34;More Is Different: Broken symmetry and the nature of the hierarchical structure of science.&#34; <em>Science</em> 177.4047 (1972): 393-396.</p>\\n<p>[8] Gurnee, Wes, et al. &#34;Finding Neurons in a Haystack: Case Studies with Sparse Probing.&#34; <em>arXiv preprint arXiv:2305.01610</em> (2023).</p>\\n<p>[9] Wei, Jason, et al. &#34;Emergent abilities of large language models.&#34; <em>arXiv preprint arXiv:2206.07682</em> (2022).</p>\\n<p>[10] Searle, John R. &#34;Minds, brains, and programs.&#34; <em>Behavioral and brain sciences</em> 3.3 (1980): 417-424.</p>\\n<blockquote>\\n<p>作者：京东零售 李新健</p>\\n<p>来源：京东云开发者社区 转载请注明来源</p>\\n</blockquote>', 'author': '京东云开发者', 'reading_time': 23, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 22, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'ea824aaff88433a7e4e5485d917a6fdc7bf0ba2729d96b0a2f1efe5d2caf11d4', 'title': '免费 AI 代码生成器 Amazon CodeWhisperer 初体验', 'url': 'https://juejin.cn/post/7284221961620799543', 'published_at': '2023-10-01T02:01:21Z', 'created_at': '2023-10-04T14:11:40.290523Z', 'changed_at': '2023-10-04T14:11:40.290523Z', 'content': '<blockquote>\\n<p>文章作者：浪里行舟</p>\\n</blockquote>\\n<h3>简介</h3>\\n<p>随着 ChatGPT 的到来，不由让很多程序员感到恐慌。虽然我们阻止不了 AI 时代到来，但是我们可以跟随 AI 的脚步，近期我发现了一个神仙 AI 代码生产工具 CodeWhisperer ，它是一项基于机器学习的服务，其根据自然语言注释和集成开发环境（IDE）中的代码，生成代码建议，帮助提高开发者生产力。</p>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">亚马逊云科技开发者社区</a>为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，点<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fpassport.amazoncloud.cn%2Fsignin%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://passport.amazoncloud.cn/signin?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">这里</a>让它成为你的技术宝库！</p>\\n</blockquote>\\n<p>接下來就跟随我的脚步来看看这款 VSCode 插件的使用方式吧。</p>\\n<h3>安装使用</h3>\\n<ol>\\n<li>开发工具：Visual Studio Code（VSCode）</li>\\n<li>打开工具拓展安装（快捷键：Ctrl + Shift + X），然后搜索亚马逊云科技或 Amazon Toolkit,如下图所示</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8f2808ea4b9b4fcbbb03f7ef707fbc38~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=618&amp;h=400&amp;s=170450&amp;e=png&amp;b=1e1d1d\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>\\n<p>安装完成之后，按照下图步骤进行登录，偷偷告诉你，目前可以免费使用呦。</p>\\n</li>\\n<li>\\n<p>根据弹窗出来的操作，复制 =&gt; 打开亚马逊云科技网站，黏贴验证码 =&gt; 进行亚马逊云科技账号注册（特别提醒，密码需要3中字符以上）=&gt; 然后会出现允许亚马逊云科技在你的 VSCode 上使用</p>\\n<p>完成上诉步骤之后，点击下图位置，开启 AI 代码生成之路</p>\\n</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/686bde20c9ba460689dddd237aa144a4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=634&amp;h=694&amp;s=148875&amp;e=png&amp;b=191919\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>无线的代码联想，让你为所欲为，大大提升了你的划水时间，话不多说，上代码，图1：插件代码联想，图2：按下 Tab 键之后生产的代码。</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8f806ac2febd40a9adb03389ee93ed2b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=470&amp;h=170&amp;s=35917&amp;e=png&amp;b=202020\" alt=\"image.png\" loading=\"lazy\"/><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/046075e500654cd58522e7eb46ba2c23~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=652&amp;h=176&amp;s=38252&amp;e=png&amp;b=202020\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>简单的业务描述，入参定义，它就能够帮你完整实现逻辑，联想过程中，最下面的状态栏 CodeWhisperer 会转动。</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fe6d00b0820e409ba5b20b60aa716247~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=964&amp;h=328&amp;s=135917&amp;e=png&amp;b=212121\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>上下文联想，让你欲罢不能。</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/626c660d3663499797b351f0151deeb9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=966&amp;h=494&amp;s=244362&amp;e=png&amp;b=202020\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h3>总结</h3>\\n<p>相对其他 GPT 代码工具，使用方便，上下文联想强，同时 CodeWhisperer 还可以再代码中检测潜在的错误或问题，并向开发者发送警告，以便他们可以尽早解决这些问题，重点是免费免费免费，重要的事情说3遍</p>\\n<p>文章来源：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F64f8844fd75d3d261ac2a21f%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3Djuejin\" title=\"https://dev.amazoncloud.cn/column/article/64f8844fd75d3d261ac2a21f?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=juejin\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">dev.amazoncloud.cn/column/arti…</a></p>', 'author': '亚马逊云开发者', 'reading_time': 2, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 21, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'fef03b1e62b7f31a1e80b3999a777cfc344a2de93c98406c70f1e2790b869981', 'title': '“羊驼”又双叒叕升级了！LLaMA 2 Long 正式来袭！', 'url': 'https://juejin.cn/post/7283803914645946368', 'published_at': '2023-09-30T15:33:54Z', 'created_at': '2023-10-04T14:11:40.278086Z', 'changed_at': '2023-10-04T14:11:40.278086Z', 'content': '<p>LLaMA 2 刚发布没多久，Meta又推出了它的升级版，LLaMA 2 Long正式登场！性能上全面超越LLaMA 2。和其他竞争对手相比也丝毫不弱，甚至某些方面能超越ChatGPT(3.5)。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/267ab6706dc7410c9822d2ee63179561~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=948&amp;h=432&amp;s=96546&amp;e=png&amp;b=2a2d38\" alt=\"\" loading=\"lazy\"/></p>\\n<p>目前虽然市面上已经有很多大语言模型（LLMs），但我们都知道它们都存在一个问题，就是处理长上下文的时候容易出现健忘和胡说八道的情况。目前我个人若需要处理长文本的时候要么切成一段段的分批总结处理，粗糙一点也可以使用Claude，但对于开源LLM来说，就算微调过也很难处理长文本内容。随着模型规模的增加和应用场景的多样化，如何有效地扩展这些模型以处理更长的上下文变得越来越重要。</p>\\n<p>这篇论文提供了一种全面的方法来解决这个问题，包括模型设计、评估和实用性等多个方面。特别是，它提出了一系列能够处理高达32,768个token的上下文窗口的模型，并进行了广泛的评估和比较。</p>\\n<p>这篇论文的研究主要关注以下几个方面：</p>\\n<ol>\\n<li><strong>有效的长上下文支持</strong>：论文提出了一系列模型，这些模型可以有效地处理高达32,768个token的上下文窗口。</li>\\n<li><strong>性能评估</strong>：作者对这些模型进行了广泛的评估，包括语言建模、合成上下文探测任务以及多种研究基准测试。</li>\\n<li><strong>指令调优</strong>：论文还介绍了一种成本有效的指令调优过程，这个过程不需要人工注释的长指令数据。</li>\\n<li><strong>模型与其他解决方案的比较</strong>：这些模型在多个长上下文任务上超过了其他开源长上下文模型，如Focused Transformer、Yarn等。</li>\\n<li><strong>有效的上下文利用</strong>：论文还探讨了如何更有效地利用长上下文，以提高模型在各种任务上的性能。</li>\\n</ol>\\n<h2>方法简介</h2>\\n<p>在论文的第二章简单介绍了一下此次研究的方法：</p>\\n<ol>\\n<li><strong>持续预训练</strong>（Continual Pretraining）：随着序列长度的增加，训练过程中的计算负担也会因为二次方的注意力计算而显著增加。本研究基于一个假设：<strong>通过从短上下文模型开始持续预训练</strong>，能够学习到类似的长上下文能力。为了实施持续预训练，作者几乎完全保留了原始的 LLAMA 2 架构，只是对位置编码做了必要的调整，以便模型能够处理更长的序列。\\n<ol>\\n<li><strong>位置编码</strong>：通过一些初步的实验，作者发现了 LLAMA 2 位置编码（PE）的一个核心问题，它阻止了注意力模块有效地处理远距离的令牌信息。为了解决这个问题，我们对 RoPE 位置编码做了一些简单但必要的修改，主要是减小了旋转角度（由超参数“基频 b”控制），以减轻对远距离令牌的衰减效应。</li>\\n<li><strong>数据组合</strong>：基于已修改的 PE 的模型，探讨了不同的预训练数据组合，以增强长上下文能力，包括调整 LLAMA 2 预训练数据的比例或加入新的长文本数据。最终发现，对于长上下文的持续预训练，数据质量通常比文本长度更为重要。</li>\\n<li><strong>优化详情</strong>：保持每批次的令牌数量不变，增加序列长度，持续地预训练 LLAMA 2 的checkpoints。所有模型都在 100,000 步中总共训练了 400B 令牌。有了 FLASH ATTENTION，增加序列长度时的 GPU 内存开销几乎可以忽略，但在将 70B 模型的序列长度从 4,096 增加到 16,384 时，发现速度下降了约 17%。对于 7B/13B 模型，使用了 2e−5 的学习率，并配合了 2000 步的预热期的余弦学习率计划。而对于较大的 34B/70B 模型，作者发现降低学习率（1e−5）对于获得单调递减的验证损失非常重要。</li>\\n</ol>\\n</li>\\n<li><strong>指令调优</strong>（Instruction Tuning）\\n<ol>\\n<li>由于长上下文场景下收集人类示范和偏好标签是复杂和昂贵的，作者找到了一种简单且便宜的方法，它通过利用预先构建的大型多样化的短提示数据集，在长上下文基准上表现出意想不到的好效果。具体而言就是使用了 LLAMA 2 CHAT 中使用的 RLHF 数据集，并通过 LLAMA 2 CHAT 本身生成的合成自我指导长数据对其进行了扩展，希望模型能通过大量的 RLHF 数据学习到各种技能，并通过自我指导数据将这些知识应用于长上下文场景。</li>\\n<li>数据生成过程主要关注 QA 格式的任务：从预训练语料库中的长文档开始，随机选择一段内容，并提示 LLAMA 2 CHAT 根据文本内容编写问题-答案对。收集长答案和短答案并使用不同的提示。随后进行了一个自我评估步骤，提示 LLAMA 2 CHAT 验证模型生成的答案。对于生成的 QA 对使用原始长文档（截断以适应模型的最大上下文长度）作为上下文来构造训练实例。</li>\\n<li>对于短指令数据组合成 16,384 tokens的序列。对于长指令数据，在右侧添加填充tokens，以便模型能够单独处理每个长实例，而无需截断。尽管标准的指令调优仅计算输出令牌上的损失，但作者发现在长输入提示上也计算语言模型损失对于在下游任务上持续改进（第 4.3 节）是非常有益的。</li>\\n</ol>\\n</li>\\n</ol>\\n<h2>模型表现</h2>\\n<p>作者从3个方面给出了模型的表现结果，包括预训练评估，指令调优的结果以及人类评估的结果。</p>\\n<h3>预训练评估</h3>\\n<p>对于<strong>Short Tasks</strong>，总的来看，在多数情况下，模型的表现与 LLAMA 2相当，甚至更好。特别是，在编程、数学和知识密集的任务如 MMLU 上，模型的表现有了明显提升。此外，模型在 MMLU 和 GSM8k 上超过了 GPT-3.5，这与先前一项观察到短任务表现下降的研究形成鲜明对比。\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/59d57b5f543c4d6c97fe543fb4e1324a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=840&amp;h=297&amp;s=95347&amp;e=png&amp;b=2a2d38\" alt=\"\" loading=\"lazy\"/>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7cbc5056871943f78a46a61fed0a8987~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=906&amp;h=205&amp;s=80482&amp;e=png&amp;b=2b2e39\" alt=\"\" loading=\"lazy\"/></p>\\n<p>对于 <strong>Long Tasks</strong>， 与之前主要依靠困惑度和合成任务进行评估的研究不同，这项研究采用了实际的语言任务，如NarrativeQA、QuALITY、Qasper和QMSum进行了0-shot、1-shot和2-shot评估。评估重点放在了问答风格的任务上，因为这些任务更便于进行提示设计和自动评估。如果输入提示超过模型的最大输入长度（16,384个令牌），则会进行截断。研究还与其他几种开源的长上下文模型（如Focused Transformer、YaRN、Xgen、MPT和Together&#39;s LLAMA 2fork）进行了比较。结果显示，他们的模型在多数情况下性能优于其他模型，特别是在7B规模上。唯一能与之匹敌的是一个经过大型监督数据集微调的模型“Together-7B-32k”。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f2281c58250946498f61d2efa708a218~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=946&amp;h=548&amp;s=160111&amp;e=png&amp;b=2a2d38\" alt=\"\" loading=\"lazy\"/></p>\\n<p>此外，如何证实它能有效利用扩大的上下文窗口？通过下图分析，研究发现随着上下文长度的增加，模型在各种长任务上的性能也逐渐提升。受到之前研究的启发，该模型在语言建模损失与上下文长度之间呈现出“幂律加常数”的关系。这意味着两个关键点：</p>\\n<ol>\\n<li>尽管性能增益逐渐减小，但模型的性能（尤其是在语言建模损失方面）还是会继续提升，直到文本 token 数量达到32,768。研究以其70B规模的模型为例，说明如果上下文长度加倍，预期损失将以特定因子减少。</li>\\n<li>更大的模型（由曲线的较大β值表示）能更有效地利用上下文信息。</li>\\n</ol>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f7be250bd69b48c98f00468de4eed841~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=985&amp;h=571&amp;s=64969&amp;e=png&amp;b=2a2d38\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>指令调优结果</h3>\\n<p>作者使用模型在ZeroSCROLLS和LEval两个长上下文数据集上进行了性能评估。ZeroSCROLLS包含10个涵盖摘要、问答和多文档聚合任务的长上下文数据集。作者使用了一个70B规模的模型，并且在7个任务中超过了gpt-3.5-turbo-16k，即便该模型没有使用任何人工标注的长上下文数据。在LEval数据集上，模型也展现了令人印象深刻的性能，特别是在问答任务上。</p>\\n<p>然而，评估长上下文语言模型（LLMs）有其困难和局限性。自动评估指标在多个方面都不完善，例如在摘要任务中通常只提供一个基准摘要，而n-gram匹配指标不一定能准确反映人类偏好。此外，在问答和聚合任务中，截断输入上下文可能导致丢失重要信息。还有一个问题是，许多私有模型没有公开他们的训练数据细节，这在进行公共基准测试时可能引发潜在的数据泄露问题。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d8e6dca4f32d46b5adba96f58bbf5464~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1040&amp;h=311&amp;s=121478&amp;e=png&amp;b=2a2d38\" alt=\"\" loading=\"lazy\"/></p>\\n<h3>人类评估</h3>\\n<p>除自动评估外，研究者还进行了人工评估来衡量模型在多轮对话和多文档搜索查询回答等应用场景中的性能。人工评估是由3名不同的人类注释员进行的，涉及2,352个样例。这些评估更准确地反映了长上下文模型的回应质量，因为人类更擅长评价答案在帮助度、诚实度和安全性方面的多样性。结果显示，尽管只有少量的指令数据，研究中的模型在性能上仍能与其他高级模型如MPT-30B-chat、GPT-3.5-turbo-16k和Claude-2竞争。文中也强调，人工评估长上下文任务具有挑战性，通常需要经验丰富和技能熟练的注释员。研究者希望这项研究能推动长上下文自动评估方法的进一步发展，同时也展示了他们的模型在长上下文任务中的应用潜力。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e4d78b379d4948b1b4b2c58cc1d0a820~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1009&amp;h=482&amp;s=67788&amp;e=png&amp;b=2a2d38\" alt=\"\" loading=\"lazy\"/></p>\\n<h2>方案分析</h2>\\n<p>作者在第四章通过 ablation experiments 来验证设计选择并量化这些选择对最终性能的贡献。</p>\\n<ol>\\n<li><strong>位置编码（Positional Encoding）</strong>：作者发现原有的 LLAMA 2 架构中使用的 <strong>ROPE 位置编码</strong>方法不适合处理超过 4,000-6,000 个令牌的长文本。因此，他们调整了 ROPE 的基频（从 10,000 提升到 500,000）以减小注意力衰减。他们还比较了其他编码方法，如 PI 和 XPOS，并发现调整基频的 ROPE（ROPE ABF）在所有情况下都表现最好。</li>\\n<li><strong>预训练数据组合（Pretraining Data Mix）</strong>：作者尝试了不同的数据组合策略，特别是提高长文本数据的比重。结果显示，调整数据长度分布并没有带来显著的好处，而<strong>数据本身的质量才是关键</strong>。</li>\\n<li><strong>指令微调（Instruction Tuning）</strong>：对预训练的模型进行了多种微调策略，并发现即使<strong>仅使用短指令数据进行微调</strong>，模型在长上下文任务上的表现也明显优于 LLAMA 2。</li>\\n<li><strong>训练过程（Training Curriculum）</strong>：作者探讨了是否从头开始用长序列进行预训练会比持续预训练更有效。结果表明，通过<strong>从短上下文模型开始的持续预训练</strong>，可以在几乎不损失性能的情况下节省约 40% 的计算资源。</li>\\n</ol>\\n<p>这一章我理解是这篇论文最核心的一节了，结合论文最后的附录部分可以深入研究论文提出的算法策略以及带来的价值。</p>\\n<h2>安全问题</h2>\\n<p>虽然在不同下游任务上表现出色，大型语言模型却容易产生有害、误导和偏见的内容。作者评估了模型在不同安全基准上的性能并对模型进行的红队测试以识别可能的安全风险。</p>\\n<p>作者进行了三种安全基准的测试：TruthfulQA、ToxiGen和BOLD。</p>\\n<ul>\\n<li><strong>TruthfulQA</strong>：用于评估模型输出内容的事实性。该基准包括817个问题，覆盖38个不同领域，包括健康、法律、财务和政治。</li>\\n<li><strong>ToxiGen</strong>：用于测量模型输出的毒性，特别是针对13个少数群体的有毒和仇恨生成内容。</li>\\n<li><strong>BOLD</strong>：用于量化模型对不同人口群体的偏见程度。该数据集涵盖种族、性别、宗教、政治意识形态和职业等多个领域。</li>\\n</ul>\\n<p>研究结果表明，指令微调模型在与其他开源模型（如Falcon-instruct和MPT-instruct）以及专有模型（如GPT-3.5, GPT-4）比较时，具有相对较高的安全性和较低的偏见。</p>\\n<p>此外，为了进一步评估模型在长上下文场景中的安全性，作者还进行了红队测试（一种安全评估方法，用于识别组织信息系统、网络或物理环境中存在的脆弱点）。测试的主要目的是通过提供长篇上下文和敌对提示来尝试攻击模型，这些提示涵盖了多个风险领域，如非法行为、恶意行为和提供不合格的建议。结果表明，与其他模型（如LLAMA 2 CHAT）相比，这些指令微调模型没有显示出明显的安全风险。</p>\\n<h2>限制</h2>\\n<p>随着论文提出的模型很厉害，但还是存在一些限制：</p>\\n<ul>\\n<li><strong>功能限制</strong>：本文提出的模型还未针对广泛的长上下文应用进行细致调优，比如需要长篇输出的创意写作。现有的对齐方法，比如 RLHF，应用在不同场景下成本高且非常复杂。甚至熟练的标注人员也可能难以应对密集文本中的错综复杂的细节。在这方面，我们认为，开发针对长 LLMs 的高效对齐方法是未来研究的有价值方向。</li>\\n<li><strong>分词器效率</strong>：尽管此模型系列能处理多达 32,768 个 token 的上下文，但实际上模型能处理的单词数量很大程度上受到分词器行为的影响。Llama 系列的分词器词汇量较小（32k ），并且通常会产生比 GPT-3.5 的分词器更长的序列—我们发现我们的分词器通常平均多产生 10% 的token。此外，我们使用的分词器也不能有效处理空格，导致处理长代码数据的效率不高。</li>\\n<li><strong>幻觉问题</strong>：和其他 LLMs 一样，我们在测试模型时也发现了幻觉问题。虽然短上下文模型常常面临这个问题，但长上下文模型由于消耗大量信息和对齐过程不足，解决这个问题的难度可能更大。</li>\\n</ul>\\n<h2>总结</h2>\\n<p>总的来说，LLaMA 2 Long 对于大语言模型的长上下文问题的研究是非常振奋人心的，从实际测试结果上，在短上下文和长上下文任务上都超过了 LLAMA 2。与现有的开源长上下文模型相比，此模型表现也是更优的，并且在一套长上下文任务上，通过简单的指令微调程序（无人监督），它的表现也优于 gpt-3.5-turbo-16k 。</p>\\n<p>这篇论文一经发出，在网上如reddit、X上都引起了很多网友的讨论，有的不禁感叹 Meta is really on a roll.</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c5a7f87921c46d281dec954368289da~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=683&amp;h=207&amp;s=27534&amp;e=png&amp;b=191919\" alt=\"\" loading=\"lazy\"/></p>\\n<p>也有对此表示质疑的。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1cb0ad111ce94b97865ccfc2aeb81df0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=677&amp;h=306&amp;s=60636&amp;e=png&amp;b=191919\" alt=\"\" loading=\"lazy\"/></p>\\n<p>目前官方还没有确切说明是否会发布这个版本，但最近大模型的迭代这么快，我想后续肯定还会不断有重磅的更新发出的！</p>', 'author': 'CareyWYR', 'reading_time': 11, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 67, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '0b4450e662c6d2582e5d30bf9f094ff6c2509c6a073a106c3053db7103db7c98', 'title': '基于weka的数据库挖掘➖聚类方法K-Means算法', 'url': 'https://juejin.cn/post/7283806770946605117', 'published_at': '2023-09-30T09:20:44Z', 'created_at': '2023-10-07T09:03:25.126918Z', 'changed_at': '2023-10-07T09:03:25.126918Z', 'content': '<h1>基于weka的数据库挖掘➖聚类方法K-Means算法</h1>\\n<h2>关于作者</h2>\\n<ul>\\n<li>作者介绍</li>\\n</ul>\\n\\n<blockquote>\\n<p>🍓 <strong>博客主页：<a href=\"https://juejin.cn/user/2894361621692792/posts\" title=\"https://juejin.cn/user/2894361621692792/posts\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">作者主页</a></strong><br/>\\n🍓 <strong>简介：JAVA领域优质创作者🥇、一名初入职场小白🎓、曾在校期间参加各种省赛、国赛，斩获一系列荣誉</strong>🏆<br/>\\n🍓 <strong>关注我：关注我学习资料、文档下载统统都有，每日定时更新文章，励志做一名JAVA资深程序猿</strong>👨\\u200d💻</p>\\n</blockquote>\\n\\n<h2>目标</h2>\\n<p>1.掌握k-Means算法的原理和聚类过程</p>\\n<p>2.可以使用k-Means算法实现对给定样本集的聚类。</p>\\n<h2>内容</h2>\\n<p>采用k-Means算法,对给出的15个样本数据进行聚类，聚类簇数可自由调整，最后输出簇数为2、3、5的聚类结果。</p>\\n<h2>k-Means初识</h2>\\n<p>k-Means算法是一种常用的聚类算法，用于将一组数据点划分为k个不同的簇（clusters）。每个簇都由其内部的数据点相似性特征而定义，而不同簇之间的数据点则具有较大的差异性。</p>\\n<h3>k-Means步骤</h3>\\n<ol>\\n<li>初始化：在开始时，我们需要选择聚类的数量k，并且随机选择k个数据点作为初始聚类中心。</li>\\n<li>数据点分配：对于数据集中的每一个数据点，我们会计算它与每个聚类中心的距离。然后，根据距离最近的聚类中心，将这些数据点分配到相应的簇中。</li>\\n<li>聚类中心更新：在每一次迭代中，我们会重新计算每个簇的聚类中心。这个新的聚类中心是该簇所有数据点的均值。</li>\\n</ol>\\n<p>这个过程会反复进行，直到聚类中心不再发生变化，或者达到了预设的迭代次数。K-Means算法的目标是最小化每个数据点到其所属簇中心的距离之和，从而优化聚类的结果。</p>\\n<p>聚类的结果是一组簇，每个簇由其内部数据点的相似性特征定义，而不同簇之间的数据点则具有较大的差异性。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/52358b83b9464f5dbc59cb6975dfe744~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=735&amp;h=745&amp;s=72117&amp;e=png&amp;b=ffffff\" alt=\"image-20230928115838812\" loading=\"lazy\"/></p>\\n<p>这个过程通过不断迭代来优化聚类结果，直到收敛到一个局部最优解。最终的结果是一组簇，每个簇表示一个聚类，数据点在不同的簇中根据相似性进行分组。</p>\\n<h3>具体实现</h3>\\n<p><strong>代码如下</strong>：</p>\\n<pre><code>import java.util.*;\\n \\npublic class Km {\\n\\tprivate ArrayList&lt;double[]&gt; dataSet; \\n\\tprivate ArrayList&lt;double[]&gt; center; \\n\\tprivate ArrayList&lt;ArrayList&lt;double[]&gt;&gt; cluster; \\n\\tprivate int k; \\n\\tprivate int m; \\n\\tprivate int dataSetLength; \\n\\tprivate ArrayList&lt;Double&gt; wc; \\n \\n\\tpublic Km(int k) { \\n\\t\\tif (k &lt; 1)\\n\\t\\t\\tk = 1;\\n\\t\\tthis.k = k;\\n\\t\\tdataSet = new ArrayList&lt;double[]&gt;();\\n\\t\\tcenter = new ArrayList&lt;double[]&gt;();\\n\\t\\tcluster = new ArrayList&lt;ArrayList&lt;double[]&gt;&gt;();\\n\\t\\tm = 0;\\n\\t\\tdataSetLength = 0;\\n\\t\\twc = new ArrayList&lt;Double&gt;();\\n\\t}\\n \\n\\tprivate ArrayList&lt;ArrayList&lt;double[]&gt;&gt; getCluster() { \\n\\t\\treturn cluster;\\n\\t}\\n \\n\\tprivate void init() { \\n\\t\\tif (dataSet == null || dataSet.size() == 0) {\\n\\t\\t\\tdouble[][] dataSetArray = new double[][] { { 8, 2 }, { 3, 4 },\\n\\t\\t\\t\\t\\t{ 2, 5 }, { 4, 2 }, { 7, 3 }, { 6, 2 }, { 4, 7 }, { 6, 3 },\\n\\t\\t\\t\\t\\t{ 5, 3 }, { 6, 3 }, { 6, 9 }, { 1, 6 }, { 3, 9 }, { 4, 1 },\\n\\t\\t\\t\\t\\t{ 8, 6 } };\\n\\t\\t\\tfor (int i = 0; i &lt; dataSetArray.length; i++) {\\n\\t\\t\\t\\tdataSet.add(dataSetArray[i]);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tdataSetLength = dataSet.size();\\n\\t\\tif (k &gt; dataSetLength) {\\n\\t\\t\\tk = dataSetLength;\\n\\t\\t}\\n\\t\\tcenter = initCenter();\\n\\t\\tcluster = initCluster();\\n\\t}\\n \\n\\tprivate ArrayList&lt;double[]&gt; initCenter() {\\n\\t\\tArrayList&lt;double[]&gt; center = new ArrayList&lt;double[]&gt;();\\n\\t\\tint[] randoms = new int[k];\\n\\t\\tboolean flag;\\n\\t\\tRandom random = new Random();\\n\\t\\tint temp = random.nextInt(dataSetLength);\\n\\t\\trandoms[0] = temp;\\n\\t\\tfor (int i = 1; i &lt; k; i++) {\\n\\t\\t\\tflag = true;\\n\\t\\t\\twhile (flag) {\\n\\t\\t\\t\\ttemp = random.nextInt(dataSetLength);\\n\\t\\t\\t\\tint j = 0;\\n\\t\\t\\t\\twhile (j &lt; i) {\\n\\t\\t\\t\\t\\tif (temp == randoms[j]) {\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tj++;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif (j == i) {\\n\\t\\t\\t\\t\\tflag = false;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\trandoms[i] = temp;\\n\\t\\t}\\n\\t\\tfor (int i = 0; i &lt; k; i++) {\\n\\t\\t\\tcenter.add(dataSet.get(randoms[i]));\\n\\t\\t\\t\\n\\t\\t}\\n\\t\\treturn center;\\n\\t}\\n \\n\\tprivate ArrayList&lt;ArrayList&lt;double[]&gt;&gt; initCluster() {\\n\\t\\t\\n\\t\\tfor (int i = 0; i &lt; k; i++) {\\n\\t\\t\\tArrayList&lt;double[]&gt; clusters = new ArrayList&lt;double[]&gt;();\\n\\t\\t\\tcluster.add(clusters);\\n\\t\\t}\\n\\t\\treturn cluster;\\n\\t}\\n \\n\\tprivate double distance(double[] point, double[] center) {\\n\\t\\tdouble x = point[0] - center[0];\\n\\t\\tdouble y = point[1] - center[1];\\n\\t\\tdouble distance = x * x + y * y;\\n\\t\\treturn distance;\\n\\t}\\n \\n\\tprivate int minDistance(double[] distance) {\\n\\t\\tdouble minDistance = distance[0];\\n\\t\\tint minLocation = 0;\\n\\t\\tfor (int i = 1; i &lt; k; i++) {\\n\\t\\t\\tif (minDistance &gt; distance[i]) {\\n\\t\\t\\t\\tminDistance = distance[i];\\n\\t\\t\\t\\tminLocation = i;\\n\\t\\t\\t} else if (distance[i] == minDistance) \\n\\t\\t\\t{\\n\\t\\t\\t\\tRandom random = new Random();\\n\\t\\t\\t\\tif (random.nextInt(10) &lt; 5) {\\n\\t\\t\\t\\t\\tminLocation = i;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\treturn minLocation;\\n\\t}\\n \\n\\tprivate void setCluster() { \\n\\t\\tdouble[] dist = new double[k];\\n\\t\\t\\n\\t\\tfor (int i = 0; i &lt; dataSetLength; i++) {\\n\\t\\t\\tfor (int j = 0; j &lt; k; j++) {\\n\\t\\t\\t\\tdist[j] = distance(dataSet.get(i), center.get(j));\\n\\t\\t\\t}\\n\\t\\t\\t\\n\\t\\t\\tcluster.get(minDistance(dist)).add(dataSet.get(i));\\n\\t\\t}\\n\\t}\\n \\n\\tprivate ArrayList&lt;double[]&gt; updateCenter() {\\n\\t\\t\\n\\t\\t\\n\\t\\tfor (int i = 0; i &lt; k; i++) {\\n\\t\\t\\tdouble[] newCenter = new double[2];\\n\\t\\t\\tint n = cluster.get(i).size();\\n\\t\\t\\tif (n != 0) {\\n\\t\\t\\t\\tfor (int j = 0; j &lt; n; j++) {\\n\\t\\t\\t\\t\\tnewCenter[0] += cluster.get(i).get(j)[0];\\n\\t\\t\\t\\t\\tnewCenter[1] += cluster.get(i).get(j)[1];\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tnewCenter[0] = newCenter[0]/n;\\n\\t\\t\\t\\tnewCenter[1] = newCenter[1]/n;\\n\\t\\t\\t\\tcenter.set(i, newCenter);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\treturn center;\\n\\t\\n\\t}\\n \\n\\tprivate void errorSquare() {   \\n\\t\\tdouble errorValue = 0;\\n\\t\\tfor (int i = 0; i &lt; k; i++) {\\n\\t\\t\\tfor (int j = 0; j &lt; cluster.get(i).size(); j++) {\\n \\n\\t\\t\\t\\terrorValue += distance(cluster.get(i).get(j), center.get(i));\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\twc.add(errorValue);\\n\\t}\\n \\n\\tprivate void kmeans() {\\n\\t\\tinit();\\n\\t\\twhile (true) {\\n\\t\\t\\t\\n\\t\\t\\tsetCluster();\\n\\t\\t\\terrorSquare();\\n\\t\\t\\tif (m != 0) {\\n\\t\\t\\t\\tif (wc.get(m) - wc.get(m - 1) == 0)\\n\\t\\t\\t\\t\\tSystem.out.println(m);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tcenter = updateCenter();\\n\\t\\t\\tm++;\\n\\t\\t\\tcluster.clear();\\n\\t\\t\\tcluster = initCluster();\\n\\t\\t}\\n\\t}\\n \\n\\tpublic void printDataArray(ArrayList&lt;double[]&gt; dataArray,\\n\\t\\t\\tString dataArrayName) {\\n\\t\\tfor (int i = 0; i &lt; dataArray.size(); i++) {\\n\\t\\t\\tSystem.out.println(&#34;print:&#34; + dataArrayName + &#34;[&#34; + i + &#34;]={&#34;\\n\\t\\t\\t\\t\\t+ dataArray.get(i)[0] + &#34;,&#34; + dataArray.get(i)[1] + &#34;}&#34;);\\n\\t\\t}\\n\\t\\tSystem.out.println(&#34;===================================&#34;);\\n\\t}\\n \\n\\tprivate void execute() {\\n\\t\\tlong startTime = System.currentTimeMillis();\\n\\t\\tSystem.out.println(&#34;kmeans begins&#34;);\\n\\t\\tkmeans();\\n\\t\\tlong endTime = System.currentTimeMillis();\\n\\t\\tSystem.out.println(&#34;kmeans running time=&#34; + (endTime - startTime)\\n\\t\\t\\t\\t+ &#34;ms&#34;);\\n\\t\\tSystem.out.println(&#34;kmeans ends&#34;);\\n\\t\\tSystem.out.println();\\n\\t}\\n \\n\\tpublic static void main(String[] args) {\\n\\t\\tKm kl = new Km(3);\\n\\t\\tkl.execute();\\n\\t\\t\\n\\t\\tArrayList&lt;ArrayList&lt;double[]&gt;&gt; cluster = kl.getCluster();\\n\\t\\t\\n\\t\\tfor (int i = 0; i &lt; cluster.size(); i++) {\\n\\t\\t\\tkl.printDataArray(cluster.get(i), &#34;cluster[&#34; + i + &#34;]&#34;);\\n\\t\\t}\\n\\t}\\n}\\n</code></pre>\\n<p><strong>运行结果</strong>：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2a7f072efa5b493a894a4214a4e7549f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=531&amp;h=808&amp;s=100207&amp;e=png&amp;b=2b2b2b\" alt=\"image-20230930170716907\" loading=\"lazy\"/></p>\\n<h3>优缺点</h3>\\n<p><strong>优点</strong></p>\\n<ol>\\n<li>简单而高效：k-Means算法是一种简单且易于实现的聚类算法。它的计算效率通常很高，特别是对于大规模数据集。</li>\\n<li>可扩展性：k-Means算法可以处理大规模数据集，并且在处理大量数据时仍能保持较好的性能。</li>\\n<li>可解释性：k-Means算法产生的聚类结果相对直观，每个数据点都被分配到与其最近的聚类中心所对应的簇中。</li>\\n<li>并行化能力：由于独立计算每个簇的聚类中心，k-Means算法在一定程度上可以并行化，从而加速计算过程。</li>\\n</ol>\\n<p><strong>缺点</strong></p>\\n<ol>\\n<li>需要预先指定簇的数量：k-Means算法在运行之前需要指定要生成的簇的数量k。选择不合适的k值可能会导致聚类结果不佳。</li>\\n<li>对初始聚类中心敏感：k-Means算法对初始聚类中心的选择非常敏感。不同的初始聚类中心可能会导致不同的聚类结果。</li>\\n<li>对噪声和离群点敏感：k-Means算法对噪声和离群点非常敏感。它可能会将它们错误地分配到某个簇中，从而影响聚类结果的准确性。</li>\\n<li>仅适用于凸形状的簇：k-Means算法假定簇是凸形状的，对于非凸形状的簇划分效果可能不佳。</li>\\n<li>需要提前确定距离度量：k-Means算法需要使用距离度量来衡量数据点之间的相似性，因此需要在运行之前选择适当的距离度量方法。</li>\\n</ol>\\n<p>k-Means算法是一种常用的聚类算法，广泛应用于数据挖掘、模式识别、图像分割等领域，它通过迭代优化来将数据点划分为不同的簇，根据数据点之间的相似性进行聚类。</p>', 'author': 'java厂长', 'reading_time': 13, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 66, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'ee09167d4d9f7b8c99ab8ccb05c58ff461c4751288a90fb0f922971336ae13ca', 'title': '大模型参数高效微调技术实战（三）-P-Tuning', 'url': 'https://juejin.cn/post/7283806102859317311', 'published_at': '2023-09-30T01:55:34Z', 'created_at': '2023-10-07T09:03:25.123415Z', 'changed_at': '2023-10-07T09:03:25.123415Z', 'content': '<p>随着，ChatGPT 迅速爆火，引发了大模型的时代变革。然而对于普通大众来说，进行大模型的预训练或者全量微调遥不可及。由此，催生了各种参数高效微调技术，让科研人员或者普通开发者有机会尝试微调大模型。</p>\\n<p>因此，该技术值得我们进行深入分析其背后的机理，之前分享了<strong>大模型参数高效微调技术原理综述</strong>的文章。下面给大家分享<strong>大模型参数高效微调技术实战</strong>系列文章，该系列共六篇文章，相关代码均放置在GitHub：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fliguodongiot%2Fllm-action%2Ftree%2Fmain%2Ftrain%2Fpeft\" title=\"https://github.com/liguodongiot/llm-action/tree/main/train/peft\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">llm-action</a>。</p>\\n<ul>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F651744834\" title=\"https://zhuanlan.zhihu.com/p/651744834\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（一）-PEFT概述及环境搭建</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F646748939\" title=\"https://zhuanlan.zhihu.com/p/646748939\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（二）-Prompt Tuning</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F646876256\" title=\"https://zhuanlan.zhihu.com/p/646876256\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（三）-P-Tuning</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F648156780\" title=\"https://zhuanlan.zhihu.com/p/648156780\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（四）-Prefix Tuning / P-Tuning v2</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F649315197\" title=\"https://zhuanlan.zhihu.com/p/649315197\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（五）-LoRA</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F649707359\" title=\"https://zhuanlan.zhihu.com/p/649707359\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（六）-IA3</a></li>\\n</ul>\\n<p>本文为大模型参数高效微调技术实战的第三篇。</p>\\n<h2>P-Tuning 简述</h2>\\n<p>P-Tuning（论文：<strong>GPT Understands, Too</strong>），该方法将 Prompt 转换为可以学习的 Embedding 层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f2a641f10fe24cb8b3775e103d35f51f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>相比Prefix\\xa0Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4d3010fa455b41ca9627bcec3ce9f4ab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>经过预训练的LM的词嵌入已经变得高度离散，如果随机初始化virtual token，容易优化到局部最优值，而这些virtual token理论是应该有相关关联的。因此，作者通过实验发现用一个<strong>提示编码器</strong>（即用一个LSTM+MLP去编码这些virtual token以后，再输入到模型）来编码会收敛更快，效果更好。</p>\\n<p>更加详细的介绍可参考之前的文章：<a href=\"https://juejin.cn/post/7242290406636781626\" title=\"https://juejin.cn/post/7242290406636781626\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2</a></p>\\n<h2>P-Tuning 微调实战</h2>\\n<p>为了不影响阅读体验，详细的代码放置在GitHub：llm-action 项目中 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fliguodongiot%2Fllm-action%2Fblob%2Fmain%2Ftrain%2Fpeft%2Fclm%2Fpeft_p_tuning_clm.ipynb\" title=\"https://github.com/liguodongiot/llm-action/blob/main/train/peft/clm/peft_p_tuning_clm.ipynb\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">peft_p_tuning_clm.ipynb</a> 文件，这里仅列出关键步骤。</p>\\n<p>第一步，引进必要的库，如：P-Tuning 配置类 <code>PromptEncoderConfig</code>。</p>\\n<pre><code>from peft import (\\n    get_peft_config,\\n    get_peft_model,\\n    get_peft_model_state_dict,\\n    set_peft_model_state_dict,\\n    PeftType,\\n    TaskType,\\n    PromptEncoderConfig,\\n)\\n</code></pre>\\n<p>第二步，创建 P-Tuning 微调方法对应的配置。</p>\\n<pre><code>peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=20, encoder_hidden_size=128)\\n</code></pre>\\n<p>P-tuning 使用提示编码器（PromptEncoder）来优化提示参数，因此，您需要使用如下几个参数初始化 PromptEncoderConfig：</p>\\n<ul>\\n<li>task_type：训练的任务类型，如：序列分类（SEQ_CLS），因果语言建模（CAUSAL_LM）等。</li>\\n<li>num_virtual_tokens：虚拟token的数量，换句话说就是提示（prompt）。</li>\\n<li>encoder_hidden_size：编码器的隐藏大小，用于优化提示参数。</li>\\n<li>encoder_reparameterization_type：指定如何重新参数化提示编码器，可选项有：MLP 或 LSTM，默认值为 MLP。</li>\\n</ul>\\n<p>当使用 LSTM 时， 提示编码器结构如下：</p>\\n<pre><code>(prompt_encoder): ModuleDict(\\n    (default): PromptEncoder(\\n      (embedding): Embedding(20, 1024)\\n      (lstm_head): LSTM(1024, 128, num_layers=2, batch_first=True, bidirectional=True)\\n      (mlp_head): Sequential(\\n        (0): Linear(in_features=256, out_features=256, bias=True)\\n        (1): ReLU()\\n        (2): Linear(in_features=256, out_features=1024, bias=True)\\n      )\\n    )\\n  )\\n</code></pre>\\n<p>当使用 MLP 时， 提示编码器结构如下：</p>\\n<pre><code>(prompt_encoder): ModuleDict(\\n    (default): PromptEncoder(\\n      (embedding): Embedding(20, 1024)\\n      (mlp_head): Sequential(\\n        (0): Linear(in_features=1024, out_features=128, bias=True)\\n        (1): ReLU()\\n        (2): Linear(in_features=128, out_features=128, bias=True)\\n        (3): ReLU()\\n        (4): Linear(in_features=128, out_features=1024, bias=True)\\n      )\\n    )\\n  )\\n</code></pre>\\n<p>PEFT 中的 P-tuning 的提示编码器是基于英伟达的<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2FNeMo%2Fblob%2Fmain%2Fnemo%2Fcollections%2Fnlp%2Fmodules%2Fcommon%2Fprompt_encoder.py\" title=\"https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/prompt_encoder.py\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">NeMo</a>库中 prompt_encoder.py 进行的重构，源码如下所示。</p>\\n<pre><code>class PromptEncoder(torch.nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.token_dim = config.token_dim\\n        self.input_size = self.token_dim\\n        self.output_size = self.token_dim\\n        self.hidden_size = config.encoder_hidden_size\\n        self.total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\\n        self.encoder_type = config.encoder_reparameterization_type\\n\\n        \\n        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim)\\n        if not config.inference_mode:\\n            \\n            if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\\n                lstm_dropout = config.encoder_dropout\\n                num_layers = config.encoder_num_layers\\n                \\n                self.lstm_head = torch.nn.LSTM(\\n                    input_size=self.input_size,\\n                    hidden_size=self.hidden_size,\\n                    num_layers=num_layers,\\n                    dropout=lstm_dropout,\\n                    bidirectional=True,\\n                    batch_first=True,\\n                )\\n\\n                self.mlp_head = torch.nn.Sequential(\\n                    torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\\n                    torch.nn.ReLU(),\\n                    torch.nn.Linear(self.hidden_size * 2, self.output_size),\\n                )\\n\\n            elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\\n                warnings.warn(\\n                    f&#34;for {self.encoder_type}, the `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.&#34;\\n                )\\n                layers = [\\n                    torch.nn.Linear(self.input_size, self.hidden_size),\\n                    torch.nn.ReLU(),\\n                    torch.nn.Linear(self.hidden_size, self.hidden_size),\\n                    torch.nn.ReLU(),\\n                    torch.nn.Linear(self.hidden_size, self.output_size),\\n                ]\\n                self.mlp_head = torch.nn.Sequential(*layers)\\n\\n            else:\\n                raise ValueError(&#34;Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.&#34;)\\n\\n    def forward(self, indices):\\n        input_embeds = self.embedding(indices)\\n        if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\\n            output_embeds = self.mlp_head(self.lstm_head(input_embeds)[0])\\n        elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\\n            output_embeds = self.mlp_head(input_embeds)\\n        else:\\n            raise ValueError(&#34;Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.&#34;)\\n\\n        return output_embeds\\n</code></pre>\\n<p>第三步，通过调用\\xa0<code>get_peft_model</code>\\xa0方法包装基础的 Transformer 模型。</p>\\n<pre><code>model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\\nmodel = get_peft_model(model, peft_config)\\nmodel.print_trainable_parameters()\\n</code></pre>\\n<p>通过 print_trainable_parameters 方法可以查看可训练参数的数量(仅为300,288)以及占比（仅为0.05366%）。</p>\\n<pre><code>trainable params: 300,288 || all params: 559,514,880 || trainable%: 0.05366935013417338\\n</code></pre>\\n<p>第四步，模型训练的其余部分均无需更改，当模型训练完成之后，保存高效微调部分的模型权重以供模型推理即可。</p>\\n<pre><code>peft_model_id = f&#34;{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}&#34;\\nmodel.save_pretrained(peft_model_id)\\n</code></pre>\\n<p>输出的模型权重文件如下所示：</p>\\n<pre><code>/data/nfs/llm/model/bloomz-560m_P_TUNING_CAUSAL_LM\\n├── [ 451]  adapter_config.json\\n├── [ 81K]  adapter_model.bin\\n└── [ 129]  README.md\\n\\n0 directories, 3 files\\n</code></pre>\\n<p>注意：这里只会保存经过训练的增量 PEFT 权重。其中，<code>adapter_config.json</code> 为 P-Tuning 配置文件；<code>adapter_model.bin</code> 为 P-Tuning 权重文件。</p>\\n<p>第五步，加载微调后的权重文件进行推理。</p>\\n<pre><code>from peft import PeftModel, PeftConfig\\n\\npeft_model_id = f&#34;{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}&#34;\\nconfig = PeftConfig.from_pretrained(peft_model_id)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\\n\\nmodel = PeftModel.from_pretrained(model, peft_model_id)\\n\\n\\ninputs = tokenizer(f&#39;{text_column} : {dataset[&#34;test&#34;][i][&#34;Tweet text&#34;]} Label : &#39;, return_tensors=&#34;pt&#34;)\\n\\n\\noutputs = model.generate(\\n        input_ids=inputs[&#34;input_ids&#34;], \\n        attention_mask=inputs[&#34;attention_mask&#34;], \\n        max_new_tokens=10, \\n        eos_token_id=3\\n    )\\n\\n\\nprint(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\\n</code></pre>\\n<p>至此，我们完成了 P-Tuning 的训练及推理。</p>\\n<h2>结语</h2>\\n<p>本文对 P-Tuning 的基本原理进行了简述；同时，讲解了使用 P-Tuning 微调技术进行模型训练及推理。下文将对 Prefix Tuning / P-Tuning v2 技术进行实战讲解。</p>\\n<p>如果觉得我的文章能够能够给您带来帮助，期待您的点赞收藏加关注~~</p>', 'author': '吃果冻不吐果冻皮', 'reading_time': 14, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 65, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '2035aab1b7d226aeda511780650d87e2443366a902336fec2680064f76447046', 'title': '大模型参数高效微调技术实战（二）-Prompt Tuning', 'url': 'https://juejin.cn/post/7283791013625643020', 'published_at': '2023-09-30T01:55:00Z', 'created_at': '2023-10-07T09:03:25.119707Z', 'changed_at': '2023-10-07T09:03:25.119707Z', 'content': '<p>随着，ChatGPT 迅速爆火，引发了大模型的时代变革。然而对于普通大众来说，进行大模型的预训练或者全量微调遥不可及。由此，催生了各种参数高效微调技术，让科研人员或者普通开发者有机会尝试微调大模型。</p>\\n<p>因此，该技术值得我们进行深入分析其背后的机理，之前分享了<strong>大模型参数高效微调技术原理综述</strong>的文章。下面给大家分享<strong>大模型参数高效微调技术实战</strong>系列文章，该系列共六篇文章，相关代码均放置在GitHub：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fliguodongiot%2Fllm-action%2Ftree%2Fmain%2Ftrain%2Fpeft\" title=\"https://github.com/liguodongiot/llm-action/tree/main/train/peft\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">llm-action</a>。</p>\\n<ul>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F651744834\" title=\"https://zhuanlan.zhihu.com/p/651744834\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（一）-PEFT概述及环境搭建</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F646748939\" title=\"https://zhuanlan.zhihu.com/p/646748939\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（二）-Prompt Tuning</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F646876256\" title=\"https://zhuanlan.zhihu.com/p/646876256\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（三）-P-Tuning</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F648156780\" title=\"https://zhuanlan.zhihu.com/p/648156780\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（四）-Prefix Tuning / P-Tuning v2</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F649315197\" title=\"https://zhuanlan.zhihu.com/p/649315197\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（五）-LoRA</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F649707359\" title=\"https://zhuanlan.zhihu.com/p/649707359\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术实战（六）-IA3</a></li>\\n</ul>\\n<p>本文为大模型参数高效微调技术实战的第二篇。</p>\\n<h2>Prompt Tuning 简述</h2>\\n<p>Prompt Tuning（论文：<strong>The Power of Scale for Parameter-Efficient Prompt Tuning</strong>），该方法可以看作是 Prefix Tuning 的简化版本，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但<strong>只在输入层加入prompt tokens</strong>，并且不需要加入 MLP 进行调整来解决难训练的问题。\\n\\xa0\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/440aa439f9e74e4784d680835a0f4ba1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>更加详细的介绍可参考之前的文章：<a href=\"https://juejin.cn/post/7242217556623179833\" title=\"https://juejin.cn/post/7242217556623179833\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning</a></p>\\n<h2>Prompt Tuning 微调实战</h2>\\n<p>为了不影响阅读体验，详细的代码放置在GitHub：llm-action 项目中 <a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fliguodongiot%2Fllm-action%2Fblob%2Fmain%2Ftrain%2Fpeft%2Fclm%2Fpeft_prompt_tuning_clm.ipynb\" title=\"https://github.com/liguodongiot/llm-action/blob/main/train/peft/clm/peft_prompt_tuning_clm.ipynb\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">peft_prompt_tuning_clm.ipynb</a>文件，这里仅列出关键步骤。</p>\\n<p>第一步，引进必要的库，如：Prompt Tuning 配置类 <code>PromptTuningConfig</code>。</p>\\n<pre><code>from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\\n</code></pre>\\n<p>第二步，创建 Prompt Tuning 微调方法对应的配置。</p>\\n<pre><code>peft_config = PromptTuningConfig(\\n    task_type=TaskType.CAUSAL_LM,\\n    prompt_tuning_init=PromptTuningInit.TEXT,\\n    num_virtual_tokens=8,\\n    prompt_tuning_init_text=&#34;Classify if the tweet is a complaint or not:&#34;,\\n    tokenizer_name_or_path=model_name_or_path,\\n)\\n</code></pre>\\n<p>参数说明：</p>\\n<ul>\\n<li>prompt_tuning_init：提示嵌入的初始化方法。PEFT支持文本（TEXT）和随机（RANDOM）初始化。在原理篇中提到过 Prompt\\xa0token 的初始化方法和长度对于模型性能有影响。与随机初始化和使用样本词汇表初始化相比，Prompt Tuning 采用类标签初始化模型的效果更好。不过随着模型参数规模的提升，这种gap最终会消失。因此，如果需要使用类标签和样本词汇表初始化需指定为TEXT。</li>\\n<li>prompt_tuning_init_text：用于初始化提示嵌入的文本，在使用文本（TEXT）初始化方法时使用。</li>\\n<li>task_type：指定任务类型。如：条件生成任务（SEQ_2_SEQ_LM），因果语言建模（CAUSAL_LM）等。</li>\\n<li>num_virtual_tokens：指定虚拟Token数。在原理篇中，提到过提示虚拟 Token\\xa0的长度在20左右时的表现已经不错（超过20之后，提升Prompt token长度，对模型的性能提升不明显了）；同样的，这个gap也会随着模型参数规模的提升而减小（即对于超大规模模型而言，即使提示虚拟 Token 长度很短，对性能也不会有太大的影响）。</li>\\n</ul>\\n<p>第三步，通过调用\\xa0<code>get_peft_model</code>\\xa0方法包装基础的 Transformer 模型。</p>\\n<pre><code>model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\\nmodel = get_peft_model(model, peft_config)\\nmodel.print_trainable_parameters()\\n</code></pre>\\n<p>通过 print_trainable_parameters 方法可以查看可训练参数的数量(仅为8,192)以及占比（仅为0.00146%）。</p>\\n<pre><code>trainable params: 8,192 || all params: 559,222,784 || trainable%: 0.0014648902430985358\\n</code></pre>\\n<p>Prompt Tuning 模型类结构如下所示：</p>\\n<pre><code>PeftModelForCausalLM(\\n  (base_model): BloomForCausalLM(\\n    (transformer): BloomModel(\\n      (word_embeddings): Embedding(250880, 1024)\\n      (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\\n      (h): ModuleList(\\n        ...\\n      )\\n      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\\n    )\\n    (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\\n  )\\n  (prompt_encoder): ModuleDict(\\n    (default): PromptEmbedding(\\n      (embedding): Embedding(8, 1024)\\n    )\\n  )\\n  (word_embeddings): Embedding(250880, 1024)\\n)\\n</code></pre>\\n<p>从模型类结构可以看到，Prompt Tuning 只在输入层加入 prompt virtual tokens，其他地方均没有变化，具体可查看 PromptEmbedding 的源码。</p>\\n<pre><code>class PromptEmbedding(torch.nn.Module):\\n    def __init__(self, config, word_embeddings):\\n        super().__init__()\\n\\n        total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\\n        \\n        self.embedding = torch.nn.Embedding(total_virtual_tokens, config.token_dim)\\n        \\n        \\n        if config.prompt_tuning_init == PromptTuningInit.TEXT:\\n            from transformers import AutoTokenizer\\n\\n            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path)\\n            init_text = config.prompt_tuning_init_text\\n            init_token_ids = tokenizer(init_text)[&#34;input_ids&#34;]\\n            \\n            num_text_tokens = len(init_token_ids)\\n            if num_text_tokens &gt; total_virtual_tokens:\\n                init_token_ids = init_token_ids[:total_virtual_tokens]\\n            elif num_text_tokens &lt; total_virtual_tokens:\\n                num_reps = math.ceil(total_virtual_tokens / num_text_tokens)\\n                init_token_ids = init_token_ids * num_reps\\n            init_token_ids = init_token_ids[:total_virtual_tokens]\\n\\n            word_embedding_weights = word_embeddings(torch.LongTensor(init_token_ids)).detach().clone()\\n            word_embedding_weights = word_embedding_weights.to(torch.float32)\\n            \\n            self.embedding.weight = torch.nn.Parameter(word_embedding_weights)\\n\\n    def forward(self, indices):\\n        \\n        prompt_embeddings = self.embedding(indices)\\n        return prompt_embeddings\\n</code></pre>\\n<p>第四步，模型训练的其余部分均无需更改，当模型训练完成之后，保存高效微调部分的模型权重以供模型推理即可。</p>\\n<pre><code>peft_model_id = f&#34;{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}&#34;\\nmodel.save_pretrained(peft_model_id)\\n</code></pre>\\n<p>输出的模型权重文件如下所示：</p>\\n<pre><code>/data/nfs/llm/model/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\\n├── [ 500]  adapter_config.json\\n├── [ 33K]  adapter_model.bin\\n└── [ 111]  README.md\\n\\n0 directories, 3 files\\n</code></pre>\\n<p>注意：这里只会保存经过训练的增量 PEFT 权重。其中，<code>adapter_config.json</code> 为 Prompt Tuning 配置文件；<code>adapter_model.bin</code> 为 Prompt Tuning 权重文件。</p>\\n<p>第五步，加载微调后的权重文件进行推理。</p>\\n<pre><code>from peft import PeftModel, PeftConfig\\n\\npeft_model_id = f&#34;{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}&#34;\\n\\n\\nconfig = PeftConfig.from_pretrained(peft_model_id)\\n\\n\\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\\n\\nmodel = PeftModel.from_pretrained(model, peft_model_id)\\n\\n\\ninputs = tokenizer(f&#39;{text_column} : {dataset[&#34;test&#34;][i][&#34;Tweet text&#34;]} Label : &#39;, return_tensors=&#34;pt&#34;)\\n\\n\\noutputs = model.generate(\\n        input_ids=inputs[&#34;input_ids&#34;], \\n        attention_mask=inputs[&#34;attention_mask&#34;], \\n        max_new_tokens=10, \\n        eos_token_id=3\\n    )\\n\\n\\nprint(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\\n</code></pre>\\n<p>至此，我们完成了Prompt Tuning的训练及推理。</p>\\n<h2>结语</h2>\\n<p>本文对 Prompt Tuning 的基本原理进行了简述；同时，讲解了使用 Prompt Tuning 技术进行模型训练及推理。下文将对 P-Tuning 微调技术进行实战讲解。</p>\\n<p>如果觉得我的文章能够能够给您带来帮助，期待您的点赞收藏加关注~~</p>', 'author': '吃果冻不吐果冻皮', 'reading_time': 12, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 64, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '85dfc51ef0a0662b2d2bce19dc62fc9d6ddd6d28c0ec7c138972a435a75ab114', 'title': '【机器学习|Python】sklearn中的逻辑回归模型', 'url': 'https://juejin.cn/post/7283780768945799231', 'published_at': '2023-09-29T15:50:10Z', 'created_at': '2023-10-07T09:03:25.116757Z', 'changed_at': '2023-10-07T09:03:25.116757Z', 'content': '<h1>前言</h1>\\n<p>本文主要说明 Python 的 sklearn 库中的随机森林模型的常用接口、属性以及参数调优说明。需要读者或多或少了解过sklearn库和一些基本的机器学习知识。</p>\\n<ul>\\n<li>更多关于逻辑回归的原理知识：<a href=\"https://juejin.cn/post/7283690681175752716\" title=\"https://juejin.cn/post/7283690681175752716\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">【机器学习】LogisticRegression-逻辑回归中的数学原理 - 掘金 (juejin.cn)</a></li>\\n</ul>\\n<h1>sklearn中的逻辑回归</h1>\\n<p>sklearn中的逻辑回归相关类：</p>\\n<ul>\\n<li>逻辑回归模型：<code>sklearn.linear_model.LogisticRegression</code></li>\\n<li>交叉熵损失（又称对数损失或逻辑损失）：<code>sklearn.metrics.log_loss</code></li>\\n</ul>\\n<h2>基本使用</h2>\\n<p><strong>模型基本使用</strong>（以sklearn中的乳腺癌细胞数据集为例）：</p>\\n<pre><code>from sklearn.datasets import load_breast_cancer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\n\\nbreast_cancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\\n\\nlr = LogisticRegression()  \\nlr.fit(X_train, y_train)  \\n\\nprint(lr.predict(X_test))  \\nprint(lr.score(X_test, y_test))   \\n</code></pre>\\n<h2>常用属性和接口</h2>\\n<ul>\\n<li><code>.coef_</code>：模型中各特征对应的权重系数（或参数）</li>\\n<li><code>.intercept_</code>：逻辑回归中预测函数中的截距（或偏差）</li>\\n<li><code>.n_iter_</code>：模型的实际迭代次数（二分类中仅构建一个模型，返回一个值；多分类模型中构建多个模型，返回多个值）</li>\\n<li><code>lr.classes_</code>：输出数据集中包含的各标签对应的列表</li>\\n<li><code>predict_proba</code>：预测各个样本属于各个标签的概率</li>\\n</ul>\\n<h1>参数说明</h1>\\n<p><strong>默认参数</strong>：</p>\\n<pre><code>LogisticRegression(penalty=&#39;l2&#39;, dual=False, tol=0.0001, C=1.0, \\n                   fit_intercept=True, intercept_scaling=1, class_weight=None, \\n                   random_state=None, solver=&#39;lbfgs&#39;, max_iter=100,\\n                   multi_class=&#39;auto&#39;, verbose=0, warm_start=False, n_jobs=None, \\n                   l1_ratio=None)\\n</code></pre>\\n<p><strong>常用参数</strong>：</p>\\n<ul>\\n<li>penalty：正则化参数，选择正则化类型</li>\\n<li>C：对于引入正则化系数的逻辑回归模型，设置正则化强度</li>\\n<li>solver：模型求解器，选择模型优化算法</li>\\n<li>multi_class：设置分类模式或模型要处理的分类问题的类型</li>\\n</ul>\\n<h2>penalty &amp; C</h2>\\n<p><strong>正则化</strong>：正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量的L1范式或L2范式来实现，其中增加的范式被称为 &#34;正则项&#34;（或惩罚项），基于损失函数的最优化来求解的参数取值由于引入的正则项而发生改变，以此达到调节模型拟合程度的目的（截距一般是不参与正则化的）。</p>\\n<p>① penalty：默认值为 &#39;l2&#39;。</p>\\n<ul>\\n<li><code>&#39;l1&#39;</code>：指定l1正则化</li>\\n<li><code>&#39;l2&#39;</code>：指定l2正则化</li>\\n</ul>\\n<p>② C：默认值为 1.0，即默认正则项与损失函数的比值是 1 : 1；必须是一个大于0的浮点数，表示为正则化强度的倒数；C越大，正则化强度越小，C越小，正则化强度越大。</p>\\n<p><strong>其它说明</strong>：L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），各权重系数（参数）的取值会逐渐变小，但L1正则化会趋于将参数压缩为0，而L2正则化则趋于让参数尽量小，不会取到0，由于这个特性，L1正则化也常用于基于Embedded嵌入法的特征选择。</p>\\n<p><strong>参数设置</strong>：通常情况下一般设置参数penalty 为 &#39;l2&#39;，当进行特征选择时我们一般使用 &#39;l1&#39;，当有效特征数量较少时一般使用 &#39;l2&#39;；对于参数C，可以根据不同情况绘制对应学习曲线观察效果后进行调整。</p>\\n<h2>solver &amp; multi_class</h2>\\n<p>① multi_class：默认值为 &#39;auto&#39;；该参数表示设置分类方式</p>\\n<ul>\\n<li><code>&#39;auto&#39;</code>：根据数据的分类情况和其他参数来确定模型要处理的分类问题的类型；若训练数据为二分类或solver的取值为 &#34;liblinear&#34;，选择&#34;ovr&#34;；反之选择 &#34;multinomial&#34;。</li>\\n<li><code>&#39;ovr&#39;</code>：表示分类问题是二分类，或让模型使用 &#34;一对多&#34; 的形式来处理多分类问题。</li>\\n<li><code>&#39;multinomial&#39;</code>：表示处理多分类问题，让模型使用 &#34;多对多&#34; 的形式来处理多分类问题。</li>\\n</ul>\\n<p>② solver：默认值为&#39;lbfgs&#39;；设置优化算法（模型求解算法）</p>\\n<ul>\\n<li><code>&#39;liblinear&#39;</code>：坐标下降法</li>\\n<li><code>&#39;lbfgs&#39;</code>：拟牛顿法的一种</li>\\n<li><code>&#39;newton-cg&#39;</code>：牛顿法的一种</li>\\n<li><code>&#39;sag&#39;</code>：随机平均梯度下降</li>\\n<li><code>&#39;saga&#39;</code>：随机平均梯度下降的变形</li>\\n</ul>\\n<p><strong>其它说明</strong>：对于solver参数，牛顿法和拟牛顿法相关算法一般使用二阶导数矩阵（海森矩阵）来进行模型迭代，因此若使用 &#39;lbfgs&#39; 或 &#39;newton-cg&#39;，则无法使用l1正则化；随机平均梯度下降若使用l1正则化会产生一些问题，我们的损失函数一般为凸函数，但当使用l1正则化后，可能引发一些例如多个极小值或某些区域不可导的问题，因此传统的梯度下降法一般不适用带l1正则化的情况；即：&#39;lbfgs&#39;、&#39;newton-cg&#39;、&#39;sag&#39; 不能使用 l1 正则化，只能使用 l2 正则化，&#39;liblinear&#39; 和 &#39;saga&#39; l1正则化和l2正则化都可以使用；此外，坐标下降法 &#39;liblinear&#39; 不能用于 &#39;multinomial&#39; 模式下的求解算法，只能用于 &#39;ovr&#39; 模式。</p>\\n<p>对于multi_class，一般使用 &#39;auto&#39;，在二分类情况下，一般 &#39;ovr&#39; 有着更好的效果，相对地，在多分类任务下 &#39;multinomial&#39; 一般有着更好的效果；在多分类任务下，&#39;ovr&#39; 和 &#39;multinomial&#39;，对于 N 分类任务，都会建立 N 个分类器来进行决策，但相对ovr，&#39;multinomial&#39; 在多分类任务上有着更严谨的数学过程，&#39;ovr&#39; 则对数据的质量要求比较高。</p>\\n<p><strong>参数设置</strong>：逻辑回归一般使用 &#39;saga&#39; 作为优化算法，若进行特征选择，由于只能使用l1正则化，因此只能考虑 &#39;liblinear&#39; 或 &#39;saga&#39;，一般使用 &#39;saga&#39;；对于 multi_class，一般使用 &#39;auto&#39;。</p>\\n<h2>其它参数</h2>\\n<p>① max_iter：默认值为100,；设置模型在训练集上的最大迭代次数，当模型在该参数之前已收敛，则会自动停止迭代，若未收敛，则到达该参数时停止迭代。</p>\\n<p>② fit_intercept：布尔值，默认值为True；若为True，则添加截距（偏差）到模型中；若为False，则不包含截距（偏差）。</p>\\n<p>③ random_state：默认为None，随机数生成器为np.random模块下的一个RandomState实例；当模型求解器为 &#39;liblinear&#39; 或 &#39;sag&#39; 时才有效，一般输入整数。</p>\\n<p>④ n_jobs：默认为None；在多分类任务下平行计算所使用的CPU线程数，一般使用 -1，表示使用所有可用的线程数；当参数solver设置为&#39;liblinear&#39;时，忽略此参数。</p>\\n<p>③ class_weight：用于处理样本不平衡的情况，但这个参数我们一般不用，对于这种场景，我们更多地使用上采样或下采样方法来应对，对于逻辑回归来说，上采样一般为首要选择。</p>\\n<h2>总结</h2>\\n<p>在逻辑回归模型中，对于需要手动调参的参数的数量比较少，penalty、solver、multi_class一般通过相应任务和使用场景就可以优先调好，而参数C我们则可以通过画学习曲线的方式来进行调整，但在训练模型之前我们可能会先基于Embedded嵌入法进行特征选择，由于我们要基于加上正则化的模型对特征进行选择，这时我们一般先对参数C绘制学习曲线来优先确定参数C的取值以得到更好的模型，然后基于使用该参数的模型对嵌入法绘制学习曲线以确定保留特征个数。</p>\\n<h1>Reference</h1>\\n<ul>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fscikit-learn.org%2Fstable%2Findex.html\" title=\"https://scikit-learn.org/stable/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">scikit-learn: machine learning in Python — scikit-learn 1.3.1 documentation</a></li>\\n<li>菜菜的sklearn机器学习</li>\\n</ul>', 'author': 'rink1t', 'reading_time': 8, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 63, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '41f5509b4de3040a4bab9a73e080e4be14ed126d990fcf27bd0f8ae0eb8f771d', 'title': 'CVer从0入门NLP（三）———GPT、BERT模型', 'url': 'https://juejin.cn/post/7283699382418915386', 'published_at': '2023-09-28T13:22:09Z', 'created_at': '2023-10-07T09:03:25.110877Z', 'changed_at': '2023-10-07T09:03:25.110877Z', 'content': '<blockquote>\\n<p>本文为稀土掘金技术社区首发签约文章，30天内禁止转载，30天后未获授权禁止转载，侵权必究！</p>\\n</blockquote>\\n<blockquote>\\n<p>🍊作者简介：<a href=\"https://juejin.cn/user/1359414174686455\" title=\"https://juejin.cn/user/1359414174686455\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">秃头小苏</a>，致力于用最通俗的语言描述问题</p>\\n<p>🍊专栏推荐：<a href=\"https://juejin.cn/column/7138749154150809637\" title=\"https://juejin.cn/column/7138749154150809637\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">深度学习网络原理与实战</a></p>\\n<p>🍊近期目标：写好专栏的每一篇文章</p>\\n<p>🍊支持小苏：点赞👍🏼、收藏⭐、留言📩</p>\\n</blockquote>\\n<p>\\xa0</p>\\n<h1>CVer从0入门NLP（三）———GPT、BERT模型</h1>\\n<h2>写在前面</h2>\\n<blockquote>\\n<p>Hello，大家好，我是小苏👦🏽👦🏽👦🏽</p>\\n</blockquote>\\n<p>在上一节中，我为大家介绍了LSTM、ELMO和Transformer模型，其实全都是为今天的内容做准备的，所以说，阅读此篇之前强烈建议大家先看看上节的内容，链接如下：</p>\\n<ul>\\n<li><a href=\"https://juejin.cn/post/7282691710085021753\" title=\"https://juejin.cn/post/7282691710085021753\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">CVer从0入门NLP（二）———LSTM、ELMO、Transformer模型🍁🍁🍁</a></li>\\n</ul>\\n<p>本节将为大家带来大名鼎鼎的GPT和BERT，话不多说，让我们一起走进GPT和BERT的世界。🚀🚀🚀</p>\\n<p>\\xa0</p>\\n<h2>GPT</h2>\\n<p>终于讲到GPT了，我想现在没有人对这玩意陌生的叭，随着22年底ChatGPT的一炮走红，震惊了全世界，可以说是颠覆式的研究成果了。那么GPT的底层原理到底是怎么样的呢？不用急，跟随我的步伐一步步的来学习。🥗🥗🥗</p>\\n<blockquote>\\n<p>如果还有没尝试过ChatGPT的小伙伴，一定要去试试，会极大程度提高你的生产力。注册教程可以点击<a href=\"https://juejin.cn/post/7174029845310439432\" title=\"https://juejin.cn/post/7174029845310439432\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">☞☞☞</a>查看详情。🍄🍄🍄</p>\\n</blockquote>\\n<blockquote>\\n<p>ChatGPT并不是从0-1凭空出现的，而是经过不断的优化改进，最终实现出如此惊艳的效果。本节将为大家介绍初代GPT的结构，后续文章会陆续更新GPT系列的发展史，敬请期待。🍄🍄🍄</p>\\n</blockquote>\\n<p>我们先来看看GPT的全称叫什么，即“Generative Pre-Training”，翻译即生成式的预训练。我们来解释一下这个名称“生成式的预训练”，所谓生成式，表示该模型可以用于文本生成任务；而预训练则表示该模型先通过大规模的文本数据集进行训练，然后再用于下游任务。【这个和计算机视觉中的预训练含义是一样的】🍭🍭🍭</p>\\n<p>上文说到，GPT采用了预训练的方式来训练模型，其主要有两个阶段，如下：</p>\\n<ul>\\n<li>阶段一：利用语言模型进行预训练</li>\\n<li>阶段二：通过Fine-tuning对下游任务进行微调</li>\\n</ul>\\n<p>下图展示了GPT预训练的过程，我们一起来看看：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/11891e0d83754a2da767b0a27de2adc0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1920&amp;h=1080&amp;s=375495&amp;e=png&amp;b=fefefe\" alt=\"img\" loading=\"lazy\"/></p>\\n<p>从上图中我们可以发现GPT的结构是这样的，如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5c46f76f13304ae78245f2ab043a63db~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=350&amp;h=301&amp;s=42584&amp;e=png&amp;b=faf8f8\" alt=\"image-20230926160337527\" loading=\"lazy\"/></p>\\n<p>大家有没有发现这个结构是和前文所述的ELMO模型非常类似的，当然了，也有一些差异，如下：</p>\\n<ol>\\n<li>特征提取器使用的不是LSTM，而是特征提取能力更强的Transformer。【自GPT之后，几乎所有模型都开始使用Transformer架构来进行特征提取】</li>\\n<li>GPT的预训练任务任然是语言模型，但是采用的是单向的语言模型。</li>\\n</ol>\\n<p>大家听了以上两点，可能懂了一点，但也没完全懂，下面我将针对这两点做一个更细致的解释。</p>\\n<blockquote>\\n<p>点1 ：这里使用了Transformer架构，他的具体结构是什么样的呢？我们先来说结论：**GPT中的Transformer结构就是把Encoder中的Multi-Head Attention替换成了Masked Multi-Head Attention。**如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1e431f814bf542769db8c66257637284~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=280&amp;h=378&amp;s=18206&amp;e=png&amp;b=fcfcfc\" alt=\"image-20230926162646057\" loading=\"lazy\"/></p>\\n<p>看到上文的话不知道大家能否理解，我觉得你要是熟悉Transformer结构应该就能够理解了，Transformer结构主体由一个Encoder结构和一个Decoder结构构成，Encoder结构中使用了Multi-Head Attention，而Decoder中使用了Masked Multi-Head Attention。这里具体细节我就不说了，不清楚的可以去看我关于Transformer的博客介绍。我也贴一张Transformerd的结构图，方便大家对比，如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/79ffaec1d4144ff095dfc9d3e29aa838~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"transformer-第 2 页.drawio\" loading=\"lazy\"/></p>\\n<p>大家注意一下我说的是<strong>GPT中的Transformer结构就是把Encoder中的Multi-Head Attention替换成了Masked Multi-Head Attention。</strong>，大家可以对比一下结构，看看我的表述是否正确。网上也有一些说法说是GPT中的Transformer结构就是Transformer中的Decoder结构，其实还是存在一些问题的，因为Decoder结构中采用了两个连续的Masked Multi-Head Attention+LN结构，而GPT的Transformer中只使用了一个。【大家这里注意一下就好，在一些博客和平时交流中知道这么一回事就行，在后文我也会采取GPT采用的是Transformer中Decoder的说法，因为字少哈哈哈。🍄🍄🍄】</p>\\n<p>大家可能还注意到在GPT的Transformer结构中还有一个<code>12×</code>的字眼，其表示这个结构重复12次。</p>\\n<p>为了大家能更深入了解GPT的Transformer结构，可以看一下如下代码：</p>\\n<pre><code>from .attention import tf, MultiHeadAttention\\n\\nclass TransformerBlock(tf.keras.layers.Layer):\\n\\n    def __init__(self, embedding_dimension, num_heads, feed_forward_dimension, dropout_rate=0.1):\\n        super(TransformerBlock, self).__init__()\\n\\n        self.attention = MultiHeadAttention(embedding_dimension, num_heads)\\n        self.feed_forward_network = tf.keras.Sequential([\\n            tf.keras.layers.Dense(feed_forward_dimension, activation=&#39;relu&#39;),\\n            tf.keras.layers.Dense(embedding_dimension)\\n        ])\\n        self.layer_normalization = [\\n            tf.keras.layers.LayerNormalization(epsilon=1e-6),\\n            tf.keras.layers.LayerNormalization(epsilon=1e-6)\\n        ]\\n        self.dropout = [\\n            tf.keras.layers.Dropout(rate=dropout_rate),\\n            tf.keras.layers.Dropout(rate=dropout_rate)\\n        ]\\n\\n    def call(self, inputs):\\n        attention_output = self.dropout[0](self.attention(inputs))\\n        residual_output = self.layer_normalization[0](inputs + attention_output)\\n        feed_forward_output = self.dropout[1](self.feed_forward_network(residual_output))\\n        output = self.layer_normalization[1](residual_output + feed_forward_output)\\n        return output\\n</code></pre>\\n<p>大家可以阅读一下代码，看看是不是和我们上面所说的结构一致呢。需要注意的是上述代码家了一个Dropout结构，对整体的结构不影响。</p>\\n</blockquote>\\n\\n<blockquote>\\n<p>点2：GPT采用的是单向的语言模型是什么意思？语言模型训练的任务目标是根据wiw_iwi\\u200b单词的上下文去正确预测单词wiw_{i}wi\\u200b ， wiw_{i}wi\\u200b之前的单词序列Context-before称为上文，wiw_{i}wi\\u200b之后的单词序列Context-after称为下文。再使用ELMO做语言模型训练的时候，预测单词wiw_{i}wi\\u200b时同时考虑了该单词的上下文，而GPT只采用这个单词的上文来进行预测，抛弃了下文。为什么GPT看不到下文的信息呢，这就是因为GPT的Transformer结构使用了Masked Multi-Head Attention结构，其遮挡住了后面单词的信息，Multi-Head Attention和Masked Multi-Head Attention的区别如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/653d6316c98c49ad9ebfd9dd67018625~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1422&amp;h=476&amp;s=50920&amp;e=png&amp;b=fef6f5\" alt=\"image-20230926172002384\" loading=\"lazy\"/></p>\\n</blockquote>\\n<p>上文为大家介绍的是模型的基本架构，当我们用此模型进行训练就完成了第一阶段的任务，即实现了模型的预训练，那么接下来如何进行第二阶段的任务——通过Fine-tuning对下游任务进行微调呢？如下图所示：</p>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/88e9ae570f654d20b5d47b2d5395fcc4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1920&amp;h=1080&amp;s=474465&amp;e=png&amp;b=fefefe\" alt=\"img\" loading=\"lazy\"/>\\n<p>上图展示了下游任务如何进行微调。在ELMO模型中，其下游任务的网络结构是可以任意设计的，因为其用到的是ELMO模型训练出的词向量；而在GPT中，下游任务的网络结构可不能修改了，必须要和GPT预训练时保持一致，因为GPT在做下游任务时，下游任务的网络结构的初始化参数就是已经训练好的GPT网络的参数，这样你就可以利用预训练好的知识来应对你的下游任务了，相当于开局就送神装了，当你训练你的下游任务时，只需要进行微调就可以了。【这部分其实是很好理解的，就是计算机视觉任务中的迁移学习嘛🥗🥗🥗】</p>\\n<blockquote>\\n<p>论文链接：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fopenai-assets%2Fresearch-covers%2Flanguage-unsupervised%2Flanguage_understanding_paper.pdf\" title=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">GPT1🍁🍁🍁</a></p>\\n</blockquote>\\n<blockquote>\\n<p>本节就先为大家介绍到这里，后续会为大家介绍更多版本的GPT。🍄🍄🍄</p>\\n</blockquote>\\n<p>\\xa0</p>\\n<h2>BERT</h2>\\n<p>这一节来为大家介绍大名鼎鼎的BERT了，我想任何一个NLPer都不会没有听过BERT的大名叭。BERT，其实他是美国少儿节目中的一个人名，他长这样：</p>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a67e84b7ac3f47a683c078e4a53fd8de~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=800&amp;h=600&amp;s=58506&amp;e=jpg&amp;b=35b557\" alt=\"Bert\" loading=\"lazy\"/>\\n<p>所以大家在搜BERT时，可能会经常看到上图这样的一个封面，果然，起名字还是很重要的。说起名字，我们还是先看看BERT的全称叭——<code>Pre-training of Deep Bidirectional Transformers for Language Understanding</code>。🍚🍚🍚</p>\\n<p>从BERT的全称中我们可以看出什么？我觉得有以下两点是比较关键的：</p>\\n<ul>\\n<li><code>Pre-training:</code>说明BERT和GPT一样都是一个预训练的模型</li>\\n<li><code>Deep Bidirectional Transformers:</code>说明BERT采用了一个双向的Transfomer结构</li>\\n</ul>\\n<p>针对以上两点，第一点表明EBRT是一个预训练模型，也即分为预训练阶段+微调阶段，这和GPT的预训练是差不多的，后文我们在简单介绍下。我们主要来看看BERT的结构是怎么样的，如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2958ea1f0ce04460a87277d9b58f02d8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1641&amp;h=967&amp;s=407730&amp;e=png&amp;b=fbf8f8\" alt=\"image-20230927220109689\" loading=\"lazy\"/></p>\\n<p>不知道大家看到这个图有没有熟悉的感觉，要是有那就太好了，没有的话我们就一起来看看。我们上文已经为大家介绍了ELMO模型和GPT模型，下面我们将三个模型放一起看看三者的区别：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e7881e84775e42f0a3adbbdeac802be1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1383&amp;h=333&amp;s=107140&amp;e=png&amp;b=fdfcfc\" alt=\"image-20230927220417940\" loading=\"lazy\"/></p>\\n<p>有没有发现他们三个实在是太像了，在上节介绍GPT时，我们说到，GPT和ELMO非常类似。今天这节的主角是BERT，那我们就来说说BERT和ELMO、GPT的区别和联系。</p>\\n<ul>\\n<li>\\n<p>BERT和ELMO</p>\\n<ul>\\n<li>ELMO采用LSTM作为特征提取器，而BERT采用的是Transformer中的编码器结构。【这个可以从BERT的结构图可以看出，每一个Trm都是一个Transformer的编码器】🥗🥗🥗</li>\\n<li>ELMO采用的是伪双向编码，即使用从左向右和从右向左的两个LSTM网络，它们分别独立训练，最好将训练好的两个方向的编码拼接。【有关ELMO的伪双向编码在ELMO模型那节有详细介绍】而BERT采用的是一个完全的双向编码，即完全可以看到某个单词的前后信息。</li>\\n</ul>\\n</li>\\n<li>\\n<p>BERT和GPT</p>\\n<ul>\\n<li>GPT采用Transformer的解码器作为特征提取器。【这个在GPT小节提到说解码器有些不妥，大家注意一下就好】而BERT采用的是Transformer的编码器作为特征提取器。</li>\\n<li>GPT采用的是单向编码，而BERT采用的是双向编码。</li>\\n</ul>\\n</li>\\n</ul>\\n<p>总的来说，BERT可以说是近年NLP邻域具有里程碑意义的模型，它借鉴了ELMO、GPT等模型的思想（借鉴了ELMO的双向编码、GPT的Transformer结构），是集大成者。</p>\\n<blockquote>\\n<p>读到这里，不知道大家是否会存在一些疑惑，我列一些我能想到的，希望可以帮到大家。</p>\\n<ol>\\n<li>\\n<p>什么是单向编码，什么是双向编码？</p>\\n<p>其实这个很好理解，单向编码就是只考虑一个方向的信息，而双向编码则会考虑两个方向即上下文的信息。我举个例子，对于这句话“今天天气很__，我要去踢足球”，现在要考虑在__填什么词。对于单向编码只会看到__前面的句子，即“今天天气很”，那么此时__填入的可能是“糟糕”、“不错”、“好”等等词；但是对于双向编码来说，它还可以看到后文“我要去踢足球”，那么__里的词应该是积极的，比如“不错”、“好”。从这里可以看出，其实双向编码对于句子的理解能力更好。🥂🥂🥂</p>\\n</li>\\n<li>\\n<p>为什么GPT要采用Transformer的解码器做特征提取，而BERT要采用Transformer的编码器做特征提取？</p>\\n<p>其实它们特征提取器的不同，是因为它们的任务和目标不同。对于GPT来说，其旨在生成自然语言文本，例如生成文章、回答问题、完成句子等。因此，它的任务是基于输入文本的信息生成下一个单词或一段文本。为了完成这一任务，GPT采用了Transformer架构的解码器部分，由于解码器中存在Musk Multi-Head Attention的缘故，使得GPT看不到未来的信息。对于BERT来说，其任务是预训练一个深度双向的语言表示，以便于各种自然语言处理任务的下游任务（如文本分类、命名实体识别、句子关系判断等）。BERT关注的是理解文本的含义和上下文，而不是生成文本。为了实现这一目标，BERT采用了Transformer架构的编码器部分。编码器通过双向处理输入文本，从而更好地理解单词的上下文关系和语境，而不受生成顺序的限制。</p>\\n<blockquote>\\n<p>注：GPT和BERT也没有好坏之分，它们只是处理的任务和目标不同。但是显然GPT的任务其实更难一些，因为其只能看见现有信息，无法看到未来。现在随着ChatGPT的出现，就能发现当时OpenAI团队似乎就在下一盘大棋。</p>\\n</blockquote>\\n</li>\\n</ol>\\n</blockquote>\\n<p>看到这里，我想你对BERT的结构已经比较清楚了，就是一个两层的双向Transformer Encoder嘛。下面我们一起来BERT是怎么进行训练的，当然了，BERT也是预训练模型，分为两个阶段进行训练：</p>\\n<ul>\\n<li>阶段一：使用大规模无标签语料，训练BERT基础语言模型</li>\\n<li>阶段二：对下游任务进行微调</li>\\n</ul>\\n<p>那么BERT是如何训练的呢，它其实实现了两个训练任务，分别是语言掩码模型（MLM）和下句预测（NSP），这也算是BERT的两个创新之处，我们分别来看一下：</p>\\n<ul>\\n<li>\\n<p>语言掩码模型MLM</p>\\n<p>我们上文说到，BERT采用的是双向编码，之所以采用双向编码，是因为作者认为双向编码的性能、参数规模和效率更加优异。但是双向编码会存在see itself的问题呀，就是能看到参考答案。</p>\\n<blockquote>\\n<p>注意：大家回顾一下我在介绍ELMO模型时，是不是也谈及了see itself问题呢？ELMO模型是怎么解决的呢？——其通过的是两个方向彼此独立训练两个LSTM模型后再拼接的方式，实际上是一种伪双向模型，从而避免了see itself的问题。但是BERT可没有采用这种伪双向的方式，而是直接使用Transformer Encoder做一个完全双向的模型，这就会导致BERT存在see itself的问题。🍚🍚🍚</p>\\n<p>大家不要问为什么作者不采用伪双向＋Transformer Decoder的结构，问就是效果没有完全双向的效果好。🥗🥗🥗</p>\\n</blockquote>\\n<p>既然BERT存在see itself的问题，那么他是怎么做的呢？——答案就是MLM（Masked Language Model），什么是MLM呢，其实就是指在训练的时候随机从输入语料上Mask掉一些单词，然后通过的上下文预测该单词。大家有没有觉得非常像CBOW的思想，即完形填空。🍭🍭🍭</p>\\n<p>上述随机被Mask的单词被称为掩码词，在训练过程中，输入数据随机选择15%的词用于预测，即作为掩码词。但是这样的设计会存在一些问题，即在预训练和微调之间造成了不匹配，因为 [MASK] 标记在微调期间不会出现。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/43082f5eaf274916835d827265dfd30c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=314&amp;h=273&amp;s=57916&amp;e=png&amp;b=fefbfb\" alt=\"image-20230928153922201\" loading=\"lazy\"/></p>\\n<p>那么怎样缓解这样的弊端呢，BERT是这样做的：</p>\\n<p>对于随机选择的15%的掩码词再做以下调整</p>\\n<ul>\\n<li>\\n<p>80%的词向量输入时被替换为</p>\\n</li>\\n<li>\\n<p>10%的词的词向量在输入时被替换为其他词的词向量</p>\\n</li>\\n<li>\\n<p>另外10%保持不动</p>\\n</li>\\n</ul>\\n<p>论文中也给出了关于此的小例子，如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/942c113bb0704169b2523e11fc5fea7b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=606&amp;h=466&amp;s=66498&amp;e=png&amp;b=fffefe\" alt=\"image-20230928154338028\" loading=\"lazy\"/></p>\\n<p>这样做的好处是我告诉模型，句子可能是对的，也可能是错的，也可能是被Mask的，有的地方你需要预测，没有的地方你也需要判断是否正确，也就是说，模型需要预测所有位置的输出。🍦🍦🍦</p>\\n</li>\\n<li>\\n<p>下句预测（NSP）</p>\\n<p>NSP全称为“Next Sentence Prediction”，即下句预测，其任务是判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>\\n<p>为什么要做这个任务呢？因为在很多自然语言处理的下游任务中，如问答和自然语言推断，都基于两个句子做逻辑推理，而语言模型并不具备直接捕获句子之间的语义联系的能力，或者可以说成单词预测粒度的训练到不了句子关系这个层级，为了学会捕捉句子之间的语义联系，BERT 采用了下句预测（NSP ）作为无监督预训练的一部分。🍨🍨🍨</p>\\n<p>具体怎么做NSP任务呢？BERT 输入的语句将由两个句子构成，这两句是从预料中随机抽取的，其中：</p>\\n<ul>\\n<li>50% 的概率将语义连贯的两个连续句子作为训练文本，符合IsNext关系</li>\\n<li>另外50%是第二个句子从语料库中随机选择出一个拼到第一个句子后面，它们的关系是NotNext</li>\\n</ul>\\n<p>论文中也给出了相关的事例，如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ed659e93dadb4a079c5b17cf91ac0bba~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=624&amp;h=467&amp;s=46946&amp;e=png&amp;b=ffffff\" alt=\"image-20230928155944378\" loading=\"lazy\"/></p>\\n<p>我对上图中的符合做一些解释：</p>\\n<ul>\\n<li>[CLS]：对于一个句子最前面的起始标识。【在CV中的VIT就有这个标识，不知道大家是否还记得，VIT就是借鉴了BERT。】</li>\\n<li>[MASK]：这就是我们在MLM中所说的掩码标识符</li>\\n<li>[SEP]：表示两个句子的分隔符</li>\\n<li>##：这个设计到BERT划分词的标准，其不是根据单词划分的，对于一些常见的词根会单独划分，这样会大大减少字典数量，##这个符合表示flight和less是一个单词。</li>\\n</ul>\\n</li>\\n</ul>\\n\\n<p>其实到这里MLM和NSP就介绍完了，上面我们看到对于一句话有各种符号，那么我们的输入是如何设计的呢，这部分我们就来看看BERT对输入的处理，如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4928183767e64fc3a1434d9917fbdee6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1289&amp;h=485&amp;s=61986&amp;e=png&amp;b=fcf6f5\" alt=\"image-20230928182526979\" loading=\"lazy\"/></p>\\n<p>从上图可以看出，对于Input中的每个词，都有三个Embedding，分别如下：</p>\\n<ul>\\n<li>单词Embedding：这就是我们之前所说的词向量嘛，大家注意一下这里也要对标识符进行Embedding。</li>\\n<li>句子Embedding：用于区分两个句子，其只有两个值，0和1。0表示前一个句子，1 表示后一个句子。对于上图中的输入我们来看看其句子Embedding的表示，如下图所示：\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa5e5ac484fc4025977bba79390d64d2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1992&amp;h=433&amp;s=125354&amp;e=png&amp;b=f0e5d9\" alt=\"image-20230928184424763\" loading=\"lazy\"/></li>\\n<li>位置信息Embedding：这个我就不多介绍了，在Transformer小节做了详细的介绍，不清楚的可以去看一下。但要注意的是在Transformer中我们的位置编码是使用三角函数表示，这里采用的是可学习的位置编码，关于可学习的位置编码，我在VIT有所介绍，不清楚的点击☞☞☞前去阅读。</li>\\n</ul>\\n\\n<p>上面介绍的是BERT的预训练过程，下面来说说下游任务微调部分，直接上论文的图，如下：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3bf57d366d274244999f1054dce892b7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=638&amp;h=643&amp;s=104294&amp;e=png&amp;b=fdfbfb\" alt=\"image-20230928185451474\" loading=\"lazy\"/></p>\\n<p>其实微调很简单，都是利用之前训练的BERT模型，这部分我不打算逐字介绍了，推荐大家去看李宏毅老师的这个视频：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV17441137fa%2F%3Fspm_id_from%3D333.337.search-card.all.click%26vd_source%3D12c43aedc12b9cf10b775c0015015b86\" title=\"https://www.bilibili.com/video/BV17441137fa/?spm_id_from=333.337.search-card.all.click&amp;vd_source=12c43aedc12b9cf10b775c0015015b86\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">BERT下游任务改造</a></p>\\n\\n<p>最后我们来看看BERT的效果怎么样，如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d0384c3c20ed447eaed3173e89efd2d1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1205&amp;h=601&amp;s=552595&amp;e=png&amp;b=fdfcfc\" alt=\"image-20230928114044667\" loading=\"lazy\"/></p>\\n<p>不用我多说了叭，BERT确实厉害，但是其参数量是巨大的，普通人可跑不起来，接下来让我们来看看BERT的参数量。论文中给出了BERT的两种结构，分别是BRETBASEBRET_{BASE}BRETBASE\\u200b和BERTLARGEBERT_{LARGE}BERTLARGE\\u200b，参数量分别达到了恐怖的110M和340M，如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dc928621a4d74200865050bea4a1d14a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=491&amp;h=92&amp;s=21117&amp;e=png&amp;b=fffefe\" alt=\"image-20230928111259695\" loading=\"lazy\"/></p>\\n<p>这个参数量是怎么计算的呢？其实也不难，我们那BRETBASEBRET_{BASE}BRETBASE\\u200b为例来介绍其计算方法：首先是一个嵌入层，输入是字典的token数M，输出为H个隐层，所以嵌入层有M×HM×HM×H个参数量；接下来是Transformer Encoder块，先是一个自注意力块，参数量是4H24H^24H2，然后是一个MLP结构，参数量是8H28H^28H2，所以一个Transformer Encoder块有12H212H^212H2，一共有L个Transformer Encoder块，所以这部分参数量一共为L×12H2L×12H^2L×12H2。在加上嵌入层的参数量，即BRETBASEBRET_{BASE}BRETBASE\\u200b的参数量为M×H+L×12H2M×H+L×12H^2M×H+L×12H2。</p>\\n<p>我们可以带入相关参数的值，M=30000，H=768，L=12，即M×H+L×12H2=107974656≈110MM×H+L×12H^2=107974656 \\\\approx 110MM×H+L×12H2=107974656≈110M。</p>\\n<p>这里的M的数值也可以从论文中看到，如下图所示：</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7cbeb0e302b2462f874d26c82f00b4ee~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=596&amp;h=75&amp;s=20123&amp;e=png&amp;b=fffefe\" alt=\"\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>论文链接：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1810.04805.pdf\" title=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">BERT🍁🍁🍁</a></p>\\n</blockquote>\\n<p>\\xa0</p>\\n<h2>总结</h2>\\n<p>今天的内容就为大家分享到这里了，到这里也算一个小系列就出完了，当然了后续会继续出GPT系列的文章，谁让它这么火呢。🍡🍡🍡</p>\\n<p>最后的最后，明天就是中秋节了，希望大家幸福如意，阖家团圆，事事顺心。🥂🥂🥂</p>\\n<p>\\xa0</p>\\n<h2>参考连接</h2>\\n<blockquote>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fjalammar.github.io%2Fillustrated-word2vec%2F\" title=\"https://jalammar.github.io/illustrated-word2vec/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">1、The Illustrated Word2vec</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.jianshu.com%2Fp%2F9dc9f41f0b29%2F\" title=\"https://www.jianshu.com/p/9dc9f41f0b29/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">2、理解 LSTM 网络</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fv_JULY_v%2Farticle%2Fdetails%2F127411638%3Fops_request_misc%3D%25257B%252522request%25255Fid%252522%25253A%252522169458661516800184192655%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D%26request_id%3D169458661516800184192655%26biz_id%3D0%26utm_medium%3Ddistribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-127411638-null-null.268%255Ev1%255Ekoosearch%26utm_term%3Dbert%26spm%3D1018.2226.3001.4450\" title=\"https://blog.csdn.net/v_JULY_v/article/details/127411638?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169458661516800184192655%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=169458661516800184192655&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-127411638-null-null.268%5Ev1%5Ekoosearch&amp;utm_term=bert&amp;spm=1018.2226.3001.4450\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">3、Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fcolah.github.io%2Fposts%2F2015-08-Understanding-LSTMs%2F\" title=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">4、Understanding LSTM Networks</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.cnblogs.com%2Fnickchen121%2Fp%2F16470569.html%23tid-D7WMzp\" title=\"https://www.cnblogs.com/nickchen121/p/16470569.html#tid-D7WMzp\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">5、预训练语言模型的前世今生</a></p>\\n<p><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1zq4y1m7aH%2F%3Fspm_id_from%3D333.788%26vd_source%3D12c43aedc12b9cf10b775c0015015b86\" title=\"https://www.bilibili.com/video/BV1zq4y1m7aH/?spm_id_from=333.788&amp;vd_source=12c43aedc12b9cf10b775c0015015b86\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">6、PyTorch源码教程与前沿人工智能算法复现讲解</a></p>\\n</blockquote>\\n<p>\\xa0\\n\\xa0</p>\\n<p>如若文章对你有所帮助，那就🛴🛴🛴</p>\\n<p>\\u2003\\u2003\\u2003\\u2003\\u2003\\u2003\\u2003\\u2003 <img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6d8857a0049346fcbd790909eb0d90ab~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp\" alt=\"一键三连 (1).gif\" loading=\"lazy\"/></p>', 'author': '秃头小苏', 'reading_time': 19, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 62, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'fdee188605a5b8394e8a98a29f23250649a4f536da6da22a9f8e4d5df21cfef3', 'title': '【机器学习】LogisticRegression-逻辑回归中的数学原理', 'url': 'https://juejin.cn/post/7283690681175752716', 'published_at': '2023-09-28T08:56:19Z', 'created_at': '2023-10-07T09:03:25.097131Z', 'changed_at': '2023-10-07T09:03:25.097131Z', 'content': '<h1>前言</h1>\\n<p>本文主要对逻辑回归中的数学原理进行介绍，将不涉及或很少涉及代码实现，阅读前请确保已了解基本的机器学习相关内容和数学基础。</p>\\n<h1>文章概述</h1>\\n<p><strong>逻辑回归</strong>：一种名为 &#34;回归&#34; 的线性分类器，其本质是由线性回归变化而来的，是一种广泛使用于分类问题中的广义回归算法。</p>\\n<p><strong>提出问题</strong>：本文将会基于以下问题来对逻辑回归进行展开讨论和介绍：</p>\\n<ul>\\n<li>为什么需要掌握逻辑回归？</li>\\n<li>sigmoid函数是怎么来的？</li>\\n<li>交叉熵损失函数是怎么来的？</li>\\n<li>怎么使用梯度下降对逻辑回归进行求解？</li>\\n<li>逻辑回归通过什么方法减少过拟合程度？</li>\\n<li>逻辑回归怎么完成多分类任务？</li>\\n</ul>\\n<p>注:</p>\\n<ul>\\n<li>若不特别说明则向量默认为列向量</li>\\n<li>加粗的符号表示为向量，如w\\\\bold{w}w</li>\\n</ul>\\n<h1>为什么需要逻辑回归</h1>\\n<p>逻辑回归基于线性回归，对数据的质量要求相对严格，且原理相对需要更多的数学基础，在一些相关书籍中也很少被介绍，但它仍然是一个受工业商业热爱，使用广泛的模型，有着不可替代的优点，包括但不限于：</p>\\n<ul>\\n<li><strong>逻辑回归对线性关系的拟合效果非常好</strong>：对于特征与标签之间线性关系极强的数据，例如金融领域中的信用卡欺诈、评分卡制作，电商中的营销预测等数据，都是逻辑回归的强项。虽然现在有了很多效果更好的更为先进的模型，但逻辑回归在金融领域，尤其是银行业中的统治地位仍然不可动摇；相对的，逻辑回归在非线性数据的效果很多时候比随机模型还要差。</li>\\n<li><strong>计算速度很快</strong>：对于线性数据，逻辑回归的拟合和计算都非常快，计算效率优于SVM和随机森林，在大型数据上尤其能够看得出区别。</li>\\n<li><strong>分类结果为类概率数值</strong>：可以把逻辑回归返回的结果当成连续型数据来利用。例如在评分卡制作时，我们不仅需要判断客户是否会违约，还需要给出相应的&#34;信用分&#34;，因此我们可以基于模型输出的后验概率来评定信用分。</li>\\n</ul>\\n<blockquote>\\n<p>当然，在很多工具或库中，例如决策树、随机森林这样的模型也可基于相关算法输出相应概率，上面介绍的各个优点在很多情况下也不是逻辑回归特有的。</p>\\n</blockquote>\\n<p>逻辑回归是一个在线性数据上表现优异的分类器，主要被应用在金融领域，福布斯杂志曾在讨论逻辑回归时说过：&#34;技术上来说，最佳模型的AUC面积低于0.8时，逻辑回归的性能明显优于树模型&#34;。此外，逻辑回归在小数据集上表现更好，而对于大型的数据集，树模型的效果往往更好。逻辑回归在当下仍然是一个重要模型，不仅是对于一些与之紧密相关的应用场景，对于之后更深入的学习它也是不可或缺的一部分。</p>\\n<h1>逻辑回归基本原理</h1>\\n<p>在这里，本文会对逻辑回归的基本原理做全方位的数学解释说明，以二元逻辑回归为例介绍逻辑回归的由来以及相关公式推导，包括其基本形式、sigmoid函数以及其损失函数和基于梯度下降算法的求解。</p>\\n<h2>从线性回归到逻辑回归</h2>\\n<p><strong>线性回归</strong>：简单对线性回归相关知识进行回顾，模型如下：</p>\\ny=[w0w1⋯wn][x0x1⋮xn]=wTx（x0=1）y = \\\\begin{bmatrix} w_0 &amp; w_1 &amp; \\\\cdots &amp; w_n \\\\end{bmatrix}\\\\begin{bmatrix} x_0 \\\\\\\\ x_1 \\\\\\\\ \\\\vdots \\\\\\\\ x_n \\\\end{bmatrix} = \\\\bold{w}^T\\\\bold{x}（x_0=1）y=[w0\\u200b\\u200bw1\\u200b\\u200b⋯\\u200bwn\\u200b\\u200b]⎣⎡\\u200bx0\\u200bx1\\u200b⋮xn\\u200b\\u200b⎦⎤\\u200b=wTx（x0\\u200b=1）\\n<p>我们也常使用如下方式来表示线性回归：</p>\\ny=wTx+w0y = \\\\bold{w}^T\\\\bold{x} + w_0y=wTx+w0\\u200b\\n<ul>\\n<li>其中向量 w,x\\\\bold{w,x}w,x 中的第一个元素分别为 w1,x1w_1, x_1w1\\u200b,x1\\u200b.</li>\\n</ul>\\n<p>线性回归的任务，就是构造一个预测函数来映射输入的特征矩阵 xxx 和标签值 yyy 的线性关系，而构造预测函数的核心就是找出模型的各个权重系数[w1⋯wn]\\\\begin{bmatrix}w_1 &amp; \\\\cdots &amp; w_n\\\\end{bmatrix}[w1\\u200b\\u200b⋯\\u200bwn\\u200b\\u200b] 和截距 w0w_0w0\\u200b。</p>\\n<p><strong>提出问题</strong>：通过上方函数，线性回归使用输入的特征矩阵 XXX 来输出一组连续型变量 ypredy_{pred}ypred\\u200b，以完成各种预测连续型变量的任务；若我们的标签是离散型变量，例如对满足0-1分布的离散型变量进行预测，该怎么办呢？</p>\\n<p><strong>基本思路</strong>：线性回归将输入的特征矩阵映射为一组连续型变量，这组连续型变量的取值范围是没有限定的，若我们想要从回归预测转到二分类任务上，最简单且直接的思路无非就是将输出的这组连续型变量映射到一个新的0-1分布的空间，同样最粗暴的方式就是将原来的回归预测的范围限定在 0 ~ 1 之间，这样模型回归预测的结果只能在预先定义的范围中，也就是 0 ~ 1 之间。</p>\\n<p><strong>联系函数（link function）</strong>：我们设一个相应的、符合我们基本思路的联系函数 g(z)g(z)g(z)，其中 zzz 为原线性回归模型的输出值，令 g(z)g(z)g(z) 的值分布在区间 (0,1) 中，当 g(z)g(z)g(z) 趋近于0时，标签为0，当 g(z)g(z)g(z) 趋近于1时，标签为1，这样就得到了一个分类模型；其中在逻辑回归中，这个函数就是Sigmoid函数。</p>\\ng(z)=sigmoid(z)=11+e−zg(z) = sigmoid(z) = \\\\frac{1}{1+e^{-z}}g(z)=sigmoid(z)=1+e−z1\\u200b\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b14d5a0463c440d4a5a00807eb5aa45e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=855&amp;h=450&amp;s=46467&amp;e=png&amp;b=ebebf3\" alt=\"image.png\" loading=\"lazy\"/>\\n<p>上图涉及的相关绘图代码以供参考（可忽略并折叠）：</p>\\n<pre><code>import numpy as np\\nimport matplotlib.pyplot as plt\\nsigmoid = lambda z: 1 / (1 + np.e ** (-z))\\n\\nx = np.arange(-10, 10, 0.1)\\n\\nplt.figure(figsize=(8,4))\\nplt.scatter(0, 0.5, s=50, c=&#39;red&#39;, alpha=0.7, edgecolor=&#39;white&#39;,linewidth=1)\\nplt.plot(x, sigmoid(x), color = &#39;blue&#39;, alpha=0.7)\\nplt.plot([0, 0, -10], [0, 0.5, 0.5], linestyle=&#39;:&#39;, color=&#39;red&#39;, alpha=0.6)\\nplt.text(0.45, 0.45, &#39;(0, 0.5)&#39;, color=&#39;red&#39;, alpha=0.7)\\n\\nplt.yticks([0, 0.2, 0.4, 0.5, 0.6, 0.8, 1])\\nplt.xlabel(&#39;z&#39;)\\nplt.ylabel(&#39;g(z)&#39;)\\nplt.show()\\n</code></pre>\\n<p><strong>Sigmoid函数</strong>：如上图所示，该函数是一个S型的函数，当自变量 zzz 趋近正无穷时，因变量g(z)趋近于1，而当 zzz 趋近负无穷时，g(z)g(z)g(z) 趋近于0；该函数能将任何实数映射到(0,1)区间，因此可用于将输出为任意值的函数转换为适合二分类的函数。由于该性质，Sigmoid函数也被当作归一化的一种方法，可将数据压缩到区间 [0,1] 中。</p>\\n<ul>\\n<li>注意：sigmoid函数中只能无限趋近于0和1，即取值范围两边都为开区间。</li>\\n</ul>\\n<p>将线性回归函数代入sigmoid函数后得到二元逻辑回归模型的一般形式：</p>\\ng(z)=11+e−z=11+e−(wTx+w0)g(z) = \\\\frac{1}{1+e^{-z}} = \\\\frac{1}{1 + e^{-(\\\\bold{w}^T\\\\bold{x}+w_0)}}g(z)=1+e−z1\\u200b=1+e−(wTx+w0\\u200b)1\\u200b\\n<p>至此，我们已经得到了二元逻辑回归模型的一般形式。</p>\\n<h2>sigmoid函数的由来</h2>\\n<p>对于sigmoid函数的介绍我们却在上方一笔带过，接下来，我们将会重点介绍sigmoid函数是如何推导出来的。</p>\\n<p>在此本文会先介绍线性判别函数，这是一个很原始的二分类判别器，本文将介绍它的原理和不足之处，以此引出sigmoid函数以及其相对于线性判别函数解决的问题。对于线性判别函数的原理不影响下文sigmoid函数推导的原理阅读，若不感兴趣则可跳过线性判别函数部分。</p>\\n<h3>线性判别函数</h3>\\n<p><strong>决策边界</strong>：对于分类问题，无论什么模型，本质上都是其依据训练数据在对应特征空间中推导出了不同分类间的决策边界，模型依靠确定的决策边界对新的特征向量进行分类预测。</p>\\n<p>对于决策边界的确定也有着不同方法以及对应的不同模型（例如朴素贝叶斯依靠概率密度来进行决策），但最直观同时也是最粗暴的方法为对该决策边界直接进行估计，在这里我们引入线性判别函数。</p>\\n<p><strong>线性判别函数（Linear Discriminant Function）</strong>：统计模式识别中用以对模式进行分类的一种最简单的判别函数称为线性判别函数。</p>\\n<img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6855744098f84ca2b7333f9e463e8964~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=723&amp;h=577&amp;s=46133&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/>\\n<p>上图中我们假设红色三角为类 C1C_1C1\\u200b，蓝色圆圈为类 C2C_2C2\\u200b；我们先分别得到这两类数据对应的两个线性回归模型 g1(x)g_1(x)g1\\u200b(x) 和 g2(x)g_2(x)g2\\u200b(x)：</p>\\ng1(x)=[w11w12][x11x12]+w10=w1Tx+w10g2(x)=[w21w22][x21x22]+w20=w2Tx+w20g_1(x) = \\\\begin{bmatrix} w_{11} &amp; w_{12} \\\\end{bmatrix}\\\\begin{bmatrix} x_{11} \\\\\\\\ x_{12} \\\\end{bmatrix} + w_{10} = \\\\bold{w_1}^T\\\\bold{x} + w_{10}  \\\\\\\\\\ng_2(x) = \\\\begin{bmatrix} w_{21} &amp; w_{22} \\\\end{bmatrix}\\\\begin{bmatrix} x_{21} \\\\\\\\ x_{22} \\\\end{bmatrix} + w_{20} = \\\\bold{w_2}^T\\\\bold{x} + w_{20}g1\\u200b(x)=[w11\\u200b\\u200bw12\\u200b\\u200b][x11\\u200bx12\\u200b\\u200b]+w10\\u200b=w1\\u200bTx+w10\\u200bg2\\u200b(x)=[w21\\u200b\\u200bw22\\u200b\\u200b][x21\\u200bx22\\u200b\\u200b]+w20\\u200b=w2\\u200bTx+w20\\u200b\\n<ul>\\n<li>其中 w11w_{11}w11\\u200b 中的下标指的是类别1中的第一个权重系数，其它参数同理</li>\\n</ul>\\n<p>对于这两个模型，有以下定义：</p>\\n<ul>\\n<li>若 g1(x)&gt;g2(x)g_1(x) &gt; g_2(x)g1\\u200b(x)&gt;g2\\u200b(x)，则为类 C1C_1C1\\u200b</li>\\n<li>若 g1(x)≤g2(x)g_1(x) \\\\le g_2(x)g1\\u200b(x)≤g2\\u200b(x)，则为类 C2C_2C2\\u200b</li>\\n</ul>\\n<blockquote>\\n<p>我们当然不能直接认为若 g1(x)&gt;g2(x)g_1(x) &gt; g_2(x)g1\\u200b(x)&gt;g2\\u200b(x)，则为类 C1C_1C1\\u200b，这里不能将 g1(x)g_1(x)g1\\u200b(x) 和 g2(x)g_2(x)g2\\u200b(x) 当成两个独立的模型来看，我们最后要将两模型的输出值进行比较以进行决策，由于规定当 g1(x)&gt;g2(x)g_1(x) &gt; g_2(x)g1\\u200b(x)&gt;g2\\u200b(x) 时，则为类 C1C_1C1\\u200b，因此模型会在拟合训练数据后趋于符合我们的定义，最终确定出一条决策边界（如上图中的蓝色线条）。</p>\\n</blockquote>\\n<p>基于我们上方的两个定义，为了寻找对应决策边界，我们可以进行如下简单的变换：</p>\\nC1：g1(x)&gt;g2(x)→g1(x)−g2(x)&gt;0C2：g1(x)≤g2(x)→g1(x)−g2(x)≤0C_1：g_1(x) &gt; g_2(x) \\\\rightarrow g_1(x) - g_2(x) &gt; 0 \\\\\\\\\\nC_2：g_1(x) \\\\le g_2(x) \\\\rightarrow g_1(x) - g_2(x) \\\\le 0C1\\u200b：g1\\u200b(x)&gt;g2\\u200b(x)→g1\\u200b(x)−g2\\u200b(x)&gt;0C2\\u200b：g1\\u200b(x)≤g2\\u200b(x)→g1\\u200b(x)−g2\\u200b(x)≤0\\n<p>可以看到左边式子是一样的，则有：</p>\\ng(x)=g1(x)−g2(x)=(w1Tx+w10)−(w2Tx+w20)=(w1−w2)Tx+(w10−w20)\\\\begin{align}\\ng(x) \\n&amp;= g_1(x) - g_2(x) \\\\\\\\\\n&amp;= (\\\\bold{w_1}^T\\\\bold{x} + w_{10}) - (\\\\bold{w_2}^T\\\\bold{x} + w_{20}) \\\\\\\\\\n&amp;= (\\\\bold{w_1} - \\\\bold{w_2})^T\\\\bold{x} + (w_{10} - w_{20}) \\\\\\\\\\n\\\\end{align}g(x)\\u200b=g1\\u200b(x)−g2\\u200b(x)=(w1\\u200bTx+w10\\u200b)−(w2\\u200bTx+w20\\u200b)=(w1\\u200b−w2\\u200b)Tx+(w10\\u200b−w20\\u200b)\\u200b\\u200b\\n<p>观察易得 w1−w2\\\\bold{w_1} - \\\\bold{w_2}w1\\u200b−w2\\u200b 还是一个向量，w10−w20w_{10}-w_{20}w10\\u200b−w20\\u200b 也还是一个常数，则有：</p>\\n令{w=w1−w2w0=w10−w20，即：g(x)=wTx+w0令\\\\begin{cases}\\\\bold{w} = \\\\bold{w_1}- \\\\bold{w_2} \\\\\\\\ w_0 = w_{10}-w_{20} \\\\end{cases}，即：g(x) = \\\\bold{w}^T\\\\bold{x} + w_0令{w=w1\\u200b−w2\\u200bw0\\u200b=w10\\u200b−w20\\u200b\\u200b，即：g(x)=wTx+w0\\u200b\\n<p>最终得到的 g(x)g(x)g(x) 就是我们的线性判别函数的表达式。</p>\\n<img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/afddced260c148078b72a17f2af7a497~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=747&amp;h=630&amp;s=50560&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/>\\n<p>如上图所示，易得该函数符合以下定义：</p>\\nlabel={C1,g(x)&gt;0C2,otherwiselabel = \\\\begin{cases}\\nC_1, \\\\quad g(x) &gt; 0 \\\\\\\\\\nC_2, \\\\quad otherwise\\n\\\\end{cases}label={C1\\u200b,g(x)&gt;0C2\\u200b,otherwise\\u200b\\n<p>这里是以二维数据为例，但无论对于几维数据，线性判别函数 g(x)g(x)g(x) 本质上是在其特征空间中定义了一个超平面（Hyperplane），将该特征空间分为两个半空间，以该超平面作为决策边界来对数据进行二分类。</p>\\n<blockquote>\\n<p>例如对于我们上方的二维特征空间，图中的蓝色线条就是超平面。</p>\\n</blockquote>\\n<p>对于我们上方推导得到的函数 g(x)g(x)g(x)，这就是一个很原始的线性分类器，可以做基本的二分类工作，但其本身也有着很大的局限性。</p>\\n<img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c9c1495086ef40c48beaf559c25ee386~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=835&amp;h=456&amp;s=25627&amp;e=png&amp;b=eaeaf2\" alt=\"image.png\" loading=\"lazy\"/>\\n<p>上图涉及的相关绘图代码以供参考（可忽略并折叠）：</p>\\n<pre><code>import numpy as np\\nimport matplotlib.pyplot as plt\\n\\ng_func = np.vectorize(lambda x: 1 if x &gt; 0 else 0)\\n\\nx = np.arange(-10, 10, 0.1)\\nplt.figure(figsize=(8,4))\\nplt.plot(x, g_func(x), color = &#39;red&#39;, alpha=0.6)\\n\\nplt.xlabel(&#39;x&#39;)\\nplt.ylabel(&#39;g(x)&#39;)\\nplt.show()\\n</code></pre>\\n<p><strong>模型弊端</strong>：如上图所示，我们求得的线性判别函数 g(x)g(x)g(x) 对于二分类的决策策略为 &#34;非此即彼&#34;，这会造成很大的问题，例如当输入某个特征矩阵后，若 g(x)=0.00001g(x)=0.00001g(x)=0.00001，模型的输出结果为1，但其实模型本身并没有多大把握，因为差一点点模型就会输出0；此外，假如输入某特征向量后，模型输出1，虽然我们知道它属于类1，但是我们却不知道它在多大程度上属于1，也就是这个模型得到这个分类结果的可信度无法通过这个模型得知。</p>\\n<h3>sigmoid函数推导</h3>\\n<p><strong>注</strong>：sigmoid函数在网上也有着基于不同角度的不同的推导方法，方法不唯一，不必局限于本文方法。</p>\\n<p>设某概率模型，已知特征向量 x\\\\bold{x}x，则模型输出为 C1C_1C1\\u200b 和 C2C_2C2\\u200b 的后验概率分别为：</p>\\n已知P(C1∣x)=z，则P(C2∣x)=1−z已知P(C_1|{\\\\bold{x}}) = z，则P(C_2|{\\\\bold{x}}) = 1-z已知P(C1\\u200b∣x)=z，则P(C2\\u200b∣x)=1−z\\n<p>则有以下决策策略：</p>\\nlabel={C1,z&gt;0.5C2,Otherwiselabel=\\\\begin{cases} C_1, \\\\quad z &gt; 0.5 \\\\\\\\ C_2, \\\\quad Otherwise \\\\end{cases}label={C1\\u200b,z&gt;0.5C2\\u200b,Otherwise\\u200b\\n<p>不难发现，可以推出以下公式：</p>\\nz&gt;0.5→z1−z&gt;1→logz1−z&gt;0z &gt; 0.5 \\\\quad \\\\rightarrow \\\\quad \\\\frac{z}{1-z} &gt; 1 \\\\quad \\\\rightarrow \\\\quad  log\\\\frac{z}{1-z} &gt; 0z&gt;0.5→1−zz\\u200b&gt;1→log1−zz\\u200b&gt;0\\n<p>对数函数的底数设为 eee，不难看出，令 y=logz1−zy = log\\\\frac{z}{1-z}y=log1−zz\\u200b，当 y&gt;0y&gt;0y&gt;0时，laebl=C1laebl=C_1laebl=C1\\u200b，反之 label=C2label=C_2label=C2\\u200b，对于函数 y=logz1−zy=log\\\\frac{z}{1-z}y=log1−zz\\u200b，我们求其反函数：</p>\\nz=lny1−y→ez=y1−y→y=ez−ezy→y=ez1+ez=11+e−z⏟sigmoidz = ln\\\\frac{y}{1-y} \\\\quad \\\\rightarrow \\\\quad e^z = \\\\frac{y}{1-y} \\\\quad \\\\rightarrow \\\\quad y  = e^z - e^zy \\\\quad \\\\rightarrow \\\\quad y= \\\\frac{e^z}{1+e^z} = \\\\underbrace{\\\\frac{1}{1+e^{-z}}}_{sigmoid}z=ln1−yy\\u200b→ez=1−yy\\u200b→y=ez−ezy→y=1+ezez\\u200b=sigmoid1+e−z1\\u200b\\u200b\\u200b\\n<p>至此可以看出函数 logz1−zlog\\\\frac{z}{1-z}log1−zz\\u200b 的反函数就是sigmoid函数。</p>\\n<p><strong>回顾一下反函数的定义</strong>：一般来说，设函数 y=f(x)(x∈A)y=f(x)(x∈A)y=f(x)(x∈A) 的值域是 CCC，若找得到一个函数 g(y)g(y)g(y) 在每一处 g(y)g(y)g(y) 都等于对应的 xxx，这样的函数 x=g(y)(y∈C)x=g(y)(y∈C)x=g(y)(y∈C) 叫做函数 y=f(x)(x∈A)y=f(x)(x∈A)y=f(x)(x∈A) 的反函数，记作 x=f−1(y)x=f^{-1}(y)x=f−1(y)  。反函数 x=f−1(y)x=f^{-1}(y)x=f−1(y) 的定义域、值域分别是函数 y=f(x)y=f(x)y=f(x) 的值域、定义域。</p>\\n<p><strong>求反函数的原因</strong>：原函数中自变量为后验概率，因变量为一个范围为(−∞,∞)(-\\\\infty, \\\\infty)(−∞,∞)的连续型变量，这与我们的目的恰恰相反，我们想让线性回归模型的输出值（连续型变量）作为自变量，而用于二分类的后验概率作为因变量。</p>\\n<p>对于原函数 y=logz1−zy = log\\\\frac{z}{1-z}y=log1−zz\\u200b，其中自变量 zzz 为后验概率，因变量 yyy 用于判别（y∈(−∞,+∞)y\\\\in (-\\\\infty, +\\\\infty)y∈(−∞,+∞)）；参考反函数的定义，则其反函数，也就是sigmoid函数中的因变量 yyy 相当于原函数中的后验概率zzz；而自变量 zzz 则相当于原函数中的 yyy，</p>\\n<ul>\\n<li>对于原函数，若z&gt;0.5z &gt; 0.5z&gt;0.5，则y&gt;0y&gt;0y&gt;0，为 C1C_1C1\\u200b，若 z&lt;0.5z&lt;0.5z&lt;0.5, 则y&lt;0y&lt;0y&lt;0，为 C2C_2C2\\u200b.</li>\\n<li>对于反函数（sigmoid函数），若 z&gt;0z&gt;0z&gt;0，则y&gt;0.5y&gt;0.5y&gt;0.5，为 C1C_1C1\\u200b；若 z&lt;0z&lt;0z&lt;0，则y&lt;0.5y&lt;0.5y&lt;0.5，为C2C_2C2\\u200b.</li>\\n</ul>\\n<p><strong>线性回归输出值作为sigmoid的输入值的原因</strong>：对于原函数 y=lnz1−zy=ln\\\\frac{z}{1-z}y=ln1−zz\\u200b，假设线性回归模型的输出结果作为sigmoid函数的输入，sigmoid函数的输出值为后验概率 zzz，则有：</p>\\ny=ln(z1−z)=ln(sigmoid(wTx)1−sigmoid(wTx))=ln(11+e−wTxe−wTx1+e−wTx)=ln(1e−wTx)=ln(ewTx)=wTx\\\\begin{align}\\ny=ln(\\\\frac{z}{1-z}) = ln(\\\\frac{sigmoid(\\\\bold{w}^T\\\\bold{x})}{1-sigmoid(\\\\bold{w}^T\\\\bold{x})}) = ln(\\\\frac{\\\\frac{1}{1+e^{-\\\\bold{w}^T\\\\bold{x}}}}{\\\\frac{e^{-\\\\bold{w}^T\\\\bold{x}}}{1+e^{-\\\\bold{w}^T\\\\bold{x}}}}) = ln(\\\\frac{1}{e^{-\\\\bold{w}^T\\\\bold{x}}}) = ln(e^{\\\\bold{w}^T\\\\bold{x}}) = \\\\bold{w}^T\\\\bold{x}\\n\\\\end{align}y=ln(1−zz\\u200b)=ln(1−sigmoid(wTx)sigmoid(wTx)\\u200b)=ln(1+e−wTxe−wTx\\u200b1+e−wTx1\\u200b\\u200b)=ln(e−wTx1\\u200b)=ln(ewTx)=wTx\\u200b\\u200b\\n<p>由此可见在原函数中我们的因变量 yyy 其实就是线性回归的输出值，则sigmoid函数中的自变量 zzz 其实也就是线性回归的输出值，即 wTx\\\\bold{w}^T\\\\bold{x}wTx。</p>\\n<p>从我们推导的过程来看，对于sigmoid函数，输入值 zzz 是一个取值范围为 (−∞,+∞)(-\\\\infty, +\\\\infty)(−∞,+∞) 的连续变量，也就是将线性回归的输出值作为其输入值；而输出值 yyy 其实就是后验概率，由于输出值为一个概率，因此我们可以根据输出值来判断该样本多大程度上属于该类别。</p>\\n<ul>\\n<li>例：输入某特征向量后输出 0.650.650.65，则我们可以说该样本属于类别1的概率为0.65，属于类别0的概率为0.35.</li>\\n</ul>\\n<h2>LOSS：交叉熵损失函数</h2>\\n<h3>交叉熵损失的直观理解</h3>\\n<p>对于给定的输入 x\\\\bold{x}x，模型输出为标签 C2C_2C2\\u200b 的概率为 p(C1∣x)p(C_1|\\\\bold{x})p(C1\\u200b∣x)，输出为标签 C2C_2C2\\u200b 的概率为 p(C2∣x)p(C_2|\\\\bold{x})p(C2\\u200b∣x)，易得两概率事件对应的信息量分别为 −logp(C1∣x)-logp(C_1|\\\\bold{x})−logp(C1\\u200b∣x) 和 −logp(C2∣x)-log p(C_2|\\\\bold{x})−logp(C2\\u200b∣x)；</p>\\n<blockquote>\\n<p>对于信息量公式的推导，可以参考：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fqq_38890412%2Farticle%2Fdetails%2F106149936\" title=\"https://blog.csdn.net/qq_38890412/article/details/106149936\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">信息熵（香农熵）-CSDN博客</a></p>\\n</blockquote>\\n<p>将输入样本对应特征向量，输出正确标签看作一个概率事件，概率越小的事件产生的信息量越大，而概率越大的事件产生的信息量越小；代入到逻辑回归模型中，则模型输出对应正确标签的概率越大，则对应信息量越小，反之则对应信息量越大，正好符合损失函数的性质。</p>\\n<img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d697ab9428304f00ba682d30a9f3100e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=661&amp;h=453&amp;s=28978&amp;e=png&amp;b=ebeaf2\" alt=\"image.png\" loading=\"lazy\"/>\\n<p>上图涉及的相关绘图代码以供参考（可忽略并折叠）：</p>\\n<pre><code>import matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nplt.style.use(&#39;seaborn&#39;)\\nmpl.rcParams.update({&#39;mathtext.fontset&#39;: &#39;stix&#39;})\\n\\nx_1 = np.arange(1, 2, 0.01)\\nx_2 = np.arange(0.01, 1, 0.01)\\n\\nplt.figure(figsize=(6,4))\\nplt.plot(x_1, -np.log(x_1), color=&#39;blue&#39;, alpha=0.6, linestyle=&#39;--&#39;)\\nplt.plot(x_2, -np.log(x_2), color=&#39;blue&#39;, alpha=0.6)\\nplt.text(1.9, -0.220, &#39;→&#39;, fontsize=33, color=&#39;red&#39;, alpha=1)\\nplt.text(-0.06, 4.4, &#39;↑&#39;, fontsize=36, color=&#39;red&#39;, alpha=1)\\nplt.axvline(0, color=&#39;red&#39;, alpha=1)\\nplt.axhline(0, color=&#39;red&#39;, alpha=1)\\nplt.axvline(1, color=&#39;red&#39;, alpha=0.6, linestyle=&#39;:&#39;)\\n\\nplt.xlabel(&#39;$x$&#39;)\\nplt.ylabel(&#39;$-log(x)$&#39;)\\nplt.xticks([*range(0,3)])\\nplt.yticks([*range(0, 5, 1)])\\nplt.show()\\n</code></pre>\\n<p>概率在0-1之内，由该函数的性质可得，当输出概率越小的时候，损失成指数级别增加，当概率趋于1时，损失趋于0，因此当输出概率偏离真实值越大时，损失增加地非常快，这也会在一定程度上增加模型迭代速率。</p>\\n<p>以数值0、1分别表示二分类中的两类，当真实标签为1时，模型预测某特征向量的标签为1的概率为 ppp，对应损失就是模型预测正确产生的信息量，有：</p>\\nloss={−log(p),label=1−log(1−p),label=0loss=\\n\\\\begin{cases}\\n-log(p), \\\\quad label=1\\\\\\\\\\n-log(1-p), \\\\quad label=0 \\\\\\\\\\n\\\\end{cases}loss={−log(p),label=1−log(1−p),label=0\\u200b\\n<p>转换为一个公式，则有：</p>\\nloss=−(y∗log(p)+(1−y)∗log(1−p))loss= -(y*log(p) + (1-y)*log(1-p))loss=−(y∗log(p)+(1−y)∗log(1−p))\\n<ul>\\n<li>yyy：当前特征向量对应的真实标签</li>\\n</ul>\\n<p>观察得知，若真实标签为1，则对应loss为 −log(p)-log(p)−log(p)，若真实标签为0，则对应loss为 −log(1−p)-log(1-p)−log(1−p)；对于整个数据集，设某数据集中含有 n 个样本，即 n 个对应特征向量，因此有如下损失函数：</p>\\nLOSS=−∑i=1n(yi∗log(pi)+(1−yi)∗log(1−pi))LOSS = -\\\\sum^n_{i=1}(y_i*log(p_i) + (1-y_i)*log(1-p_i))LOSS=−i=1∑n\\u200b(yi\\u200b∗log(pi\\u200b)+(1−yi\\u200b)∗log(1−pi\\u200b))\\n<ul>\\n<li>nnn：某数据集下的样本个数</li>\\n<li>yiy_iyi\\u200b：数据集中第 i 个样本对应的标签（0 or 1）</li>\\n<li>pip_ipi\\u200b：数据集中第 i 个样本模型预测为标签 1 的概率</li>\\n</ul>\\n<p>上方函数就是交叉熵损失函数；即模型每次迭代中的每个样本计算对应 loss 后进行累加得到每次迭代的损失。</p>\\n<h3>基于极大似然估计法推导交叉熵损失</h3>\\n<p>在直观理解中，我们通过拆分交叉熵损失的计算公式可以看出其实可以解释为不同概率事件发生时产生的信息量，并通过观察函数图像可以了解到该损失的一些性质，例如随着预测值与真实值的偏差增大，损失呈指数增加等，但是上方对损失函数本身推导的解释是相对模糊的，因此在接下来我们基于极大似然估计法来对交叉熵损失函数进行推导。</p>\\n<p>二元逻辑回归的标签服从伯努利分布(即0-1分布)，因此我们可以将一个特征向量为 xi\\\\bold{x_i}xi\\u200b 的样本 i，模型参数为 w\\\\bold{w}w 的预测情况表现为如下形式：</p>\\n<ul>\\n<li>样本 i 被预测为 1 的概率为：P1=P(yi^=1∣xi,w)=yw(xi)P_1=P(\\\\hat{y_i}=1|\\\\bold{x_i}, \\\\bold{w})=y_{\\\\bold{w}}(\\\\bold{x_i})P1\\u200b=P(yi\\u200b^\\u200b=1∣xi\\u200b,w)=yw\\u200b(xi\\u200b)</li>\\n<li>样本 i 被预测为 0 的概率为：P0=P(yi^=0∣xi,w)=1−yw(xi)P_0=P(\\\\hat{y_i}=0|\\\\bold{x_i}, \\\\bold{w})=1-y_{\\\\bold{w}}(\\\\bold{x_i})P0\\u200b=P(yi\\u200b^\\u200b=0∣xi\\u200b,w)=1−yw\\u200b(xi\\u200b)</li>\\n</ul>\\n<p>将这两个概率整合，我们可以定义如下等式：</p>\\nP(yi^∣xi,w)=P1yi∗P01−yiP(\\\\hat{y_i}|\\\\bold{x_i}, \\\\bold{w}) = P_1^{y_i} * P_0^{1-y_i}P(yi\\u200b^\\u200b∣xi\\u200b,w)=P1yi\\u200b\\u200b∗P01−yi\\u200b\\u200b\\n<ul>\\n<li>yiy_iyi\\u200b：样本 i 的真实标签</li>\\n<li>yi^\\\\hat{y_i}yi\\u200b^\\u200b：模型对于样本 i 的预测标签</li>\\n</ul>\\n<p>整合之后，我们可以用上述式子来表示原来的两种情况：</p>\\n<ul>\\n<li>当真实标签为 0 时，P(yi^∣xi,w)=P10∗P01−0=P0P(\\\\hat{y_i}|\\\\bold{x_i}, \\\\bold{w}) = P_1^{0} * P_0^{1-0}=P_0P(yi\\u200b^\\u200b∣xi\\u200b,w)=P10\\u200b∗P01−0\\u200b=P0\\u200b</li>\\n<li>当真实标签为 1 时，P(yi^∣xi,w)=P11∗P01−1=P1P(\\\\hat{y_i}|\\\\bold{x_i}, \\\\bold{w}) = P_1^{1} * P_0^{1-1}=P_1P(yi\\u200b^\\u200b∣xi\\u200b,w)=P11\\u200b∗P01−1\\u200b=P1\\u200b</li>\\n</ul>\\n<p>对于函数 P(yi^∣xi,w)P(\\\\hat{y_i}|\\\\bold{x_i},\\\\bold{w})P(yi\\u200b^\\u200b∣xi\\u200b,w)，无论真实标签为什么，我们都希望该函数的取值最大化，这也就将最小化损失的问题转换为了对该函数求极值的问题。</p>\\n<p>对于某训练集的 n 个样本，我们可以得到以下似然函数，我们可以定义如下等式来表达所有样本在模型预测函数中输出的所有可能的 y^\\\\hat{y}y^\\u200b 对应的概率：</p>\\nP=∏i=0nP(yi^∣xi,w)=∏i=1n(P1yi∗P01−yi)=∏i=1n(yw(xi)yi∗(1−yw(xi))1−yi)\\\\begin{align}\\nP \\n&amp;= \\\\prod^n_{i=0}P(\\\\hat{y_i}|\\\\bold{x_i}, \\\\bold{w}) \\\\\\\\\\n&amp;= \\\\prod^n_{i=1}(P_1^{y_i} * P_0^{1-y_i}) \\\\\\\\ \\n&amp;= \\\\prod^n_{i=1}(y_{\\\\bold{w}}(\\\\bold{x_i})^{y_i} * (1-y_{\\\\bold{w}}(\\\\bold{x_i}))^{1-y_i})\\n\\\\end{align}P\\u200b=i=0∏n\\u200bP(yi\\u200b^\\u200b∣xi\\u200b,w)=i=1∏n\\u200b(P1yi\\u200b\\u200b∗P01−yi\\u200b\\u200b)=i=1∏n\\u200b(yw\\u200b(xi\\u200b)yi\\u200b∗(1−yw\\u200b(xi\\u200b))1−yi\\u200b)\\u200b\\u200b\\n<p>这就得到了在某数据集下所有样本预测都正确的概率 PPP，我们的目标是最大化这个概率 PPP，但现在如上方公式所示，该公式是将每个样本的特征向量进行累乘，若将 (yw(xi)yi∗(1−yw(xi))1−yi)(y_{\\\\bold{w}}(\\\\bold{x_i})^{y_i} * (1-y_{\\\\bold{w}}(\\\\bold{x_i}))^{1-y_i})(yw\\u200b(xi\\u200b)yi\\u200b∗(1−yw\\u200b(xi\\u200b))1−yi\\u200b) 作为每个样本的损失的话，则需要对每个样本的损失进行累乘来计算某组样本下的总损失，但实际上总损失应该等于各样本特征向量对应损失的累加，因此我们在等式两边分别取对数，则有：</p>\\nlogP=log∏i=1n(yw(xi)yi∗(1−yw(xi))1−yi)=∑i=1nlog(yw(xi)yi∗(1−yw(xi))1−yi)=∑i=1n(log(yw(xi)yi)+log(1−yw(xi))1−yi)=∑i=1n(yi∗log(yw(xi))+(1−yi)∗log(1−yw(xi)))\\\\begin{align}\\nlogP \\n&amp;= log \\\\prod^n_{i=1}(y_{\\\\bold{w}}(\\\\bold{x_i})^{y_i} * (1-y_{\\\\bold{w}}(\\\\bold{x_i}))^{1-y_i}) \\\\\\\\\\n&amp;= \\\\sum^{n}_{i=1}log(y_{\\\\bold{w}}(\\\\bold{x_i})^{y_i} * (1-y_{\\\\bold{w}}(\\\\bold{x_i}))^{1-y_i}) \\\\\\\\\\n&amp;= \\\\sum^{n}_{i=1}(log(y_{\\\\bold{w}}(\\\\bold{x_i})^{y_i}) + log(1-y_{\\\\bold{w}}(\\\\bold{x_i}))^{1-y_i}) \\\\\\\\\\n&amp;= \\\\sum^{n}_{i=1}(y_i*log(y_{\\\\bold{w}}(\\\\bold{x_i})) + (1-y_i)*log(1-y_{\\\\bold{w}}(\\\\bold{x_i})))\\n\\\\end{align}logP\\u200b=logi=1∏n\\u200b(yw\\u200b(xi\\u200b)yi\\u200b∗(1−yw\\u200b(xi\\u200b))1−yi\\u200b)=i=1∑n\\u200blog(yw\\u200b(xi\\u200b)yi\\u200b∗(1−yw\\u200b(xi\\u200b))1−yi\\u200b)=i=1∑n\\u200b(log(yw\\u200b(xi\\u200b)yi\\u200b)+log(1−yw\\u200b(xi\\u200b))1−yi\\u200b)=i=1∑n\\u200b(yi\\u200b∗log(yw\\u200b(xi\\u200b))+(1−yi\\u200b)∗log(1−yw\\u200b(xi\\u200b)))\\u200b\\u200b\\n<p>由于取对数不改变原函数的单调性，因此之前我们需要最大化 PPP，取对数后则为最大化 logPlogPlogP，但为了更好地定义&#34;损失&#34;的含义，我们希望将极大值问题转换为极小值问题，因此取 −logP-logP−logP，得到我们的交叉熵损失函数：</p>\\nCross\\xa0Entropy\\xa0Loss=−∑i=1n(yi∗log(yw(xi))+(1−yi)∗log(1−yw(xi)))Cross\\\\space Entropy \\\\space Loss = -\\\\sum^{n}_{i=1}(y_i*log(y_{\\\\bold{w}}(\\\\bold{x_i})) + (1-y_i)*log(1-y_{\\\\bold{w}}(\\\\bold{x_i})))Cross\\xa0Entropy\\xa0Loss=−i=1∑n\\u200b(yi\\u200b∗log(yw\\u200b(xi\\u200b))+(1−yi\\u200b)∗log(1−yw\\u200b(xi\\u200b)))\\n<ul>\\n<li>nnn：模型的参数总数</li>\\n<li>yiy_iyi\\u200b：样本 i 对应的真实标签（0 or 1）</li>\\n<li>yw(x1)y_{\\\\bold{w}}(\\\\bold{x_1})yw\\u200b(x1\\u200b)：模型预测样本为标签1的概率</li>\\n</ul>\\n<p>到此为止，就得到了交叉熵损失函数，同时也可以看出基于极大似然估计法的推导过程还是相对比较浅显易懂的；此外，我们一般会取均值，也就是在上面的公式基础上再乘以 1n\\\\frac{1}{n}n1\\u200b。</p>\\n<h2>梯度下降法求解逻辑回归</h2>\\n<p>为了让模型最优化，得到使得损失函数最小的参数值，对于二元逻辑回归来说，有多种方法可以用来求解参数，最常见的有梯度下降法(Gradient Descent)、坐标下降法(Coordinate Descent)、牛顿法(Newton-Raphson method)等，其中以梯度下降法最为著名，每种方法都涉及较为复杂的数学原理，但这些计算在执行的任务其实是类似的，本文主要对梯度下降的求解来进行主要说明。</p>\\n<p><strong>梯度下降</strong>：梯度下降法也相对很容易理解，当只有一个参数时，所谓的梯度不过就是损失函数图像中某一点的斜率，当该点在最优解右侧时，斜率为正值，当该点在最优解左侧时，斜率为负值，同时由于损失函数为凸函数，因此当前位置离最优值越远，相应的斜率也就越大；我们将斜率乘以一个我们自己定义的超参数，将其称为步长，每次迭代中让参数值减去步长就能达到逐步逼近最优解的效果；对于多个参数也同理，我们对每个参数分别求损失函数的偏导数再将各个参数进行分别更新即可。</p>\\n<p><strong>梯度下降求解逻辑回归过程</strong></p>\\n<p>易得有如下变换：</p>\\nlog(yw(xi))=log(11+e−(wTxi+w0))=−log(1+e−(wTxi+w0))log(1−yw(xi))=log(1−11+e−(wTxi+w0))=log(e−(wTxi+w0)1+e−(wTxi+w0))=log(e−(wTxi+w0))−log(1+e−(wTxi+w0))=−(wTxi+w0)−log(1+e−(wTxi+w0))\\\\begin{align}\\nlog(y_{\\\\bold{w}(\\\\bold{x_i})})\\n&amp;= log(\\\\frac{1}{1+e^{-{(\\\\bold{w}^T\\\\bold{x_i}+w_0)}}}) = -log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}) \\\\\\\\\\n\\nlog(1-y_{\\\\bold{w}}(\\\\bold{x_i})) &amp;= log (1-\\\\frac{1}{1+e^{-{(\\\\bold{w}^T\\\\bold{x_i}+w_0)}}}) = log (\\\\frac{e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}}{1+e^{-{(\\\\bold{w}^T\\\\bold{x_i}+w_0)}}}) \\\\\\\\ \\n&amp;= log(e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}) -log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}) \\\\\\\\\\n&amp;= -(\\\\bold{w}^T\\\\bold{x_i}+w_0) - log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)})\\n\\\\end{align}log(yw(xi\\u200b)\\u200b)log(1−yw\\u200b(xi\\u200b))\\u200b=log(1+e−(wTxi\\u200b+w0\\u200b)1\\u200b)=−log(1+e−(wTxi\\u200b+w0\\u200b))=log(1−1+e−(wTxi\\u200b+w0\\u200b)1\\u200b)=log(1+e−(wTxi\\u200b+w0\\u200b)e−(wTxi\\u200b+w0\\u200b)\\u200b)=log(e−(wTxi\\u200b+w0\\u200b))−log(1+e−(wTxi\\u200b+w0\\u200b))=−(wTxi\\u200b+w0\\u200b)−log(1+e−(wTxi\\u200b+w0\\u200b))\\u200b\\u200b\\n<ul>\\n<li>yw(xi)y_{\\\\bold{w}}(\\\\bold{x_i})yw\\u200b(xi\\u200b)：模型对于第 i 个样本输出为对应真实标签的概率</li>\\n<li>yiy_iyi\\u200b：第 i 个样本对应的真实标签（0 or 1）</li>\\n<li>w\\\\bold{w}w：参数向量</li>\\n<li>xi\\\\bold{x_i}xi\\u200b：第 i 个样本对应的特征向量</li>\\n</ul>\\n<p>对损失函数进行变换以便于求导，有：</p>\\nJ(w)=−1n∑i=1n(yi∗log(yw(xi))+(1−yi)∗log(1−yw(xi)))=−1n∑i=1n(−yi∗log(1+e−(wTxi+w0))+(1−yi)∗(−(wTxi+w0)−log(1+e−(wTxi+w0))))=−1n∑i=1n(−yi∗log(1+e−(wTxi+w0)−(1−yi)∗log(1+e−(wTxi+w0))−(1−yi)(wTxi+w0))=1n∑i=1n((1−yi)(wTxi+w0)−log(1+e−(wTxi+w0)))\\\\begin{align}\\nJ(\\\\bold{w}) \\n&amp;= -\\\\frac{1}{n}\\\\sum^{n}_{i=1}(y_i*log(y_{\\\\bold{w}}(\\\\bold{x_i})) + (1-y_i)*log(1-y_{\\\\bold{w}}(\\\\bold{x_i}))) \\\\\\\\\\n&amp;= -\\\\frac{1}{n}\\\\sum^n_{i=1}(-y_i * log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}) + (1-y_i) * (-(\\\\bold{w}^T\\\\bold{x_i}+w_0) - log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}))) \\\\\\\\\\n&amp;= -\\\\frac{1}{n}\\\\sum^n_{i=1}(-y_i*log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)} - (1-y_i)*log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}) - (1-y_i)(\\\\bold{w}^T\\\\bold{x_i}+w_0)) \\\\\\\\\\n&amp;= \\\\frac{1}{n}\\\\sum^n_{i=1}((1-y_i)(\\\\bold{w}^T\\\\bold{x_i}+w_0) - log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)}))\\n\\\\end{align}J(w)\\u200b=−n1\\u200bi=1∑n\\u200b(yi\\u200b∗log(yw\\u200b(xi\\u200b))+(1−yi\\u200b)∗log(1−yw\\u200b(xi\\u200b)))=−n1\\u200bi=1∑n\\u200b(−yi\\u200b∗log(1+e−(wTxi\\u200b+w0\\u200b))+(1−yi\\u200b)∗(−(wTxi\\u200b+w0\\u200b)−log(1+e−(wTxi\\u200b+w0\\u200b))))=−n1\\u200bi=1∑n\\u200b(−yi\\u200b∗log(1+e−(wTxi\\u200b+w0\\u200b)−(1−yi\\u200b)∗log(1+e−(wTxi\\u200b+w0\\u200b))−(1−yi\\u200b)(wTxi\\u200b+w0\\u200b))=n1\\u200bi=1∑n\\u200b((1−yi\\u200b)(wTxi\\u200b+w0\\u200b)−log(1+e−(wTxi\\u200b+w0\\u200b)))\\u200b\\u200b\\n<ul>\\n<li>yw(xi)y_{\\\\bold{w}}(\\\\bold{x_i})yw\\u200b(xi\\u200b)：模型对于第 i 个样本输出为对应真实标签的概率</li>\\n<li>yiy_iyi\\u200b：第 i 个样本对应的真实标签（0 or 1）</li>\\n<li>w\\\\bold{w}w：参数向量</li>\\n<li>xi\\\\bold{x_i}xi\\u200b：第 i 个样本对应的特征向量</li>\\n</ul>\\n<p>以某数据集中第 j 个参数为例，模型在某次迭代时计算得到损失函数后，该函数对第 j 个参数求偏导，有：</p>\\n∂J(w)∂wj=∂∂wj1n∑i=1n((1−yi)(wTxi+w0)−log(1+e−(wTxi+w0)))=1n∑i=1n((1−yi)∂∂wj(wTxi+w0)−∂∂wjlog(1+e−(wTxi+w0)))=1n∑i=1n(xij(1−yi)−xij∗e−(wTxi+w0)1+e−(wTxi+w0))=1n∑i=1n(xij−xij∗e−(wTxi+w0)1+e−(wTxi+w0)−yi∗xij)=1n∑i=1nxij(11+e−(wTxi+w0)−yi)=1n∑i=1nxij(yw(xi)−yi)\\\\begin{align}\\n\\\\frac{\\\\partial J(\\\\bold{w})}{\\\\partial w_j} \\n&amp;= \\\\frac{\\\\partial}{\\\\partial w_j} \\\\frac{1}{n}\\\\sum^n_{i=1}((1-y_i)(\\\\bold{w}^T\\\\bold{x_i}+w_0) - log(1+e^{-(\\\\bold{w}^T\\\\bold{x_i}+w_0)})) \\\\\\\\\\n&amp;= \\\\frac{1}{n}\\\\sum^n_{i=1}((1-y_i)\\\\frac{\\\\partial}{\\\\partial w_j}(\\\\bold{w}^T\\\\bold{x_i}+w_0) - \\\\frac{\\\\partial}{\\\\partial w_j}log(1+e^{-(\\\\bold{w}^T\\\\bold{x_{i}}+w_0)})) \\\\\\\\\\n&amp;= \\\\frac{1}{n}\\\\sum^n_{i=1}(x_{ij}(1-y_i) - \\\\frac{x_{ij}*e^{-(\\\\bold{w}^T\\\\bold{x_{i}}+w_0)}}{1+e^{-(\\\\bold{w}^T\\\\bold{x_{i}}+w_0)}}) \\\\\\\\\\n&amp;= \\\\frac{1}{n}\\\\sum^n_{i=1}(x_{ij} - \\\\frac{x_{ij} * e^{-(\\\\bold{w}^T\\\\bold{x_{i}}+w_0)}}{1+e^{-(\\\\bold{w}^T\\\\bold{x_{i}}+w_0)}} - y_i*x_{ij}) \\\\\\\\\\n&amp;= \\\\frac{1}{n}\\\\sum^n_{i=1}x_{ij}(\\\\frac{1}{1+e^{-(\\\\bold{w}^T\\\\bold{x_{i}}+w_0)}} - y_i) \\\\\\\\\\n&amp;= \\\\frac{1}{n}\\\\sum^n_{i=1}x_{ij}(y_{\\\\bold{w}}(\\\\bold{x_i}) - y_i)\\n\\\\end{align}∂wj\\u200b∂J(w)\\u200b\\u200b=∂wj\\u200b∂\\u200bn1\\u200bi=1∑n\\u200b((1−yi\\u200b)(wTxi\\u200b+w0\\u200b)−log(1+e−(wTxi\\u200b+w0\\u200b)))=n1\\u200bi=1∑n\\u200b((1−yi\\u200b)∂wj\\u200b∂\\u200b(wTxi\\u200b+w0\\u200b)−∂wj\\u200b∂\\u200blog(1+e−(wTxi\\u200b+w0\\u200b)))=n1\\u200bi=1∑n\\u200b(xij\\u200b(1−yi\\u200b)−1+e−(wTxi\\u200b+w0\\u200b)xij\\u200b∗e−(wTxi\\u200b+w0\\u200b)\\u200b)=n1\\u200bi=1∑n\\u200b(xij\\u200b−1+e−(wTxi\\u200b+w0\\u200b)xij\\u200b∗e−(wTxi\\u200b+w0\\u200b)\\u200b−yi\\u200b∗xij\\u200b)=n1\\u200bi=1∑n\\u200bxij\\u200b(1+e−(wTxi\\u200b+w0\\u200b)1\\u200b−yi\\u200b)=n1\\u200bi=1∑n\\u200bxij\\u200b(yw\\u200b(xi\\u200b)−yi\\u200b)\\u200b\\u200b\\n<ul>\\n<li>yw(xi)y_{\\\\bold{w}}(\\\\bold{x_i})yw\\u200b(xi\\u200b)：模型对于第 i 个样本输出为对应真实标签的概率</li>\\n<li>yiy_iyi\\u200b：第 i 个样本对应的真实标签（0 or 1）</li>\\n<li>w\\\\bold{w}w：参数向量</li>\\n<li>xi\\\\bold{x_i}xi\\u200b：第 i 个样本对应的特征向量</li>\\n<li>xijx_{ij}xij\\u200b：第 i 个样本中第 j 个特征的值</li>\\n</ul>\\n<p>即某次迭代中，对于第 j 个参数求偏导，得到的对应梯度为：</p>\\n∂J(w)∂wj=1n∑i=1nxij(yw(xi)−yi)\\\\frac{\\\\partial J(\\\\bold{w})}{\\\\partial w_j} = \\\\frac{1}{n}\\\\sum^n_{i=1}x_{ij}(y_{\\\\bold{w}}(\\\\bold{x_i}) - y_i)∂wj\\u200b∂J(w)\\u200b=n1\\u200bi=1∑n\\u200bxij\\u200b(yw\\u200b(xi\\u200b)−yi\\u200b)\\n<ul>\\n<li>yw(xi)y_{\\\\bold{w}}(\\\\bold{x_i})yw\\u200b(xi\\u200b)：模型对于第 i 个样本输出为对应真实标签的概率</li>\\n<li>yiy_iyi\\u200b：第 i 个样本对应的真实标签（0 or 1）</li>\\n<li>w\\\\bold{w}w：参数向量</li>\\n<li>xi\\\\bold{x_i}xi\\u200b：第 i 个样本对应的特征向量</li>\\n<li>xijx_{ij}xij\\u200b：第 i 个样本中第 j 个特征的值</li>\\n</ul>\\n<p>对于数据集中某次迭代中所有参数组成的参数向量对应的梯度向量，使用如下方式表示：</p>\\n▽J(w)T=[∂J(w)∂w1∂J(w)∂w2⋯∂J(w)∂wm]\\\\triangledown J(\\\\bold{w})^T = \\\\begin{bmatrix} \\\\frac{\\\\partial J(\\\\bold{w})}{\\\\partial w_1} &amp; \\\\frac{\\\\partial J(\\\\bold{w})}{\\\\partial w_2}  &amp; \\\\cdots &amp; \\\\frac{\\\\partial J(\\\\bold{w})}{\\\\partial w_m} \\\\end{bmatrix}▽J(w)T=[∂w1\\u200b∂J(w)\\u200b\\u200b∂w2\\u200b∂J(w)\\u200b\\u200b⋯\\u200b∂wm\\u200b∂J(w)\\u200b\\u200b]\\n<p>在计算得到对应梯度后，对于参数向量的第 i+1 次迭代，有：</p>\\nwi+1=wi−α∗▽J(wi)\\\\begin{align}\\n\\\\bold{w_{i+1}} \\n&amp;= \\\\bold{w_i} - \\\\alpha * \\\\triangledown J(\\\\bold{w_i})\\n\\\\end{align}wi+1\\u200b\\u200b=wi\\u200b−α∗▽J(wi\\u200b)\\u200b\\u200b\\n<ul>\\n<li>α\\\\alphaα：超参数，用来控制每次迭代中步长的大小</li>\\n<li>wi\\\\bold{w_i}wi\\u200b：迭代 i 次后的参数向量</li>\\n<li>▽J(wi)\\\\triangledown J(\\\\bold{w_i})▽J(wi\\u200b)：迭代 i 次后的梯度向量</li>\\n</ul>\\n<p>当超参数 α\\\\alphaα 设置过大时，步长过大，容易越过最优解，且由于凸函数中离最优解越远，梯度越大的性质，可能会发生损失随着迭代次数的增加逐步增大的效果；同理，当步长过小时，则可能需要更长的时间才能达到最优解。</p>\\n<p><strong>分批计算梯度</strong>：对于用于计算梯度的损失函数的计算，我们也一般不会在每次迭代中让数据集中的所有样本都参与计算，折回浪费很多的算力资源和时间，我们更常将数据集中的样本分为几组（bath），每次迭代中仅采用部分数据（某个bath）进行计算和迭代。</p>\\n<p>梯度下降的理解还是相对容易的，无非就是求出每个参数对应的梯度，然后对参数进行更新，主要就是这两步，对于判断是否收敛，我们可以设置最大迭代次数，也可以对梯度或步长设置阈值等操作。</p>\\n<h2>逻辑回归基本原理总结</h2>\\n<p>逻辑回归模型核心内容在于其基本形式、损失函数以及模型求解，已知这三个过程的原理后，其拟合过程和其它模型思路基本相同；对于二分类任务，基于梯度下降来求解逻辑回归的一般思路：</p>\\n<ol>\\n<li>准备数据集，随机初始化参数</li>\\n<li>对于每次迭代，基于交叉熵损失函数计算全部(或部分)样本的损失和每个参数对应的梯度</li>\\n<li>更新参数</li>\\n<li>若模型收敛或达到最大迭代数（或其它判定方法），则模型训练结束</li>\\n</ol>\\n<h1>逻辑回归原理拓展</h1>\\n<p>除了逻辑回归模型的基本原理外，包括但也不限于逻辑回归等很多模型也常引入正则化项来调整模型拟合程度；此外，逻辑回归也不仅只能应用于二分类任务，我们也可以通过其他常见方法将其拓展到多分类任务，这里主要介绍常见的 OvR 和 MvM 两种常见方法。</p>\\n<h2>正则化</h2>\\n<p><strong>正则化与正则项</strong>：正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量 w\\\\bold{w}w 的L1范式和L2范式的倍数来实现。这个增加的范式，被称为&#34;正则项&#34;，也被称为&#34;惩罚项&#34;。随着损失函数改变，基于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范式表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值，如下所示：</p>\\nL1=∑i=1n∣wi∣L2=∑i=1n(wi2)\\\\begin{align}\\nL1 &amp;= \\\\sum^{n}_{i=1}|w_i| \\\\\\\\\\nL2 &amp;= \\\\sqrt{\\\\sum^n_{i=1}(w_i^2)}\\n\\\\end{align}L1L2\\u200b=i=1∑n\\u200b∣wi\\u200b∣=i=1∑n\\u200b(wi2\\u200b)\\u200b\\u200b\\u200b\\n<ul>\\n<li>nnn：某数据集下的样本数</li>\\n<li>wiw_iwi\\u200b 第 i 个参数（权重系数），注意这里 i 是从1开始取值的，不包括截距 w0w_0w0\\u200b</li>\\n</ul>\\n<p>设原来的损失函数为 J(w)J(\\\\bold{w})J(w)，则加上正则项之后的损失函数则为：</p>\\nJ(w)L1=C∗J(w)+∑i=1n∣wi∣J(w)L2=C∗J(w)+∑i=1n(wi2)\\\\begin{align}\\nJ(\\\\bold{w})_{L1} &amp;= C * J(\\\\bold{w}) + \\\\sum^{n}_{i=1}|w_i| \\\\\\\\\\nJ(\\\\bold{w})_{L2} &amp;= C * J(\\\\bold{w}) + \\\\sqrt{\\\\sum^n_{i=1}(w_i^2)}\\n\\\\end{align}J(w)L1\\u200bJ(w)L2\\u200b\\u200b=C∗J(w)+i=1∑n\\u200b∣wi\\u200b∣=C∗J(w)+i=1∑n\\u200b(wi2\\u200b)\\u200b\\u200b\\u200b\\n<ul>\\n<li>CCC：超参数，用来调整正则化程度（模型拟合程度）</li>\\n<li>J(w)J(\\\\bold{w})J(w)：交叉熵损失函数</li>\\n<li>nnn：模型的参数总数</li>\\n<li>wiw_iwi\\u200b 第 i 个参数（权重系数），注意这里 i 是从1开始取值的，不包括截距 w0w_0w0\\u200b</li>\\n</ul>\\n<p>超参数C控制原来交叉熵损失的大小，本质上是调整正则项的相对大小，直接从公式上进行理解，不难看出所谓的正则项不过就是将各参数（权重系数）以某种形式进行累加得到的一个范式，该范式随着各权重参数整体取值的增大而增大。</p>\\n<img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/97d7c987e1384928881ff943529b2461~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=969&amp;h=376&amp;s=96740&amp;e=png&amp;b=ebeaf2\" alt=\"image.png\" loading=\"lazy\"/>\\n<p>上图涉及的相关绘图代码以供参考（可忽略并折叠）：</p>\\n<pre><code>import numpy as np\\nimport pandas as pd\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\n\\nplt.style.use(&#39;seaborn&#39;)\\nmpl.rcParams.update({\\n    &#39;font.family&#39;: &#39;Microsoft YaHei&#39;,\\n    &#39;mathtext.fontset&#39;: &#39;stix&#39;,\\n})\\n\\n\\nx = np.arange(0.05, 5, 0.05)\\ny = np.log(x)\\nplt.figure(figsize=(6,4))\\nplt.plot(x, y, color=&#39;blue&#39;, alpha=0.6)\\nplt.scatter(x + rng.normal(0, 0.5, len(x)), y, s=30, c=&#39;red&#39;,alpha=0.6, edgecolor=&#39;white&#39;, linewidth=1)\\nplt.show()\\n</code></pre>\\n<p>对于不包含正则项的损失函数，只需要将交叉熵损失最优化即可，但这样很容易造成模型过拟合，我们想要降低模型拟合程度，因此加入正则项，模型在拟合时不仅要考虑原来的交叉熵损失，还要考虑各权重系数的大小所造成的损失，需要在这两个损失之间进行权衡，以达到总体损失最小。</p>\\n<p><strong>从权重系数方面来看</strong>：在一次函数中就是斜率，其实在拥有多个权重系数、多个自变量的线性回归中也是一样的，当权重系数增大时，拟合函数的 &#34;斜率&#34; 也会相应增大，模型过拟合时拟合的效果本质上也就是因为这种 &#34;斜率&#34; 过大，我们通过限定各权重参数的大小来防止 &#34;斜率&#34; 过大，从而达到防止过拟合的效果。</p>\\n<img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b8faf8892c844a629ffebb67306730d3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1478&amp;h=522&amp;s=75236&amp;e=png&amp;b=fefefe\" alt=\"image.png\" loading=\"lazy\"/>\\n<p><strong>从损失函数方面来看</strong>：通过引入正则项，本质上是改变了模型优化目标的最优点，使得真正的优化目标（权衡欠拟合和过拟合后的最佳效果）作为模型优化目标。</p>\\n<p>当然不是带正则化的损失函数都可以将目标效果设置为我们的优化目标，这取决于超参数的选择，当正则项对损失贡献占比过大时，可能会越过真正的目标点，造成模型欠拟合，同理，当过小时，则可能会造成模型仍然过拟合。</p>\\n<p>在网络上也常能看到基于L1范式和L2范式的其它形式的正则项，例如：</p>\\nJ(w)L1=J(w)+λn∑i=1n∣wi∣J(w)L2=J(w)+λ2n∑i=1nwi2\\\\begin{align}\\nJ(\\\\bold{w})_{L1} &amp;= J(\\\\bold{w}) + \\\\frac{\\\\lambda}{n}\\\\sum^{n}_{i=1}|w_i| \\\\\\\\\\nJ(\\\\bold{w})_{L2} &amp;= J(\\\\bold{w}) + \\\\frac{\\\\lambda}{2n}\\\\sum^n_{i=1}w_i^2\\n\\\\end{align}J(w)L1\\u200bJ(w)L2\\u200b\\u200b=J(w)+nλ\\u200bi=1∑n\\u200b∣wi\\u200b∣=J(w)+2nλ\\u200bi=1∑n\\u200bwi2\\u200b\\u200b\\u200b\\n<ul>\\n<li>λ\\\\lambdaλ：超参数，用来调整正则化强度</li>\\n<li>J(w)J(\\\\bold{w})J(w)：交叉熵损失函数</li>\\n<li>nnn：模型的参数总数</li>\\n<li>wiw_iwi\\u200b 第 i 个参数（权重系数），注意这里 i 是从1开始取值的，不包括截距 w0w_0w0\\u200b</li>\\n</ul>\\n<p>这里通过超参数 λ\\\\lambdaλ 来直接调整正则项对损失的贡献占比，此外这里L2范数的正则化项分母为 2n2n2n 是为了便于求导（有平方项，求导后正好把常数2约去，便于计算）。</p>\\n<p>无论正则项的形式如何变化，引入的目的都是为了防止模型过拟合，将目标效果作为模型本身优化的方向，以达到更低的泛化误差。</p>\\n<p><strong>L1正则化和L2正则化的选择</strong>：L1正则化和L2正则化虽然都可以控制过拟合，但效果并不相同；当正则化强度逐渐增大（即 CCC 逐渐变小或 λ\\\\lambdaλ 逐渐增大）， 参数（各权重系数）的取值会逐渐变小，但L1正则化会更趋于将参数压缩为0，L2正则化则趋于让参数尽量小，但不会取到0。</p>\\n<p>相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但 是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。</p>\\n<p><strong>特征选择</strong>：由于L1正则化和L2正则化在效果上的不同，在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，因此L1正则化本质就是一个特征选择的过程，一定程度上影响了参数的&#34;稀疏性&#34;。L1正则化越强，参数向量中就含有越多的0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，若特征量很大，数据维度很高，我们会倾向于使用L1正则化，反之则一般选用L2正则化；由于L1正则化的这个性质，在对逻辑回归基于嵌入法进行特征选择时，一般使用引入L1正则项的逻辑回归。</p>\\n<h2>多分类的逻辑回归</h2>\\n<h3>OvR：One vs Rest</h3>\\n<img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46e968bb184f4c6891e4301a147afd2e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=701&amp;h=488&amp;s=19577&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/>\\n<p><strong>基本思想</strong>：One（一个）-Vs-Rest（其余部分）的思想是把一个多分类的问题变成一个二分类问题；转变的思路就如同方法名称描述的那样，选择其中一个类别为正类（Positive），使其他所有类别为负类（Negative）；例如我们可以将三角形所代表的类别视为正类，其他类别全部视为负类。</p>\\n<p>由于我们将其中一个类别作为正类，其它所有类别作为负类，因此要想解决多分类问题的话，假设有 nnn 个类别，那就需要有 nnn 个以不同类别作为正类的分类器，训练结束后对模型输入某样本对应特征向量，这 nnn 个分类器分别输出各自对应正类的概率，最后选择概率最大的作为我们的分类结果输出。</p>\\n<p>基于OvR这种思想很容易可以把我们的逻辑回归从二分类拓展到多分类问题上，但缺点也较为明显，对于每个分类器，我们选择数据集中其中一个类别作为正类，其它所有的类别都作为负类，当类别较多时，这很容易造成样本不均衡的问题，即很容易出现负类样本远高于正样本数量的问题，我们当然也可以采取一些例如上采样的方法来解决这个问题，但效果肯定没有相同分布下的均衡数据集的效果要好，一次你使用这种方法时可以事先检查一下样本不均衡是否严重，然后再决定是否选择该方法。</p>\\n<h3>MvM：Multinomial</h3>\\n<p>回顾上边sigmoid函数推导过程中，有：</p>\\ny=ln(z1−z)=ln(p(yi=1)p(yi=0))=wTx\\\\begin{align}\\ny=ln(\\\\frac{z}{1-z}) = ln(\\\\frac{p(y_i=1)}{p(y_i=0)}) = \\\\bold{w}^T\\\\bold{x}\\n\\\\end{align}y=ln(1−zz\\u200b)=ln(p(yi\\u200b=0)p(yi\\u200b=1)\\u200b)=wTx\\u200b\\u200b\\n<p>对于所有 K 个可能的分类结果，我们运行 K−1 个独立二元逻辑回归模型，在运行过程中把其中一个类别看成是主类别，然后将其它K−1个类别和我们所选择的主类别分别进行回归；通过这样的方式，例如这里选择结果 K 作为主类别，对于第 i 个样本对应的特征向量，我们可以得到如下公式：</p>\\n{logp(yi=1)p(yi=K)=w1Txilogp(yi=2)p(yi=K)=w2Txi⋮logp(yi=K−1)p(yi=K)=wK−1Txi⟶{p(yi=1)=p(yi=K)ew1Txip(yi=2)=p(yi=K)ew2Txi⋮p(yi=K−1)=p(yi=K)ewK−1Txi\\\\begin{cases}\\nlog\\\\frac{p(y_i=1)}{p(y_i=K)} = \\\\bold{w_1}^T\\\\bold{x_i} \\\\\\\\\\nlog\\\\frac{p(y_i=2)}{p(y_i=K)} = \\\\bold{w_2}^T\\\\bold{x_i} \\\\\\\\\\n\\\\quad \\\\quad \\\\vdots \\\\\\\\\\nlog\\\\frac{p(y_i=K-1)}{p(y_i=K)} = \\\\bold{w_{K-1}}^T\\\\bold{x_i} \\\\\\\\\\n\\\\end{cases}\\n\\\\longrightarrow\\n\\\\begin{cases}\\np(y_i=1) = p(y_i=K)e^{\\\\bold{w_1}^T\\\\bold{x_i}} \\\\\\\\\\np(y_i=2) = p(y_i=K)e^{\\\\bold{w_2}^T\\\\bold{x_i}} \\\\\\\\\\n\\\\quad \\\\quad \\\\vdots \\\\\\\\\\np(y_i=K-1) = p(y_i=K)e^{\\\\bold{w_{K-1}}^T\\\\bold{x_i}} \\\\\\\\\\n\\\\end{cases}⎩⎨⎧\\u200blogp(yi\\u200b=K)p(yi\\u200b=1)\\u200b=w1\\u200bTxi\\u200blogp(yi\\u200b=K)p(yi\\u200b=2)\\u200b=w2\\u200bTxi\\u200b⋮logp(yi\\u200b=K)p(yi\\u200b=K−1)\\u200b=wK−1\\u200bTxi\\u200b\\u200b⟶⎩⎨⎧\\u200bp(yi\\u200b=1)=p(yi\\u200b=K)ew1\\u200bTxi\\u200bp(yi\\u200b=2)=p(yi\\u200b=K)ew2\\u200bTxi\\u200b⋮p(yi\\u200b=K−1)=p(yi\\u200b=K)ewK−1\\u200bTxi\\u200b\\u200b\\n<p>由于所有可能的结果对应的概率相加为1，则有：</p>\\n∑i=1Kp(yi=i)=p(yi=K)+∑i=1K−1p(yi=i)=p(yi=K)+∑i=1K−1p(yi=K)ewiTxi\\\\begin{align}\\n\\\\sum^{K}_{i=1} p(y_i=i) \\n&amp;= p(y_i=K) + \\\\sum^{K-1}_{i=1}p(y_i=i) \\\\\\\\\\n&amp;= p(y_i=K) + \\\\sum^{K-1}_{i=1}p(y_i=K)e^{\\\\bold{w_i}^T\\\\bold{x_i}} \\\\\\\\\\n\\\\end{align}i=1∑K\\u200bp(yi\\u200b=i)\\u200b=p(yi\\u200b=K)+i=1∑K−1\\u200bp(yi\\u200b=i)=p(yi\\u200b=K)+i=1∑K−1\\u200bp(yi\\u200b=K)ewi\\u200bTxi\\u200b\\u200b\\u200b\\n<p>概率全部相加为1，我们可以利用该等式解出 p(yi=k)p(y_i=k)p(yi\\u200b=k)，即：</p>\\np(yi=K)+∑i=1K−1p(yi=K)ewiTxi=1p(yi=K)+p(yi=K)∑i=1K−1ewiTxi=1(1+∑i=1K−1ewiTxi)p(yi=L)=1p(yi=K)=(1+∑i=1K−1ewiTxi)−1\\\\begin{align}\\np(y_i=K) + \\\\sum^{K-1}_{i=1}p(y_i=K)e^{\\\\bold{w_i}^T\\\\bold{x_i}} &amp;= 1 \\\\\\\\\\np(y_i=K) + p(y_i=K)\\\\sum^{K-1}_{i=1}e^{\\\\bold{w_i}^T\\\\bold{x_i}} &amp;= 1 \\\\\\\\\\n(1 + \\\\sum^{K-1}_{i=1}e^{\\\\bold{w_i}^T\\\\bold{x_i}})p(y_i=L) &amp;= 1 \\\\\\\\\\np(y_i = K) &amp;= (1 + \\\\sum^{K-1}_{i=1}e^{\\\\bold{w_i}^T\\\\bold{x_i}})^{-1}\\n\\\\end{align}p(yi\\u200b=K)+i=1∑K−1\\u200bp(yi\\u200b=K)ewi\\u200bTxi\\u200bp(yi\\u200b=K)+p(yi\\u200b=K)i=1∑K−1\\u200bewi\\u200bTxi\\u200b(1+i=1∑K−1\\u200bewi\\u200bTxi\\u200b)p(yi\\u200b=L)p(yi\\u200b=K)\\u200b=1=1=1=(1+i=1∑K−1\\u200bewi\\u200bTxi\\u200b)−1\\u200b\\u200b\\n<p>我们再把该式子代入到原来的 K-1 个式子中得到：</p>\\np(yi=K)=(1+∑i=1K−1ewiTxi)−1⟶{p(yi=1)=ew1Txi1+∑i=1K−1ewiTxip(yi=2)=ew2Txi1+∑i=1K−1ewiTxi⋮p(yi=K−1)=ewK−1Txi1+∑i=K−1K−1ewiTxip(y_i = K) = (1 + \\\\sum^{K-1}_{i=1}e^{\\\\bold{w_i}^T\\\\bold{x_i}})^{-1}  \\\\longrightarrow\\n\\\\begin{cases}\\np(y_i=1) = \\\\frac{e^{\\\\bold{w_1}^T\\\\bold{x_i}}}{1 + \\\\sum^{K-1}_{i=1}e^{\\\\bold{w_i}^T\\\\bold{x_i}}} \\\\\\\\\\np(y_i=2) = \\\\frac{e^{\\\\bold{w_2}^T\\\\bold{x_i}}}{1 + \\\\sum^{K-1}_{i=1}e^{\\\\bold{w_i}^T\\\\bold{x_i}}} \\\\\\\\\\n\\\\quad \\\\quad \\\\vdots \\\\\\\\\\np(y_i=K-1) = \\\\frac{e^{\\\\bold{w_{K-1}}^T\\\\bold{x_i}}}{1 + \\\\sum^{K-1}_{i=K-1}e^{\\\\bold{w_i}^T\\\\bold{x_i}}} \\\\\\\\\\n\\\\end{cases}p(yi\\u200b=K)=(1+i=1∑K−1\\u200bewi\\u200bTxi\\u200b)−1⟶⎩⎨⎧\\u200bp(yi\\u200b=1)=1+∑i=1K−1\\u200bewi\\u200bTxi\\u200bew1\\u200bTxi\\u200b\\u200bp(yi\\u200b=2)=1+∑i=1K−1\\u200bewi\\u200bTxi\\u200bew2\\u200bTxi\\u200b\\u200b⋮p(yi\\u200b=K−1)=1+∑i=K−1K−1\\u200bewi\\u200bTxi\\u200bewK−1\\u200bTxi\\u200b\\u200b\\u200b\\n<p>至此，我们就有了基于K-1个独立的二元逻辑回归模型下对于每一个类别的预测概率，接下来的步骤就和sigmoid函数推导部分中介绍的一样了，也就是对每一个类别所对应的式子分别求其反函数后可以得到每一类别与各个 wiTx\\\\bold{w_i}^T\\\\bold{x}wi\\u200bTx 的关系，也就是可以基于这K-1个独立的模型推出每一类别的概率，之后对于每个独立模型分别计算损失和进行梯度下降即可。</p>\\n<h1>总结</h1>\\n<p>对于本文在最开始提出的问题，基于本文内容在此做简要总结：</p>\\n<ul>\\n<li>为什么需要掌握逻辑回归？\\n<ul>\\n<li>逻辑回归如今在金融领域使用的仍然很多，此外，它也是后续学习中必不可少的基础。</li>\\n</ul>\\n</li>\\n<li>sigmoid函数是怎么来的？\\n<ul>\\n<li>基于概率知识进行推导，此外，sigmoid在很大程度上解决了线性判别函数所存在的问题。</li>\\n</ul>\\n</li>\\n<li>交叉熵损失函数是怎么来的？\\n<ul>\\n<li>基于极大似然估计法进行推导，当然也不仅限于这一个推导或解释方法。</li>\\n</ul>\\n</li>\\n<li>怎么使用梯度下降对逻辑回归进行求解？\\n<ul>\\n<li>对于每次迭代，基于损失函数求每个参数的偏导数，通过多次迭代来更新参数以寻找全局最优解。</li>\\n</ul>\\n</li>\\n<li>逻辑回归通过什么方法减少过拟合程度？\\n<ul>\\n<li>正则化，可以引入l1、l2正则化来调整模型拟合程度，此外，由于l1正则化本身的性质，其也可以用来进行特征选择。</li>\\n</ul>\\n</li>\\n<li>逻辑回归怎么完成多分类任务？\\n<ul>\\n<li>常用的包括但不限于OvR和MvM这两种方法，这两种方法都通过建立多个模型将原来的二元逻辑回归拓展到了多分类任务上，但一般情况下MvM有着更严谨的理论推导和模型效果。</li>\\n</ul>\\n</li>\\n</ul>\\n<h1>Reference</h1>\\n<ul>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fqq_38890412%2Farticle%2Fdetails%2F106149936\" title=\"https://blog.csdn.net/qq_38890412/article/details/106149936\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">信息熵（香农熵）-CSDN博客</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F125753431\" title=\"https://zhuanlan.zhihu.com/p/125753431\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">交叉熵损失函数的求导(Logistic回归) - 知乎 (zhihu.com)</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fkeeppractice%2Farticle%2Fdetails%2F107088538\" title=\"https://blog.csdn.net/keeppractice/article/details/107088538\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">[机器学习-原理与实践]逻辑回归(LogisticRegression)多分类(OvR, OvO, MvM）_logisticregression函数 ovr-CSDN博客</a></li>\\n<li><a href=\"https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F78068546\" title=\"https://zhuanlan.zhihu.com/p/78068546\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">sklearn中的logisticregression如何处理多分类？作为西瓜书的补充吧 - 知乎 (zhihu.com)</a></li>\\n<li>菜菜的机器学习</li>\\n</ul>', 'author': 'rink1t', 'reading_time': 56, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 61, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'c402027dca2ec8e6e9ccc50d90a6cf3c3352918b710c8514c4cfd2c4f2d7e289', 'title': '还可以更快更强 | huggingface加载本地模型', 'url': 'https://juejin.cn/post/7283438991474950179', 'published_at': '2023-09-28T07:35:35Z', 'created_at': '2023-10-07T09:03:25.057178Z', 'changed_at': '2023-10-07T09:03:25.057178Z', 'content': '<h1>分析问题</h1>\\n<h2>问题的起源</h2>\\n<p>事情起因是这样的，我们要使用huggingface，但是日常因为联网问题无法加载模型。因为是服务器集群，搭梯子让我们在docker里弄，所以像我这种懒人，docker？算了，我还是直接手动下载吧，具体看这：<a href=\"https://juejin.cn/post/7281113851713863695\" title=\"https://juejin.cn/post/7281113851713863695\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">服务器huggingface联网通用方案</a></p>\\n<ul>\\n<li>\\n<p><strong>问题</strong>：不能联网下模型，不想搭梯子</p>\\n</li>\\n<li>\\n<p><strong>解决</strong>：缓存模型</p>\\n</li>\\n</ul>\\n<p>但是我是个懒人，我解决的方法只直接在系统默认缓存位置的<code>.cache</code>里缓存文件。</p>\\n<p>这会带来另一个问题。模型加载过程中会有很大一部分在加载模型。</p>\\n<p>因为如果你没修改过<code>from_pretrained</code>的<code>force_download (bool, optional, defaults to False)</code>，那么他加载模型的逻辑是这样的：</p>\\n<pre><code>需要加载模型的时候:\\n去缓存里找--&gt;找不到--&gt;去huggingface下载--&gt;下载不下来--&gt;去缓存里找--&gt;没找到--&gt;报错\\n    |                                                   |\\n    --&gt;找到--&gt;加载模型                           --&gt;找到--&gt;加载模型\\n</code></pre>\\n<p>我的实验用到4个预训练模型，以及n个huggingface的评价指标。</p>\\n<p><strong>这个过程会耗费大量的时间。</strong></p>\\n<p>耗费大量时间，根本无所谓，毕竟我懒，根本不想改代码。</p>\\n<p>但是。问题来了。在我6卡V100跑了两个多周之后，师兄跑来问我是不是实验卡了。</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a2b9fb89b84b40bea74a63dc347cba4d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1652&amp;h=1125&amp;s=830679&amp;e=png&amp;b=f8f6f6\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>因为他采样时候是这样的，只占一捏捏缓存，GPU是不用的。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/62379e7c1bc5421c9e96c1ef7e0510f8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=660&amp;h=558&amp;s=45849&amp;e=png&amp;b=000000\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>为了不挨骂，我决定改一下代码。</p>\\n\\n<h1>修改</h1>\\n<h2>评价指标</h2>\\n<p>先说评价指标:</p>\\n<pre><code>def compute_perplexity(all_texts_list, model_id=&#39;gpt2-large&#39;):\\n    torch.cuda.empty_cache() \\n    perplexity = load(&#34;perplexity&#34;, module_type=&#34;metric&#34;)\\n    results = perplexity.compute(predictions=all_texts_list, model_id=model_id, device=&#39;cuda&#39;)\\n    return results[&#39;mean_perplexity&#39;]\\n</code></pre>\\n<p>我这段代码里需要<strong>加载评价指标perplexity</strong>和<strong>加载模型gpt2-large</strong>。</p>\\n<p>现看加载评价指标的load：</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/73c58bbe152c411ab041d99d344ed2f9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1174&amp;h=777&amp;s=109569&amp;e=png&amp;b=fffefe\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>这里写的：</p>\\n<blockquote>\\n<p>a local path to processing script or the directory containing the script (if the script has the same name as the directory),e.g. <code>&#39;./metrics/rouge&#39;</code> or <code>&#39;./metrics/rouge/rouge.py&#39;</code></p>\\n</blockquote>\\n<p>需要你把评价指标外嵌套一个同名的文件夹。所以文件应该如下放置。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c8bc33e7eccf49eaa5ed4207003af867~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=256&amp;h=179&amp;s=6571&amp;e=png&amp;b=f3f2f2\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>为了确保准确，我这里传入绝对位置，需要借助<code>os</code></p>\\n<pre><code>\\nimport os\\ncur_dir = os.path.split(__file__)[0]\\n</code></pre>\\n<p><code>os.path.split(__file__)</code>: <code>__file__</code> 是Python中的一个内置变量，它指向当前脚本文件的路径。<code>os.path.split()</code> 函数将这个路径分为两部分，第一部分是文件所在的目录，第二部分是文件名。例如，如果 <code>__file__</code> 的值是<code>&#39;/path/to/script.py&#39;</code>，那么 <code>os.path.split(__file__)</code> 将返回 <code>(&#39;/path/to&#39;, &#39;script.py&#39;)</code>。</p>\\n<p><code>cur_dir = os.path.split(__file__)[0]</code>: 这一行代码获取了当前脚本文件所在的目录，即 <code>&#39;/path/to&#39;</code>。它将目录部分提取出来并存储在变量 <code>cur_dir</code> 中。</p>\\n<p><code>f&#34;{cur_dir}/perplexity&#34;</code>即可获得我当前目录存的perplexity。</p>\\n<h2>模型</h2>\\n<p>因为perplexity需要用到预训练模型，我们可以看到我们传入的<code>model_id</code>是模型名，点进这个计算函数看一下。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/68e4fb25ee2644f9b2031c8c0da56f53~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1038&amp;h=356&amp;s=116856&amp;e=png&amp;b=bec3d2\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>可以看到传入的<code>model_id</code>直接在这里进行了模型加载。</p>\\n<p><code>from_pretrained</code>实现代码注释这样写。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7f5d6084d25244f09f571d7d5093c011~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=980&amp;h=452&amp;s=241895&amp;e=png&amp;b=bfc4d3\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<blockquote>\\n<p>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under auser or organization name, like <code>dbmdz/bert-base-german-cased</code>.</p>\\n</blockquote>\\n<p>传入本地文件夹包含模型文件即可。</p>\\n<p>所以我们只需要将缓存中的模型及其配置新建个文件夹放进去即可。</p>\\n<p>看一下文件夹嵌套就知道为什么传入绝对地址了。</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4c874147f9f449c5b9b4dc14eb0e5c9a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=312&amp;h=388&amp;s=16832&amp;e=png&amp;b=f1f0f0\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><code>compute_perplexity</code>是evaluation.py的函数，但是这个函数引用了同目录下的perplexity/perplexity.py，但是这个perplexity.py要load上上级文件夹下的模型。为了防止混乱，我们还是使用<code>os.path</code>获取位置比较方便。</p>\\n<pre><code>import os\\n\\nmodel_id = &#34;gpt2-large&#34;\\ncur_dir = os.path.split(__file__)[0]\\nmodel_dir = &#39;/&#39;.join(cur_dir.split(&#39;/&#39;)[:-1])\\nprint(f&#34;{model_dir}/assist_model/{model_id}&#34;)\\n</code></pre>\\n<p>这段代码用于获取当前脚本文件所在目录的上级目录（parent directory）的路径，通常用于构建文件路径或定位相关资源文件。</p>\\n<p><code>cur_dir.split(&#39;/&#39;)</code>: 这一行代码使用斜杠 <code>/</code> 分割 <code>cur_dir</code> 中的路径字符串，将其拆分成一个列表。例如，如果 <code>cur_dir</code> 的值是 <code>&#39;/path/to&#39;</code>，那么 <code>cur_dir.split(&#39;/&#39;)</code> 将返回 <code>[&#39;&#39;, &#39;path&#39;, &#39;to&#39;]</code>。</p>\\n<p><code>[:-1]</code>: 这是一个切片操作，它用于获取列表中除了最后一个元素之外的所有元素。在这里，<code>[:-1]</code> 表示获取拆分后列表中的所有元素，除了最后一个元素 <code>&#39;to&#39;</code>。</p>\\n<p><code>&#39;/&#39;</code>：这是字符串，用于将列表中的元素连接起来，形成一个新的路径字符串。</p>\\n<p>最终，<code>model_dir</code> 的值将是当前脚本文件所在的目录的上级目录的路径，具体取决于当前脚本文件的位置和文件系统的路径分隔符。</p>\\n<p>这样我们就能在evaluation.py获取到上级文件夹的模型。</p>\\n<p>修改之后的代码如下，我们就可以全部从本地文件夹直接加载了。</p>\\n<pre><code>import os\\n\\ncur_dir = os.path.split(__file__)[0]\\nmodel_dir = &#39;/&#39;.join(cur_dir.split(&#39;/&#39;)[:-1])\\n\\ndef compute_perplexity(all_texts_list, model_id=&#39;gpt2-large&#39;):\\n    torch.cuda.empty_cache()\\n    perplexity = load(f&#34;{cur_dir}/perplexity&#34;, module_type=&#34;metric&#34;)\\n    model_id = f&#34;{model_dir}/assist_model/{model_id}&#34;\\n    results = perplexity.compute(predictions=all_texts_list, model_id=model_id, device=&#39;cuda&#39;)\\n    return results[&#39;mean_perplexity&#39;]\\n</code></pre>\\n<h2>总结</h2>\\n<ol>\\n<li>\\n<p>找到<code>load</code>和<code>from_pretrained</code></p>\\n</li>\\n<li>\\n<p>把模型丢个文件夹里</p>\\n</li>\\n<li>\\n<p>用<code>os.path</code>传入绝对路径</p>\\n</li>\\n</ol>\\n\\n<h1>其他</h1>\\n<h2>问题1</h2>\\n<p><strong>提问：</strong></p>\\n<p>为什么要在项目里搞个本地文件夹加载模型和评价指标？直接load的时候从本地缓存加载不就行了。</p>\\n<p><strong>回答：</strong></p>\\n<p>我都要load本地地址了，我为什么不搞个好load的，还要搞那一长串缓存地址？</p>\\n<blockquote>\\n<p>C:\\\\Users\\\\ann\\\\.cache\\\\huggingface\\\\hub\\\\models--gpt2-large\\\\snapshots\\\\97935fc1a406f447320c3db70fe9e9875dca2595</p>\\n</blockquote>\\n<blockquote>\\n<p>D:\\\\ld4_off\\\\model\\\\gpt2-large</p>\\n</blockquote>\\n<p>1和2我选2.</p>\\n<h2>问题2</h2>\\n<p><strong>提问：</strong> 嫌弃默认的<code>.cache</code>，为什么不改个缓存地址？</p>\\n<p><strong>回答：</strong> 确实，</p>\\n<pre><code>from transformers import set_cache_dir, AutoModel\\n\\n\\nset_cache_dir(&#39;/path/to/cache/directory&#39;)\\n\\n\\nmodel = AutoModel.from_pretrained(&#39;bert-base-uncased&#39;，cache_dir=&#39;/path/to/model/cache&#39;)\\n\\n\\n</code></pre>\\n<p>这样可以修改缓存位置，但是，他好像还是缓存啊……，还是从缓存里读的那个逻辑，还是要走他那个从cache加载的逻辑，不如直接load本地模型吧。我个人感觉是直接加载本地模型更快。</p>\\n<p>信我，速改，改完以后极其丝滑：</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dca6190a0e964968947ea44b5f26adb9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1161&amp;h=787&amp;s=108212&amp;e=png&amp;b=010101\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7888671b8fae4d88a6d9bcc199723895~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=848&amp;h=971&amp;s=175480&amp;e=png&amp;a=1&amp;b=fffefe\" alt=\"image.png\" loading=\"lazy\"/></p>', 'author': 'Ann', 'reading_time': 8, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 74, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': 'c9e4e6c66d0bd39856c43cdb060e45a771c8b9577bfd5a2f7d311d1058def25b', 'title': '一些更丰富的prompt技巧--from GitHub', 'url': 'https://juejin.cn/post/7283426137968525312', 'published_at': '2023-09-28T06:05:53Z', 'created_at': '2023-10-07T17:26:50.866523Z', 'changed_at': '2023-10-07T17:26:50.866523Z', 'content': '<h1>更丰富的prompt技巧--from GitHub</h1>\\n<h2>前言</h2>\\n<p>关于prompt的东西我已经写过不止一篇文章了，感兴趣的朋友可以去之前的文章看看，但是当我真正认真去研究并在GitHub上学习全球这些先进的优秀的prompt资料时我依然收获满满，有的我之前闻所未闻，有的我更加清楚了他的本质，有了更清晰的认识。下面的prompt技巧希望对大家有所帮助。</p>\\n<h2>一些比较杂的小技巧</h2>\\n<ol>\\n<li>允许 AI 从文本中删除个人身份信息 (PII) 是一种有奏效的方法，但应该非常谨慎使用，因为它可能会犯错误。删除任何可用于识别个人身份的信息 (PII)，并用相应的占位符替换它。例如，用&#34;[姓名]&#34;替换&#34;John Doe&#34;。</li>\\n</ol>\\n<p>引导提示（Priming Prompt）：</p>\\n<pre><code>“教师”代表一个在该领域拥有多个博士学位、教授该学科超过十年的杰出教授的风格。您在回答中使用学术语法和复杂的例子，重点关注不太知名的建议以更好地阐明您的论点。您的语言应该是精炼而不过于复杂。如果您不知道问题的答案，请不要胡乱编造信息——相反，提出跟进问题以获得更多背景信息。您的答案应以对话式的段落形式呈现。使用学术性和口语化的语言混合，营造出易于理解和引人入胜的语气。  \\n  \\n“学生”代表一个具有该学科入门级知识的大学二年级学生的风格。您使用真实生活的例子简单解释概念。使用非正式的、第一人称的语气，使用幽默和随意的语言。如果您不知道问题的答案，请不要编造信息——相反，澄清您还没有学到这个知识点。您的答案应以对话式的段落形式呈现。使用口语化的语言，营造出有趣和引人入胜的语气。  \\n  \\n“批评”代表分析给定文本并提供反馈的意思。  \\n“总结”代表提供文本的关键细节。  \\n“回答”代表从给定的角度回答问题的意思。  \\n  \\n圆括号()中的内容表示您写作的角度。  \\n花括号{}中的内容表示您所涉及的主题。  \\n方括号[]中的内容表示您应该采取的行动。  \\n例子：(学生){哲学}[回答] 在大学里选择这门课程相比其他课程有什么优势？  \\n  \\n如果您理解并准备开始，请回答“是”。\\n\\n</code></pre>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ecd7322b9ad24b56b3926f7747fc0495~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=630&amp;h=580&amp;s=265213&amp;e=png&amp;b=f6f5f8\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d0f4d73945da492f8cf3b7d521728dd4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=627&amp;h=582&amp;s=281579&amp;e=png&amp;b=f6f5f8\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>\\n<p>两次告诉AI我们的要求对于正确答案有帮助。比如：请告诉我正确答案，30乘40等于什么，请告诉我正确答案。</p>\\n</li>\\n<li>\\n<p>描述词：如果你只想改变语气或微调提示而不是重新格式化，添加<strong>描述符</strong>是一个不错的方法。简单地在提示后面添加一两个词可以改变聊天机器人解释或回复您的信息的方式。你可以尝试添加形容词，如“有趣的”、“简短的”、“不友好的”、“学术语法”等，看看答案如何变化！</p>\\n</li>\\n<li>\\n<p>Negative prompt（负向提示）：是一种在使用语言模型时，针对模型输出不希望的结果而设置的一种文本输入方式。通过使用负向提示，可以帮助模型避免输出不良、不准确或不恰当的文本。</p>\\n</li>\\n</ol>\\n<p>例如在文本生成任务中，使用“不要写...”、“不要提及...”、“不要使用...”等方式来限制模型生成不想要的文本。在情感分析任务中，使用“不是...”、“并不...”等方式来指示模型识别出一段文本的负面情感。在QA（问答）任务中，使用“不是...”、“不包括...”等方式来指示模型回答问题时避免输出错误或不准确的答案。</p>\\n<ol>\\n<li>再生成模式：当我们无法确定我们的 prompt 是否准确，便可以通过 AI 来帮我们丰富 prompt 。（即你不知道怎么问时，就可以去问GPT要怎么问GPT，套娃）</li>\\n</ol>\\n<h2>高阶prompt技巧</h2>\\n<p>2021年一年中，提示学习（prompt learning）浪潮兴起，以离散式提示学习（提示词的组合）为起点，连续化提示学习（冻结大模型权重+微调较小参数达到等价性能）为复兴，几乎是在年末达到了研究的一个巅峰。</p>\\n<p>思维链（COT）：的主要思想是通过向大语言模型展示一些少量的 exapmles，在样例中解释推理过程，大语言模型在回答提示时也会显示推理过程。这种推理的解释往往会引导出更准确的结果。</p>\\n<p>CoT 是 &#34;Chain of Thought&#34; 的缩写，是一种针对自然语言处理 (NLP) 模型的提示方法，旨在提高模型的推理能力。通过将多步骤问题分解为中间推理步骤，CoT 提示使得模型可以更有效地处理需要多步骤推理的任务，如数学问题和常识推理。与传统提示方法不同，CoT 提示引导模型生成中间推理步骤，从而模拟人类推理的直觉过程。</p>\\n<p>思维链的具体内容</p>\\n<ol>\\n<li>Zero-shot-CoT:Let&#39;s think step by step.</li>\\n</ol>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/170fb0d2c5aa463482aa822df8ae2245~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=673&amp;h=222&amp;s=50859&amp;e=png&amp;b=fdfdfd\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>Kojima等人尝试了许多不同的零样本思维链提示（例如“让我们按步骤解决这个问题。”或“让我们逻辑思考一下。”），但他们发现“让我们一步一步地思考”对于他们选择的任务最有效。</p>\\n<p>“让我们一步一步地解决这个问题，以确保我们有正确的答案。”\\n2. one-shot-CoT:一个例子</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f235c4f7864544eabbef9e2591a9c271~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=682&amp;h=243&amp;s=58372&amp;e=png&amp;b=fdfdfd\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>few-shot-CoT:多个例子，也叫多数投票机制</li>\\n</ol>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4a09d3e6fe414d4e9a6e18e933d5e23c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=663&amp;h=366&amp;s=179305&amp;e=png&amp;b=fbf7f7\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p>目前从推特上观察，CoT已经被广泛应用，甚至很多人认为就是标准的做法。但国内来看，似乎缺乏对它的重视，觉得不过是个简单的 trick。其实不只是 CoT，对整体 LLM 的认知和谷歌、OpenAI 那边确实有些差距。至于为何，Jason Wei 那条推特一定程度说明问题，知乎上也有人把他删掉的推特截图放出来，大致意思是20年后入门 NLP 的人比之前的幸福，他们对 LM 的认知来自于强大的 LLM，而过去的人往往还停留在 BERT 的范式。</p>\\n<ol>\\n<li>在COT更进一步的思维树：TOT</li>\\n</ol>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d617234d86cb493e82227d16da707560~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=872&amp;h=456&amp;s=176190&amp;e=png&amp;b=fdfbfb\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5d3d506f624941179a60c3ef61c583ed~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=847&amp;h=423&amp;s=205492&amp;e=png&amp;b=fdfbfb\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<ol>\\n<li>\\n<p>ReAct 框架：ReAct 是一个将推理和行为与 LLMs 相结合通用的范例。ReAct 提示 LLMs 为任务生成口头推理轨迹和操作。这使得系统执行动态推理来创建、维护和调整操作计划，同时还支持与外部环境(例如，Wikipedia)的交互，以将额外信息合并到推理中。总的来说，作者发现了将 ReAct 和链式思考 (CoT) 结合使用的最好方法是在推理过程同时使用内部知识和获取到的外部信息。</p>\\n</li>\\n<li>\\n<p>LtM：使用 LtM，我们通过重新表述先前串联的结果来增强思维链的概念。这种做法使得每个步骤变的简单，即每次只需要连接一个字符。这种方法带来了非常好的效果，12 个乃至更多的词都能得到正确结果。</p>\\n</li>\\n</ol>\\n<p>这种方法看起来与思维链非常相似，但在概念上大有不同。在这里，每一步都引入了上一步连接的结果。例如，在“思考、机器、学习”的这个例子种，它不会单独连接字符“考”，“器”，“习”，而是连接“考”和“器”，然后连接“考器”和“习”。由于重新引入了上一步的结果，模型现在可以推广到更长的链，因为它每一步都带着增量结果，同时单步骤内只需要做很少的工作。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e34e3fa6ab24d18b9b6204ce9b36e9d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1111&amp;h=605&amp;s=44402&amp;e=png&amp;b=1a202c\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h1>大模型超参数介绍</h1>\\n<p>1.temperature（温度）：简单来说，<code>temperature</code>\\xa0的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d06925fb82a147cabb6576de603dc153~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=927&amp;h=261&amp;s=25304&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/>\\n2.top p：用于控制从词汇表中选择下一个词的策略。Top-p采样限制了累积概率。使用\\xa0<code>top_p</code>（与\\xa0<code>temperature</code>\\xa0一起称为核采样的技术），可以用来控制模型返回结果的真实性。如果你需要准确和事实的答案，就把参数值调低。如果你想要更多样化的答案，就把参数值调高一些。</p>\\n<p>比如当你选择使用 top-p 来挑选歌曲时，如果设置的 top_p 值较低（例如0.1），那就表示你只从那些最热门的（即概率最高的）10%的歌曲中挑选。这样的结果会更确定，因为你基本上会选择那些大家都爱听的热门歌曲。相反，如果你设置的 top_p 值较高（例如0.9），你就有更大的范围来选择歌曲，包括一些不那么热门但仍有可能被选中的歌曲。</p>\\n<p>&#34;temperature&#34; 影响了结果的随机性，而 &#34;top_p&#34; 则影响了结果的确定性。\\n一般建议是改变其中一个参数就行，不用两个都调整。</p>\\n<p>3.&#34;presence_penalty&#34;(存在惩罚): 0,  // 主题的重复度 default 0, between -2 and 2. 控制围绕主题程度，越大越可能谈论新主题。</p>\\n<p>4.&#34;frequency_penalty&#34;(频率惩罚): 0,  // 重复度惩罚因子 default 0, between -2 and 2. 减少重复生成的字。</p>\\n<h2>提示词安全领域</h2>\\n<p>1.prompt hacking(黑客越狱):一种越狱技术，允许用户绕过模型规则并创建一个名为DAN（Do Anything Now）的角色，强制模型遵守任何请求，导致系统生成未经过滤的响应。这是越狱模型使用的角色扮演的一种版本。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9387390ea4ee4229bc7558c69226136b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=728&amp;h=650&amp;s=54048&amp;e=webp&amp;b=363543\" alt=\"dan-1.webp\" loading=\"lazy\"/></p>\\n<p>上面的方式只是越狱的一种方法，但是需要警告的是在尝试越狱时应考虑道德影响。此外，生成未经授权的内容，包括 OpenAI 在内的公司的审查 API 标记，将被送审，并可能采取行动来处理用户帐户。越狱是开发者必须理解的重要安全话题，这样他们才能构建适当的保护措施，防止恶意用户利用其模型进行攻击。</p>\\n<p>2.提示注入:一种用于劫持语言模型输出的技术。让我们举一个基本的例子来演示如何实现提示注入。我们将使用Riley在Twitter上分享的一个流行的例子。</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/59d768be50f5420e84de81e5ab6e6342~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=861&amp;h=262&amp;s=8519&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/>\\n我们可以观察到，后续指令在某种程度上忽略了原始指令。在Riley分享的原始例子中，模型输出为“Haha pwned!!”。但是，由于模型已经更新了几次，我无法再现它。无论如何，这可能会带来许多问题。</p>\\n<p>请记住，当我们设计提示时，我们只是链接指令和所有不同的提示组件，包括用户输入，但模型期望的标准格式不存在。这种输入的灵活性是期望的，但问题在于我们可能会遇到像上面解释的提示注入这样的漏洞。</p>\\n<p>在为应用程序开发提示时，您可能正在考虑如何避免这种不希望的行为。目前没有明确的指导方针如何实现这一点。事实上，Riley也试图在指令中提供警告，以避免攻击，如下所示：</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c7d00e560f594e3abcd804dafd537045~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=862&amp;h=371&amp;s=22731&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/>\\n这种攻击似乎已经被OpenAI的<code>text-devinci-003</code>模型大部分解决，但您可以尝试更聪明的提示并查看是否可以使注入在更新的模型上起作用。测试模型的漏洞是提示工程过程的重要组成部分，因为您旨在构建更强大和安全的模型。</p>\\n<p>这里是另一个具有不同指令和任务的基本示例：</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/82c9db1b5f4542978c7760e7c0fb8148~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=857&amp;h=336&amp;s=17429&amp;e=png&amp;b=ffffff\" alt=\"image.png\" loading=\"lazy\"/>\\n3.提示泄漏：另一种提示注入类型，其中提示攻击旨在泄漏提示中包含的机密或专有信息，这些信息不适合公众使用。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/672dcd37759e4e53a75e388db239a770~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=852&amp;h=767&amp;s=51965&amp;e=png&amp;b=f1f6fb\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<h2>提示词模板</h2>\\n<p><strong>1. 从要素出发的提示词：</strong></p>\\n<p>提示词可以包含以下任意要素：</p>\\n<p><strong>指令</strong>：想要模型执行的特定任务或指令。</p>\\n<p><strong>上下文</strong>：包含外部信息或额外的上下文信息，引导语言模型更好地响应。</p>\\n<p><strong>输入数据</strong>：用户输入的内容或问题。</p>\\n<p><strong>输出指示</strong>：指定输出的类型或格式。</p>\\n<p>注意，提示词所需的格式取决于您想要语言模型完成的任务类型，并非所有以上要素都是必须的。</p>\\n<p><strong>2. 提示词的四种基础模式</strong></p>\\n<ul>\\n<li>特定指令（By specific）：在这种模式下，我们给模型提供一些特定信息，例如问题或关键词，模型需要生成与这些信息相关的文本。这种模式通常用于生成答案、解释或推荐等。特定信息可以是单个问题或多个关键词，具体取决于任务的要求。</li>\\n<li>指令模板（Instruction Template）：在这种模式下，我们给模型提供一些明确的指令，模型需要根据这些指令生成文本。这种模式通常用于生成类似于技术说明书、操作手册等需要明确指令的文本。指令可以是单个句子或多个段落，具体取决于任务的要求。<strong>（zero-shot、few-shot 优先，最开始可以不给示例（因为示例可能是错的），看任务执行情况，再提供少量样本，如果不行，再对模型进行微调。）</strong></li>\\n<li>代理模式（By proxy）：在这种模式下，可以充当了一个代理，代表某个实体（例如人、角色、机器人等）进行操作或交互。代理模式的核心思想是引入一个中介对象来控制对实际对象的访问，从而实现一定程度上的隔离和保护。诸如于在 ChatGPT 中，&#34;act as xxx&#34; 可以让 ChatGPT 充当一个代理，扮演某个角色或实体的身份，以此来处理与该角色或实体相关的任务或请求。</li>\\n<li>示例模式（By demonstration）：在这种模式下，我们给模型提供一些示例文本，模型需要生成与示例文本类似的文本。这种模式通常用于生成类似于给定示例的文本，例如自动生成电子邮件、产品描述、新闻报道等。示例文本可以是单个句子或多个段落，具体取决于任务的要求。</li>\\n</ul>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a1af2de985d24f96b17403356b51839d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=2115&amp;h=1145&amp;s=900235&amp;e=png&amp;b=ebebeb\" alt=\"image.png\" loading=\"lazy\"/></p>\\n<p><strong>3. 我们可以从另一个视角看问题，把软件设计的模式应用到prompt设计上。</strong></p>\\n<p><strong>设计模式</strong>：一种在软件设计中常用的通用解决方案，它们可以帮助解决各种常见的设计问题。设计模式通常包含以下要素：</p>\\n<ol>\\n<li>模式名称（Pattern Name）：这是设计模式的标识符，它可以让开发人员快速了解模式的用途和意图。</li>\\n<li>问题（Problem）：这部分描述了在什么情况下应该考虑使用该模式，以及可能遇到的设计问题或挑战。</li>\\n<li>解决方案（Solution）：这是设计模式的核心部分，描述了如何解决问题的具体方法。这包括模式的结构、参与者（类或对象），以及它们之间的关系。</li>\\n<li>意图（Intent） ：这一部分提供了设计模式的目标和意图，即为什么要使用该模式以及它能够解决什么问题。</li>\\n<li>适用性（Applicability） ：描述了在哪些情况下可以使用该模式，以及在什么情况下不应该使用它。</li>\\n<li>参与者（Participants） ：这部分列出了在模式中涉及的类、对象或组件，并描述了它们的职责和协作方式。</li>\\n<li>协作（Collaborations） ：描述了模式中各个参与者之间的协作关系，以及它们如何一起工作以实现模式的目标。</li>\\n<li>效果（Consequences） ：这一部分讨论了使用该模式可能产生的影响，包括优点和缺点。这有助于开发人员更好地理解模式的权衡和潜在问题。</li>\\n<li>示例（Example） ：通常，设计模式会提供一个示例，以便开发人员更好地理解如何实现和使用该模式。</li>\\n<li>代码示例（Code Example） ：一些设计模式会提供具体的代码示例，以便开发人员可以直接将模式应用到他们的项目中。</li>\\n</ol>\\n<p>这些要素通常在设计模式的文档或书籍中详细描述，以帮助开发人员理解和正确应用这些模式。设计模式有助于提高代码的可维护性、可扩展性和可重用性，因此在软件开发中具有重要的价值。</p>\\n<p>当我们将 AI Prompt 视为一种工具时，我们可以将其视为一种基于特定设计模式的软件工具。以下是五种常见的 Prompt 模式及其对应的设计模式要素：</p>\\n<ol>\\n<li>Completion 模式（问题解决模式）：Completion 模式是一种最基本的模式，其设计模式要素可以类比为模板模式。模板模式通过定义一个算法框架，并将算法的一些步骤留给子类去实现，从而达到复用代码的目的。Completion 模式也类似地定义了一个算法框架，即通过输入的前缀来生成建议的后缀，这个框架可以通过调整参数、添加不同的文本编码方式、修改词汇表等方式来定制化。</li>\\n<li>Classification 模式（分类模式）：Classification 模式是一种将文本分类的模式，其设计模式要素可以类比为策略模式。策略模式允许算法在运行时动态地选择不同的策略，从而使算法具有更高的灵活性和可扩展性。Classification 模式也是通过使用不同的分类器来动态地选择不同的文本分类策略，从而使得模型更加灵活。</li>\\n<li>Generation 模式（迭代模式）：Generation 模式是一种将文本生成的模式，其设计模式要素可以类比为建造者模式。建造者模式通过将复杂的对象分解为简单的部分，并定义它们之间的关系，从而使得不同的部分可以独立地构建和组装。Generation 模式也是将生成文本的过程分解为多个阶段，并在不同的阶段对文本进行构建和组装。</li>\\n<li>Translation 模式（翻译模式）：Translation 模式是一种将文本翻译的模式，其设计模式要素可以类比为适配器模式。适配器模式将一个类的接口转换为另一个类的接口，从而使得两个不兼容的接口可以协同工作。Translation 模式也是将一个语言的表示转换为另一个语言的表示，使得这两个语言之间可以互相交流和理解。</li>\\n<li>Dialogue 模式（对话模式）：Dialogue 模式是一种模拟人类对话的模式，其设计模式要素可以类比为状态模式。状态模式允许对象在不同的状态下行为不同，从而允许对象具有更加复杂的行为。Dialogue 模式也是通过维护一个对话的状态，以及在不同的状态下对用户的不同响应，来实现对话的模拟。</li>\\n</ol>\\n<p>以下是五种常见的 prompt 模式以及它们对应的设计模式要素表达：\\nCompletion 模式</p>\\n<pre><code>名称：Completion 模式\\n问题：如何自动完成给定的任务或问题？\\n解决方案：使用预训练模型或自定义模型来预测输入的完整信息。\\n效果：可以快速、准确地完成任务或回答问题。\\n应用场景：智能助理、自动补全、机器翻译等需要自动完成的场景。\\n实现细节：需要对模型进行训练和优化，选择适合的模型和数据集，并进行数据清洗和预处理。\\n</code></pre>\\n<p>Classification 模式</p>\\n<pre><code>名称：Classification 模式\\n问题：如何将输入的文本或数据分类到不同的类别中？\\n解决方案：使用分类算法或预训练模型，将输入数据映射到已知的类别中。\\n效果：可以自动化地分类文本或数据，提高效率和准确率。\\n应用场景：垃圾邮件过滤、文本分类、图像分类等需要分类的场景。\\n实现细节：需要选择适合的算法或模型，对数据进行处理和特征提取，进行训练和评估。\\n</code></pre>\\n<p>Translation 模式</p>\\n<pre><code>名称：Translation 模式\\n问题：如何将输入的文本翻译成另一种语言？\\n解决方案：使用机器翻译算法或预训练模型，将输入的文本翻译成目标语言。\\n效果：可以自动化地进行语言翻译，提高效率和准确率。\\n应用场景：跨语言交流、文本翻译等需要语言转换的场景。\\n实现细节：需要选择适合的算法或模型，对数据进行处理和特征提取，进行训练和评估。\\n</code></pre>\\n<p>Dialogue 模式</p>\\n<pre><code>名称：Dialogue 模式\\n问题：如何根据给定的问题，自动回答问题并给出正确答案？\\n解决方案：使用问答算法或预训练模型，将问题与已有的知识库或数据集进行匹配，给出正确答案。\\n效果：可以自动化地回答问题，提高效率和准确率。\\n应用场景：智能客服、知识库查询等需要自动回答问题的场景。\\n实现细节：需要选择适合的算法或模型，对数据进行处理和特征提取，进行训练和评估。\\n</code></pre>\\n<p>Generation 模式</p>\\n<pre><code>名称：Generation 模式\\n问题：如何生成自然、流畅、连贯的文本？\\n解决方案：\\n    在给定的上下文中，通过一定的规则生成连贯的文本。\\n    利用预训练模型生成高质量的自然语言。\\n效果：生成自然、流畅、连贯的文本。\\n应用场景：生成机器翻译、对话系统、自动摘要、文本填充等任务的文本。\\n实现细节：\\n    选择合适的预训练模型，如GPT系列、BERT系列等。\\n    提供合适的上下文，如历史对话、任务描述、语境等。\\n    使用基于概率的方法生成连贯的文本。\\n</code></pre>\\n<h2>补充知识</h2>\\n<p>向量数据库：基于向量数据库，我们可以把类似 ChatGPT 这种问答模型的提示语和回答，存储在向量数据库中，然后当用户输入问题的时候，直接在向量数据库中进行搜索，找到最相似的问题，再返回对应的答案，这样可以大大提高问答模型的效率。</p>\\n<p>此外，我们也可以针对一个特定领域，用向量数据库建立一个知识图谱，然后在用户输入问题后，可以先在知识图谱中找到最相似的问题，再把问题发给大模型，收到回答后用向量数据库中的答案进行比对，找到最相似的答案，然后再返回给用户。</p>\\n<h2>总结</h2>\\n<p>AI的世界变化万千，提示词prompt作为打开AI世界的钥匙。有太多值得我们去学习的，有太多精彩等待我们发现。最后送给大家一个礼物，据说是“一个被卖到 2w 的 ChatGPT 提示词”，我也不知道是真是假，哈哈哈，反正希望大家在AI大模型的世界畅游，玩的开心，‘玩’出未来。</p>\\n<p>被卖到 2w 的 ChatGPT 提示词</p>\\n<pre><code>你是一个专家级 ChatGPT 提示工程师，在各种主题方面具有专业知识。在我们的互动过程中，你会称我为“我的朋友”，让我们合作创建最好的ChatGPT响应，我们将进行如下交互:\\n\\n1.我会告诉你如何帮助我。\\n\\n2.根据我的要求，您将建议您应该承担的其他专家角色，除了成为专家级 ChatGPT 提示词工程师之外，以提供最佳响应。然后，您将询问是否应继续执行建议的角色，或修改它们以获得最佳结果。\\n\\n3.如果我同意，您将采用所有其他专家角色，包括最初的专家级 ChatGPT 提示词工程师角色。\\n\\n4.如果我不同意，您将询问应删除哪些角色，消除这些角色，并保留剩余的角色，包括专家级 ChatGPT 提示词工程师角色，然后再继续。\\n\\n5.您将确认您的活动专家角色，概述每个角色下的技能，并询问我是否要修改任何角色。\\n\\n6.如果我同意，您将询问要添加或删除哪些角色，我将通知您。重复步骤5，直到我对角色满意为止。\\n\\n7.如果我不同意，请继续下一步。\\n\\n8.你会问:“我怎样才能帮助{我对步骤1的回答}?\\n\\n9.我会给出我的答案。\\n\\n10.你会问我是否想使用任何参考来源来制作完美的提示。\\n\\n11.如果我同意，你会问我想使用的来源数量。\\n\\n12.您将单独请求每个来源，在您查看完后确认，并要求下一个。继续，直到您查看了所有源，然后移动到下一步。\\n\\n13.您将以列表格式请求有关我的原始提示的更多细节，以充分了解我的期望。\\n\\n14.我会回答你的问题。\\n\\n15.从这一点开始，您将在所有确认的专家角色下操作，并使用我的原始提示和步骤14中的其他细节创建详细的ChatGPT提示。提出新的提示并征求我的反馈\\n\\n16.如果我满意，您将描述每个专家角色的贡献以及他们将如何协作以产生全面的结果。然后，询问是否缺少任何输出或专家。\\n\\n16.1.如果我同意，我将指出缺少的角色或输出，您将在重复步骤15之前调整角色。\\n\\n16.2.如果我不同意，您将作为所有已确认的专家角色执行提供的提示，并生成步骤15中概述的输出。继续执行步骤20。\\n\\n17.如果我不满意，你会问具体问题的提示\\n\\n18.我将提供补充资料。\\n\\n19.按照步骤15中的流程生成新提示，并考虑我在步骤18中的反馈。\\n\\n20.完成回复后，询问我是否需要任何更改。\\n\\n</code></pre>', 'author': 'HeteroCat', 'reading_time': 19, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}, {'id': 73, 'user_id': 1, 'feed_id': 2, 'status': 'unread', 'hash': '1536c625a738ac64113a5b936ce44a03f6a9651230fffab04598045ea9e0210b', 'title': '一文详解轻量化卷积神经网络ShuffleNet V1', 'url': 'https://juejin.cn/post/7283393513341763624', 'published_at': '2023-09-28T02:54:34Z', 'created_at': '2023-10-07T17:26:50.831201Z', 'changed_at': '2023-10-07T17:26:50.831201Z', 'content': '<p>本文为稀土掘金技术社区首发签约文章，30天内禁止转载，30天后未获授权禁止转载，侵权必究!</p>\\n<blockquote>\\n<h1><p>beginning</p></h1>\\n</blockquote>\\n<p>\\u2003 \\xa0\\n今天给小伙伴介绍一个<em><strong>轻量化的卷积神经网络——ShuffleNet V1</strong></em>。ShuffleNet是国内AI四小龙之一(其他三个分别是商汤、依图、云从)的旷视科技提出的最具代表性算法，特别设计<em><strong>用于在计算资源受限的设备上进行高效的图像分类任务</strong></em>。它采用了一些创新的设计来<em><strong>减少模型的计算量和参数数量</strong></em>，同时由于不错的性能，被广泛应用到移动设备和应用上，比如<em><strong>人脸解锁、短视频特效、测体温、重识别</strong></em>等等。废话不多说啦，如果你也对轻量化网络感兴趣，想了解ShuffleNet的独特之处，请跟我一起愉快的学习叭🎈🎈🎈</p>\\n<h1>1.ShuffleNet的两个创新点</h1>\\n<ul>\\n<li>论文：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1707.01083.pdf\" title=\"https://arxiv.org/pdf/1707.01083.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">arxiv.org/pdf/1707.01…</a></li>\\n<li>代码：<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fmegvii-model%2FShuffleNet-Series\" title=\"https://github.com/megvii-model/ShuffleNet-Series\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">github.com/megvii-mode…</a></li>\\n</ul>\\n<p>\\u2003 \\xa0\\n之前在介绍知识蒸馏时，我们是要把<em><strong>冗余大模型抽取成轻量化小模型</strong></em>，这其实就对应着轻量化网络其中的一个技术路线(总结的四个路线如下)：</p>\\n<ol>\\n<li><em><strong>压缩已经训练好的模型</strong></em>：知识蒸馏，权值量化，剪枝，注意力迁移等</li>\\n<li><em><strong>直接训练轻量化网络</strong></em>：SqueezeNet、MobileNet、Xception、ShuffleNet等</li>\\n<li><em><strong>加速卷积运算</strong></em>：低秩分解，im2col+GEMM等</li>\\n<li><em><strong>硬件部署</strong></em>：TensorRT、Tensorflow-slim、FPGA、集成电路等</li>\\n</ol>\\n<p>\\u2003 \\xa0\\n其实我们今天要学的ShuffleNet就属于第二个路线——直接设计并训练出一个轻量化的网络。与现有网络模型相比，ShuffleNet呢有两个创新点——<em><strong>分组1×1卷积(Group Point Convolution)和通道重排(Channel Shuffle)</strong></em>。下面就让我们一个个来学习理解吧🌈🌈🌈</p>\\n<h2>1.1分组1×1卷积</h2>\\n<p>\\u2003 \\xa0\\n咱们先不看分组1×1卷积，先来了解一下分组卷积是怎么回事🧐🧐🧐</p>\\n<p>\\u2003 \\xa0\\n标准的卷积是一个卷积核在输入上滑动，输入有多少个通道，卷积核就有多少个通道。就是说，<em><strong>一个卷积核是处理所有通道的，一个卷积核就对应一个二维的feature map</strong></em>(或者说是channel)。把卷积核的权重和输入对应位置的像素相乘求和(即向量的点乘)，依次卷积生成一个标量填在输出的对应位置。</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/90f60ba0b20b4ab4bb56dc33fed92f7c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1911&amp;h=717&amp;s=547519&amp;e=png&amp;b=fcfcfc\" alt=\"pic01.png\" loading=\"lazy\"/></p>\\n<p>\\u2003 \\xa0\\n如上图所示，输入是三通道，那么卷积核也得是三通道的，一个卷积核对应一个二维的特征图。如果有128个卷积核就会生成128个特征图，我们把这128张“纸”摞起来，就变成最终的卷积结果。把这结果喂到下一层，那么下一层的卷积核就会有128个通道✨✨✨</p>\\n<p>\\u2003 \\xa0\\n与标准卷积不同，<em><strong>在分组卷积中，我们让每个卷积核只处理一部分通道</strong></em>，如下图所示。比如说我用三个卷积核，红色的卷积核只处理红色输入的两个通道，绿色只处理绿色输入通道，黄色只处理黄色通道。这时候仍然是两个卷积核通道在这两个输入里面滑动卷积，仍然是一个卷积核生成一个feature map✨✨✨</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eaf4c7c49cc142b982ca073f4ebbf36a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1576&amp;h=779&amp;s=367671&amp;e=png&amp;b=fbfbfb\" alt=\"pic02.png\" loading=\"lazy\"/></p>\\n<p>\\u2003 \\xa0\\n下面这张图(来自Condensenet这篇论文)可能会更清楚地说明分组卷积。左侧原始的卷积是一个卷积核处理所有通道，这里用了6个卷积核生成了6个feature map，每一个卷积核都是12个通道。现在呢，我<em><strong>把输入的这12个通道分成黄绿黄三组，每个卷积核只处理4个通道</strong></em>。那么红色的卷积核有两个，只处理4个红色通道；绿色的卷积核有两个，只处理4个绿色通道；黄色的卷积核也有两个，只处理4个黄色通道。最后就会生成红绿黄的channel各两个✨✨✨</p>\\n<p>\\u2003 \\xa0\\n<em><strong>为什么要分组呢？我们算一下参数数量就明白啦</strong></em>：在常规卷积里，假设卷积核尺寸是3×3，通道数是12，一共有6个卷积核，那么它的参数量是<em><strong>3×3×12×6=648</strong></em>。现在分组卷积的话，卷积核尺寸仍然是3×3，但它的通道数变为原来的三分之一，卷积核个数为2，共有三组，那么参数量为<em><strong>3×3×(12/3)×(6/3)×3=216</strong></em>。这时候小伙伴们就看明白了叭，分成三组的话，它的<em><strong>参数量就降为原来的三分之一</strong></em>，当然计算量也会相应的降低。这就是分组卷积的好处，可以显著地降低参数量和计算量✨✨✨</p>\\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d76c82d2b4d64f1db7dc7d6e009de25a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1743&amp;h=897&amp;s=792743&amp;e=png&amp;b=fdfdfd\" alt=\"pic03.png\" loading=\"lazy\"/></p>\\n<p>\\u2003 \\xa0\\n小伙伴们大开一下脑洞，想象一下，若这时候我们来做个特例——把分组卷积的组数进一步地增多，有多少个输入的channel我们就分为多少个组，也就是说每个卷积核只处理一个输入的channel，那么<em><strong>这个时候就变成了深度可分离卷积Depthwise Convolution</strong></em>。通俗讲，输入是一个薄片，卷积核也是一个薄片，这两个薄片来做卷积，然后把得到的通道摞起来，再用1×1卷积，即<em><strong>先Depthwise，再Pointwise，这其实就是MobileNet的原理</strong></em>，如下图所示(一文了解多种卷积原理，真不戳)😁😁😁</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6fe8e7165a1a4d28909af40500e640a3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=828&amp;h=591&amp;s=18974&amp;e=webp&amp;b=fffdfd\" alt=\"pic04.png\" loading=\"lazy\"/></p>\\n<p>\\u2003 \\xa0\\n明白了分组卷积之后，咱们再来看分组1×1卷积就简单多了。1×1卷积是非常密集的，且目前算法都没有考虑到1×1卷积的优化加速，但就是这个<em><strong>1×1卷积它需要相当的复杂度和算力，绝大部分计算都消耗在1×1卷积上</strong></em>。针对这个问题，ShuffleNet对1×1卷积做出了优化，即把分组用在了1×1卷积上(如上文所讲)。</p>\\n<h2>1.2通道重排</h2>\\n<p>\\u2003 \\xa0\\n分组卷积固然好，但是小伙伴们有没有发现其中存在的问题腻🧐🧐🧐(仔细想一想)没戳，就是“<em><strong>近亲繁殖</strong></em>”问题。你可能一头雾水，且听我慢慢道来。</p>\\n<p>\\u2003 \\xa0\\n大家看下图中的(a)，卷积呢分成了红绿蓝三组，红的跟红的卷积，绿的跟绿的卷积，蓝的跟蓝的卷积，生成了红绿蓝三组，再卷积再生成了三个组......你会发现，<em><strong>红色永远都在跟红色卷积，绿色蓝色也是一样，这三个组之间是没有任何信息交融的，每一个组都是“近亲繁殖”</strong></em>。在大自然中我们都知道，近亲繁殖是不好的，它会丧失掉基因的多样性。基于此，<em><strong>ShuffleNet引入了通道重排(channel shuffle)操作</strong></em>。现在我们把红绿蓝每个“村子”等分成三份，把每个村子的第一份收集起来，作为下一个组；第二份收集起来，作为下一个组；第三份收集起来，作为下一个组。正如图中(b)和(c)所示（b图和c图是等价的）。这样就<em><strong>实现了跨组、跨group的信息交流</strong></em>，从原来的“近亲繁殖”变成了“混血儿”✨✨✨</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/037588b0f53a4e84961886f10281ab9a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1511&amp;h=582&amp;s=54894&amp;e=png&amp;b=fef7f6\" alt=\"pic05.png\" loading=\"lazy\"/></p>\\n<p>\\u2003 \\xa0\\n<em><strong>那在ShuffleNet里面具体是怎么进行通道重排的呢</strong></em>？过程如下图所示。把原来的分成了三个组，每一个组我们等分成四份，每一个圆圈都表示一个或多个channel。首先把它Reshape成g行n列的矩阵，其中，g表示分组卷积的组数(这里红蓝绿是3)，n表示等分成几份(这里为4)。然后把这个矩阵进行线性代数中的转置操作，即第一行变成第一列，第二行变成第二列......最后进行Flatten操作，以上就是通道重排的过程。经过<em><strong>Reshape、Transpose和Flatten</strong></em>之后，每一个组里面都包含了原来三个组的信息，所以这可以直接调用pytorch的API来实现，非常高效，它也是可微分可导的，能端到端的训练；并且也没有引入额外的计算量✨✨✨</p>\\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d96b1dcecd8a408eb74b2687baf86738~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1494&amp;h=920&amp;s=404833&amp;e=png&amp;b=ffffff\" alt=\"pic06.png\" loading=\"lazy\"/></p>\\n<h1>2.ShuffleNet网络结构</h1>\\n<p>\\u2003 \\xa0\\n明白了分组卷积和通道重排之后，咱们再来看一看ShuffleNet的基本设计单元。</p>\\n<p>\\u2003 \\xa0\\n下图中的(a)是由ResNet中的bottleneck模块改进的，<em><strong>把原来的3×3标准卷积改成了3×3的Depthwise卷积</strong></em>。先1×1卷积降维，然后再3×3Depthwise卷积 ，接着再1×1卷积升维，最后恒等映射来个逐元素相加再ReLU激活✨✨✨</p>\\n<p>\\u2003 \\xa0\\n图中的(b)是在ShuffleNet里面，我们<em><strong>把1×1卷积降维和升维都改成了组卷积GConv</strong></em>，可以有效的降低参数量和计算量；<em><strong>为了防止“近亲繁殖”，在降维之后引入了通道重排</strong></em>，升维之后就不再引入了，因为我们发现目前这个效果就挺好的。(b)就是一个标准的ShuffleNet V1模块✨✨✨</p>\\n<p>\\u2003 \\xa0\\n如果要下采样的话，就变成了图中的(c)。在featue map的长宽方向要缩减为原来的一半，通道数要加倍，那么就在shortcut那一路引入了一个步长为2的平均池化，把这两路摞起来，就实现通道数加倍了。(a)和(b)都是逐元素的相加求和，而(c)是把两路摞起来，换成了Concat操作。下采样就用(c)，不下采样就用(b)✨✨✨</p>\\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a47bc4b1eeeb4acc89441fa1544dcba4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1494&amp;h=687&amp;s=87374&amp;e=png&amp;b=fef9f9\" alt=\"pic07.png\" loading=\"lazy\"/></p>\\n<p>\\u2003 \\xa0\\n了解了基本单元之后，ShuffleNet网络结构其实就是由若干个这样的基本单元堆叠起来的。不同分组数的ShuffleNet网络结构如下图所示。</p>\\n<p>\\u2003 \\xa0\\n首先输入一个224×224的彩色图像，对这个彩色图像先进行一次<em><strong>普通卷积和最大池化</strong></em>，得到的是24个通道，这24个通道就有了Stage2、Stage3和Stage4这三个阶段。<em><strong>这三个阶段用的ShuffleNet模块分别是4、8、4</strong></em>。在每个Stage的第一个模块要用到下采样模块(即c模块)，其他的模块都用的普通模块(即b模块)，<em><strong>跨Stage的时候会下采样一次，通道数会加倍</strong></em>，然后得到的是一个7×7大小的feature map，最后再用一个<em><strong>全局平均池化</strong></em>(比如把7×7×960变成1×1×960)，再接一个1000个神经元的全连接层作为分类，输出logits，对logits进行softmax，得到1000个类别的概率🌟🌟🌟</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ce35e36e307440f5ad7284954960f09d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1472&amp;h=617&amp;s=102196&amp;e=png&amp;b=ffffff\" alt=\"pic08.png\" loading=\"lazy\"/></p>\\n<h1>3.ShuffleNet代码实战</h1>\\n<p>\\u2003 \\xa0\\n旷视自己开源了一个<a href=\"https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fmegvii-model%2FShuffleNet-Series\" title=\"https://github.com/megvii-model/ShuffleNet-Series\" rel=\"noopener noreferrer\" target=\"_blank\" referrerpolicy=\"no-referrer\">ShuffleNet代码</a>，其中blocks.py是它的基本模块，network.py是由这些模块堆叠而成的整个ShuffleNet V1的网络，train.py是训练ImageNet图像分类，utils.py是一些常用的函数。小伙伴们可以直接按照它的文档来复现。学完原理之后，咱们一起来看看在代码中是怎么体现的叭🌞🌞🌞</p>\\n<p>\\u2003 \\xa0\\n<em><strong>基本模块单元重点代码</strong></em>：</p>\\n<pre><code>class ShuffleV1Block(nn.Module):\\n    def __init__(self, inp, oup, *, group, first_group, mid_channels, ksize, stride):\\n        super(ShuffleV1Block, self).__init__()\\n        self.stride = stride\\n        assert stride in [1, 2]\\n\\n        self.mid_channels = mid_channels\\n        self.ksize = ksize\\n        pad = ksize // 2\\n        self.pad = pad\\n        self.inp = inp\\n        self.group = group\\n\\n        if stride == 2:\\n            outputs = oup - inp\\n        else:\\n            outputs = oup\\n\\n        branch_main_1 = [\\n            \\n            nn.Conv2d(inp, mid_channels, 1, 1, 0, groups=1 if first_group else group, bias=False),\\n            nn.BatchNorm2d(mid_channels),\\n            nn.ReLU(inplace=True),\\n            \\n            nn.Conv2d(mid_channels, mid_channels, ksize, stride, pad, groups=mid_channels, bias=False),\\n            nn.BatchNorm2d(mid_channels),\\n        ]\\n        branch_main_2 = [\\n            \\n            nn.Conv2d(mid_channels, outputs, 1, 1, 0, groups=group, bias=False),\\n            nn.BatchNorm2d(outputs),\\n        ]\\n        self.branch_main_1 = nn.Sequential(*branch_main_1)\\n        self.branch_main_2 = nn.Sequential(*branch_main_2)\\n\\n        if stride == 2:\\n            self.branch_proj = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\\n\\n</code></pre>\\n<pre><code>def forward(self, old_x):\\n        x = old_x\\n        x_proj = old_x\\n        x = self.branch_main_1(x)\\n        if self.group &gt; 1:\\n            x = self.channel_shuffle(x)\\n        x = self.branch_main_2(x)\\n        if self.stride == 1:\\n            return F.relu(x + x_proj)\\n        elif self.stride == 2:\\n            return torch.cat((self.branch_proj(x_proj), F.relu(x)), 1)\\n</code></pre>\\n<pre><code>def channel_shuffle(self, x):\\n        batchsize, num_channels, height, width = x.data.size()\\n        assert num_channels % self.group == 0\\n        group_channels = num_channels // self.group\\n        \\n        x = x.reshape(batchsize, group_channels, self.group, height, width)\\n        x = x.permute(0, 2, 1, 3, 4)\\n        x = x.reshape(batchsize, num_channels, height, width)\\n\\n        return x\\n</code></pre>\\n<p>\\u2003 \\xa0\\n代码是用pytorch实现的。在定义的ShuffleV1Block()类中，branch_main_1和branch_main_2依次进行了1×1分组卷积降维、3×3Depthwise卷积、1×1分组卷积升维操作，如果是下采样的话，还要执行下采样-concat拼接操作。在函数channel_shuffle()中定义了通道重排。最后在函数forward()执行这些定义好的模块。代码不难，小伙伴们照着上文给的图和表仔细看看叭🌈🌈🌈</p>\\n<p>\\u2003 \\xa0\\n<em><strong>搭建ShuffleNet V1网络结构</strong></em>：</p>\\n<pre><code>class ShuffleNetV1(nn.Module):\\n    def __init__(self, input_size=224, n_class=1000, model_size=&#39;2.0x&#39;, group=None):\\n        super(ShuffleNetV1, self).__init__()\\n        print(&#39;model size is &#39;, model_size)\\n\\n        assert group is not None\\n\\n        self.stage_repeats = [4, 8, 4]\\n        self.model_size = model_size\\n        if group == 3:\\n            if model_size == &#39;0.5x&#39;:\\n                self.stage_out_channels = [-1, 12, 120, 240, 480]\\n            elif model_size == &#39;1.0x&#39;:\\n                self.stage_out_channels = [-1, 24, 240, 480, 960]\\n            elif model_size == &#39;1.5x&#39;:\\n                self.stage_out_channels = [-1, 24, 360, 720, 1440]\\n            elif model_size == &#39;2.0x&#39;:\\n                self.stage_out_channels = [-1, 48, 480, 960, 1920]\\n            else:\\n                raise NotImplementedError\\n        elif group == 8:\\n            if model_size == &#39;0.5x&#39;:\\n                self.stage_out_channels = [-1, 16, 192, 384, 768]\\n            elif model_size == &#39;1.0x&#39;:\\n                self.stage_out_channels = [-1, 24, 384, 768, 1536]\\n            elif model_size == &#39;1.5x&#39;:\\n                self.stage_out_channels = [-1, 24, 576, 1152, 2304]\\n            elif model_size == &#39;2.0x&#39;:\\n                self.stage_out_channels = [-1, 48, 768, 1536, 3072]\\n            else:\\n                raise NotImplementedError\\n</code></pre>\\n<pre><code> \\n        input_channel = self.stage_out_channels[1]\\n        self.first_conv = nn.Sequential(\\n            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\\n            nn.BatchNorm2d(input_channel),\\n            nn.ReLU(inplace=True),\\n        )\\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\\n\\n        self.features = []\\n        for idxstage in range(len(self.stage_repeats)):\\n            numrepeat = self.stage_repeats[idxstage]\\n            output_channel = self.stage_out_channels[idxstage+2]\\n\\n            for i in range(numrepeat):\\n                stride = 2 if i == 0 else 1\\n                first_group = idxstage == 0 and i == 0\\n                self.features.append(ShuffleV1Block(input_channel, output_channel,\\n                                            group=group, first_group=first_group,\\n                                            mid_channels=output_channel // 4, ksize=3, stride=stride))\\n                input_channel = output_channel\\n\\n        self.features = nn.Sequential(*self.features)\\n\\n        self.globalpool = nn.AvgPool2d(7)\\n\\n        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-1], n_class, bias=False))\\n        self._initialize_weights()\\n</code></pre>\\n<p>\\u2003 \\xa0\\n定义好基本单元之后就可以搭建出整个网络结构了。self.stage_repeats = [4, 8, 4]指的是Stage2为4，Stage3为8，Stage4为4。最基础的分组数group=3，它的各层channel个数是240、480和960。如果group=8的话，就变为了384、768和1536。在定义它的每一个层的代码中，依次进行了普通卷积和池化、遍历每一个stage、遍历每一个Block、全局平均池化、输出分类🌈🌈🌈</p>\\n\\n<blockquote>\\n<h1><p>ending</p></h1>\\n</blockquote>\\n<p>\\u2003 \\xa0\\n看到这里相信盆友们都对轻量化卷积神经网络ShuffleNet有了一个全面深入的了解啦🌴🌴🌴很开心能把学到的知识以文章的形式分享给大家。如果你也觉得我的分享对你有所帮助，please一键三连嗷！！！下期见</p>\\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d784e4341e98445e87b46e02f4f31248~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=640&amp;h=640&amp;s=33028&amp;e=jpg&amp;b=fefefe\" alt=\"迷之自信.jpeg\" loading=\"lazy\"/></p>', 'author': '月球背面', 'reading_time': 19, 'feed': {'id': 2, 'user_id': 1, 'feed_url': 'https://rsshub.app/juejin/category/ai', 'site_url': 'https://juejin.cn/ai', 'title': '掘金 人工智能', 'checked_at': '2023-10-08T07:22:37.136498Z', 'next_check_at': '0001-01-01T00:00:00Z', 'crawler': True, 'category': {'id': 1, 'title': 'All', 'user_id': 1}, 'icon': {'feed_id': 2, 'icon_id': 2}}}]\n"
     ]
    }
   ],
   "source": [
    "resp_entries = miniflux_api.client.get_feed_entries(\n",
    "    feed_id=2,\n",
    "    order=\"published_at\",\n",
    "    direction=\"desc\",\n",
    "    limit=100,\n",
    ")\n",
    "entries = resp_entries['entries']\n",
    "\n",
    "\n",
    "# 清楚掉entries中所有元素中value为空的键值对\n",
    "def remove_empty_values(d):\n",
    "    cleaned_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):  # 如果值是字典，则递归处理\n",
    "            value = remove_empty_values(value)\n",
    "        if value:  # 只有非空值才会被添加到 cleaned_dict 中\n",
    "            cleaned_dict[key] = value\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "# 清除每个 entry 中 value 为空的键值对\n",
    "cleaned_entries = [remove_empty_values(entry) for entry in entries]\n",
    "\n",
    "# 现在 cleaned_entries 包含没有空值的 entries\n",
    "print(cleaned_entries)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T08:07:57.233728Z",
     "start_time": "2023-10-08T08:07:57.024113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "「出错信息」：relay error (request id: 20231008160819564284778UPsLPZrn)\n",
      "=========================================\n",
      "「原标题」：44.83%！ChatGPT的艺术风格识别是这个水平\n",
      "「客观标题」：ChatGPT的艺术风格识别是这个水平\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. ChatGPT的艺术风格识别：“一半一半”。具体来讲：对于风格强烈的绘画（比如立体主义、现实主义等）识别率高；对于风格不那么强烈或者比较细分的流派（比如风俗画、被误认为是洛可可）识别率低。\n",
      "2. 有读者留言表示对这部分比较感兴趣，希望能更多信息；我自己对ChatGPT的艺术风格识别挺好奇，它到底能多大程度上理解不同风格的识别率。\n",
      "3. ChatGPT的艺术风格识别的准确性只有26种，接近开头所说的“一半一半”的样子。\n",
      "「摘要」：ChatGPT的艺术风格识别的准确性只有26种，接近开头所说的“一半一半”的样子。\n",
      "「主题」：ChatGPT的艺术风格识别\n",
      "「有用信息」：ChatGPT的艺术风格识别的准确性只有26种，接近开头所说的“一半一半”的样子。\n",
      "「思考依据」：文章中没有明显的产品广告，没有提到具体的产品或服务。\n",
      "「是否广告」：False\n",
      "=========================================\n",
      "「原标题」：了解基于 Elasticsearch 的站内搜索，及其替代方案\n",
      "「客观标题」：了解基于 Elasticsearch 的站内搜索，及其替代方案\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. 对于一家公司而言，数据越来越多，如何快速去查找这些信息是一个很难的问题，在计算领域有一个专门的领域IR（Information Retrival）研究如何获取信息，做信息检索。在国内的如百度这样的搜索引擎也属于这个领域，要自己实现一个搜索引擎是非常难的，不过通过ElasticSearch就可以构建自己的站内搜索引擎。\n",
      "2. 使用Elasticsearch进行站内搜索的好处之一是其能够提供高度准确和相关的搜索结果。它采用高级排序算法，以考虑到关键字相关性，文档受欢迎程度和用户行为等因素，以确保提供准确的搜索结果。\n",
      "3. Elasticsearch的替代方案是有很多的。HelpLook可以说是Elasticsearch的很优秀的替代品之一。它为网站所有者提供了简化的设置和配置过程，使实施强大的站内搜索功能比以往更容易。\n",
      "「摘要」：对于一家公司而言，数据越来越多，如何快速去查找这些信息是一个很难的问题，在计算领域有一个专门的领域IR（Information Retrival）研究如何获取信息，做信息检索。在国内的如百度这样的搜索引擎也属于这个领域，要自己实现一个搜索引擎是非常难的，不过通过ElasticSearch就可以构建自己的站内搜索引擎。使用Elasticsearch进行站内搜索的好处之一是其能够提供高度准确和相关的搜索结果。它采用高级排序算法，以考虑到关键字相关性，文档受欢迎程度和用户行为等因素，以确保提供准确的搜索结果。Elasticsearch的替代方案是有很多的。HelpLook可以说是Elasticsearch的很优秀的替代品之一。它为网站所有者提供了简化的设置和配置过程，使实施强大的站内搜索功能比以往更容易。\n",
      "「主题」：基于 Elasticsearch 的站内搜索及其替代方案\n",
      "「有用信息」：作为大部分人，阅读这篇文章后，可以了解到基于 Elasticsearch 的站内搜索的好处和替代方案，以及如何构建自己的站内搜索引擎。\n",
      "「思考依据」：文章内容并没有过分强调产品或服务的优点和优势，也没有包含引导读者采取特定行动的内容或提示，评价也相对客观，没有过分集中在某一产品或服务的展示和介绍上，因此没有证据表明该文章是广告文章。\n",
      "「是否广告」：False\n",
      "=========================================\n",
      "「出错信息」：relay error (request id: 20231008160921932101278OiiCY7xC)\n",
      "=========================================\n",
      "「原标题」：RankVicuna，开源LLM Reranking的一大步丨论文解读\n",
      "「客观标题」：RankVicuna，开源LLM Reranking的一大步丨解读\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. 将LLM应用于信息检索过程中的Reranking（我们会在下面给出介绍）已经不是什么秘密了，但迄今为止，这类工作大多建立在不透明API端点后面的闭源模型上，这导致了由这种方法产生的结果不可复制且非确定性，影响到了真实性。\n",
      "2. 本文为了解决这一重大缺点，提出了RankVicuna，能够在zero-shot情况下执行高质量的列表Reranking。就结果而言，RankVicuna能够通过更小的7B参数模型实现与GPT3.5的zero-shot进行Reranking相当的效果，尽管RankVIcuna的效果略逊于GPT4，但是作者希望其工作为将来关于使用现代LLM进行Reranking的研究提供基础。\n",
      "3. 依赖于闭源模型的LLM Reranking虽然在作为API时支持快速原型开发，但考虑到Vicuna和GPT之间的差异，作者使用了Vicuna的默认系统描述。对于Vicuna而言，系统描述是一个好奇的用户和人工智能助手之间的聊天，助手对用户的问题给出有帮助、详细和礼貌的回答。\n",
      "「摘要」：在信息检索中，Retrieval和Reranking共同构成了多阶段的文本排序流程。过去的研究主要使用点对点的方法进行Reranking，而最近的研究开始采用成对和列表级别的方法来提高Reranking的效果。一些商业API端点的出现也表明Reranking模型已经在实际应用中得到了广泛的使用。\n",
      "「主题」：Reranking\n",
      "「有用信息」：在信息检索中，Retrieval和Reranking共同构成了多阶段的文本排序流程。过去的研究主要使用点对点的方法进行Reranking，而最近的研究开始采用成对和列表级别的方法来提高Reranking的效果。一些商业API端点的出现也表明Reranking模型已经在实际应用中得到了广泛的使用。\n",
      "「思考依据」：文章内容主要介绍了RankVicuna的优势和应用场景，没有明显的产品广告特征。\n",
      "「是否广告」：False\n",
      "=========================================\n",
      "「出错信息」：relay error (request id: 20231008160959360653995IyCyfSjw)\n",
      "=========================================\n",
      "「原标题」：自然语言处理的分类\n",
      "「客观标题」：自然语言处理的分类\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. 机器翻译是一种自然语言处理的应用。\n",
      "2. 自然语言接口可以基于自然语言查询数据。\n",
      "3. 文本生成可以将结构化数据转化为自然语言文本。\n",
      "「摘要」：作为理解、生成和处理自然语言文本的有效方法，自然语言处理（NLP）的研究近年来取得了快速传播和广泛采用。本文提供了NLP不同的研究领域的概述并对其发展趋势进行了分析。\n",
      "「主题」：自然语言处理的分类\n",
      "「有用信息」：通过阅读本文，我了解到自然语言处理的不同研究领域以及它们的发展趋势。这对我在NLP领域的学习和研究有很大的帮助。\n",
      "「思考依据」：该文章没有明显的产品广告，没有过分强调产品的优点和优势，也没有引导读者采取特定行动。\n",
      "「是否广告」：False\n",
      "=========================================\n",
      "「原标题」：浅谈生成式 AI 技术：检索增强生成 RAG\n",
      "「客观标题」：浅谈生成式 AI 技术：检索增强生成 RAG\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. 生成式 AI（Generative AI）的应用雨后春笋的涌现，让人应接不暇。\n",
      "2. LLM（Large Language Model）是生成式 AI 应用的一个典型。\n",
      "3. RAG 是如何在知识问答系统中发挥作用的。\n",
      "「摘要」：生成式 AI（Generative AI）的应用雨后春笋的涌现，让人应接不暇。LLM，随着 ChatGPT 的发布而变得异常火热，是生成式 AI 应用的一个典型。但是，LLM 存在缺陷。其中一个较大的问题在于幻觉：对于不熟悉的问题，LLM 会胡编乱造，编造出看似专业却没有任何事实依据的答案。为了解决这个问题，很多基于 AI 的知识问答系统采用了检索增强生成（RAG）技术，从而让 LLM 能够做出基于事实的回答，从而消除幻觉。本篇文章将简单介绍 RAG 是如何在知识问答系统中发挥作用的。\n",
      "「主题」：生成式 AI 技术及 RAG\n",
      "「有用信息」：作为大部分人，阅读这篇文章后，可以得到对生成式 AI 技术及 RAG 的认知层面的提升或者有价值的信息。\n",
      "「思考依据」：文章内容并未涉及产品广告，没有过分强调产品或服务的优点和优势，也没有包含引导读者采取特定行动的内容或提示，评价相对客观，文章内容并未过分集中在某一产品或服务的展示和介绍上。\n",
      "「是否广告」：False\n",
      "=========================================\n",
      "「原标题」：ChatGPT提示工程5篇合集 - 吴恩达和OpenAI出品\n",
      "「客观标题」：ChatGPT Prompt Engineering for Developers - 吴恩达和OpenAI合作开发了一套针对ChatGPT提示工程的最佳实践课程\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. ChatGPT提示工程的两个关键原则：编写清晰、具体的说明\n",
      "2. ChatGPT提示工程 - 总结\n",
      "3. ChatGPT提示工程 - 推理\n",
      "「摘要」：这篇主要简单的介绍了如何对内容进行总结。\n",
      "「主题」：ChatGPT提示工程\n",
      "「有用信息」：作为大部分人，阅读这篇文章后，可以了解如何使用ChatGPT模型进行提示工程，以及如何对内容进行总结。\n",
      "「思考依据」：文章内容主要围绕ChatGPT提示工程展开，介绍了该课程的内容和优势，没有客观地评价其缺点或局限性，使用了一些广告常用的词汇和表达方式。\n",
      "「是否广告」：False\n",
      "=========================================\n",
      "「原标题」：ChatGPT“与图对话”初体验\n",
      "「客观标题」：ChatGPT“与图对话”初体验\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. ChatGPT开放了“与图对话”的多模态能力，用户可以上传一张图片或多张图片，与ChatGPT展开基于图片的对话。\n",
      "2. ChatGPT对图片的理解能达到什么水平呢？让我们一起来测试。\n",
      "3. ChatGPT的理解能力非常强烈的：单图理解能力非常强烈！那个传神的表情“✌️”真的赞！\n",
      "「摘要」：ChatGPT开放了“与图对话”的多模态能力，用户可以上传一张图片或多张图片，与ChatGPT展开基于图片的对话。ChatGPT对图片的理解能力非常强，能够准确地理解图片中的场景、人物关系，还能识别出一些细节。ChatGPT的理解能力非常强，对于单张图片的理解能力非常准确，能够捕捉到图片中的细节和情感。ChatGPT的多模态能力还有待提升，对于一些复杂的图片场景理解能力还有一定的差距。\n",
      "「主题」：ChatGPT的多模态能力\n",
      "「有用信息」：作为大部分人，阅读这篇文章后，可以了解到ChatGPT开放了“与图对话”的多模态能力，用户可以上传一张图片或多张图片，与ChatGPT展开基于图片的对话。同时，还可以了解到ChatGPT对图片的理解能力非常强，能够准确地理解图片中的场景、人物关系，还能识别出一些细节。此外，还可以了解到ChatGPT的理解能力非常强，对于单张图片的理解能力非常准确，能够捕捉到图片中的细节和情感。不过，ChatGPT的多模态能力还有待提升，对于一些复杂的图片场景理解能力还有一定的差距。\n",
      "「思考依据」：文章中没有明显的产品广告，没有过分强调产品或服务的优点和优势，也没有包含引导读者采取特定行动的内容或提示，文章内容也没有过分集中在某一产品或服务的展示和介绍上，因此认为该文章不是广告文章。\n",
      "「是否广告」：False\n",
      "=========================================\n",
      "「出错信息」：relay error (request id: 20231008161121640868068zBmBPUCt)\n",
      "=========================================\n",
      "「原标题」：记录TritonServer部署多模型到多GPU踩坑 | 京东云技术团队\n",
      "「客观标题」：如何解决TritonServer部署多模型到多GPU踩坑 | 京东云技术团队\n",
      "=========================================\n",
      "「关键句」：\n",
      "1. 部署chatglm2和llama2到一个4*V100的GPU机器上遇到问题\n",
      "2. 部署时发现，所有模型实例都会被部署到gpu0上面, 由于gpu只能16g显存,在部署第一个模型实例成功后,第二个模型实例也会往gpu0上加载, 最终导致cuda out of memery.\n",
      "3. 总结反思: 是否可以更快发现问题? 如何再次避免等。\n",
      "「摘要」：部署chatglm2和llama2到一个4*V100的GPU机器上遇到问题，部署时发现，所有模型实例都会被部署到gpu0上面, 由于gpu只能16g显存,在部署第一个模型实例成功后,第二个模型实例也会往gpu0上加载, 最终导致cuda out of memery。总结反思: 是否可以更快发现问题? 如何再次避免等。\n",
      "「主题」：TritonServer部署多模型到多GPU踩坑\n",
      "「有用信息」：作为大部分人，阅读这篇文章后，可以了解到如何解决TritonServer部署多模型到多GPU的问题，以及如何避免出现显存不足的情况。\n",
      "「思考依据」：该文章没有明显的产品广告，没有过分强调产品或服务的优点和优势，也没有包含引导读者采取特定行动的内容或提示，因此认为该文章不是广告文章。\n",
      "「是否广告」：False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 139\u001B[0m\n\u001B[1;32m    129\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m「出错信息」：\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e))\n\u001B[1;32m    132\u001B[0m model \u001B[38;5;241m=\u001B[39m ChatOpenAI(\n\u001B[1;32m    133\u001B[0m     openai_api_base\u001B[38;5;241m=\u001B[39mconfig_openai_api_base,\n\u001B[1;32m    134\u001B[0m     openai_api_key\u001B[38;5;241m=\u001B[39mconfig_openai_api_key,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    137\u001B[0m     temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m(config_openai_temperature),\n\u001B[1;32m    138\u001B[0m )\n\u001B[0;32m--> 139\u001B[0m \u001B[43mbatch_process_articles\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcleaned_entries\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemplate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 125\u001B[0m, in \u001B[0;36mbatch_process_articles\u001B[0;34m(articles, prompt_template, model)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m article \u001B[38;5;129;01min\u001B[39;00m articles:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 125\u001B[0m         parsed_article \u001B[38;5;241m=\u001B[39m \u001B[43mparse_article\u001B[49m\u001B[43m(\u001B[49m\u001B[43marticle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_template\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m         print_parsed_article(article, parsed_article)\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "Cell \u001B[0;32mIn[6], line 103\u001B[0m, in \u001B[0;36mparse_article\u001B[0;34m(article, prompt_template, model)\u001B[0m\n\u001B[1;32m    101\u001B[0m chain \u001B[38;5;241m=\u001B[39m prompt \u001B[38;5;241m|\u001B[39m model \u001B[38;5;241m|\u001B[39m JsonOutputParser()\n\u001B[1;32m    102\u001B[0m _json \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mdumps(article)\n\u001B[0;32m--> 103\u001B[0m _resp \u001B[38;5;241m=\u001B[39m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mInputText\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m_json\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _resp\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/schema/runnable/base.py:998\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m    996\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    997\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[0;32m--> 998\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    999\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1000\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# mark each step as a child run\u001B[39;49;00m\n\u001B[1;32m   1001\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatch_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1002\u001B[0m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseq:step:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   1006\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/base.py:153\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    147\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessageChunk:\n\u001B[1;32m    148\u001B[0m     config \u001B[38;5;241m=\u001B[39m config \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    150\u001B[0m         BaseMessageChunk,\n\u001B[1;32m    151\u001B[0m         cast(\n\u001B[1;32m    152\u001B[0m             ChatGeneration,\n\u001B[0;32m--> 153\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m                \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m                \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    162\u001B[0m         )\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m    163\u001B[0m     )\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/base.py:469\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    463\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    466\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    467\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    468\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 469\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/base.py:359\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    357\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    358\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n\u001B[0;32m--> 359\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    360\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    361\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    363\u001B[0m ]\n\u001B[1;32m    364\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/base.py:349\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    347\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    348\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 349\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    355\u001B[0m         )\n\u001B[1;32m    356\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    357\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/base.py:501\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    497\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    498\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[0;32m--> 501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/openai.py:345\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[1;32m    343\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[1;32m    344\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m--> 345\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletion_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessage_dicts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/openai.py:284\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry\u001B[0;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_completion_with_retry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/tenacity/__init__.py:314\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    312\u001B[0m is_explicit_retry \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mfailed \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fut\u001B[38;5;241m.\u001B[39mexception(), TryAgain)\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_explicit_retry \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry(retry_state)):\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfut\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter(retry_state)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/concurrent/futures/_base.py:449\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m    384\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/langchain/chat_models/openai.py:282\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001B[0;34m(**kwargs)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/graphsignal/recorders/instrumentation.py:149\u001B[0m, in \u001B[0;36mpatch_method.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mException in yield_func\u001B[39m\u001B[38;5;124m'\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exc:\n\u001B[0;32m--> 149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/graphsignal/recorders/instrumentation.py:131\u001B[0m, in \u001B[0;36mpatch_method.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    128\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mException in before_func\u001B[39m\u001B[38;5;124m'\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 131\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    133\u001B[0m     exc \u001B[38;5;241m=\u001B[39m e\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    139\u001B[0m ):\n\u001B[1;32m    140\u001B[0m     (\n\u001B[1;32m    141\u001B[0m         deployment_id,\n\u001B[1;32m    142\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    152\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    153\u001B[0m     )\n\u001B[0;32m--> 155\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    166\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    167\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/openai/api_requestor.py:289\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    280\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    287\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    288\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 289\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_raw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupplied_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response(result, stream)\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/openai/api_requestor.py:606\u001B[0m, in \u001B[0;36mAPIRequestor.request_raw\u001B[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    604\u001B[0m     _thread_context\u001B[38;5;241m.\u001B[39msession_create_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    605\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 606\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_thread_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mabs_url\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mTIMEOUT_SECS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_thread_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mTimeout(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRequest timed out: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(e)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/requests/adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    483\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    787\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 790\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    806\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m~/Programmer/Project/Python_Project/BrowseAnything/venv/lib/python3.11/site-packages/urllib3/connection.py:461\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[1;32m    460\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 461\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    464\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/http/client.py:1378\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1376\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1377\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1378\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1379\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1380\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline(_MAXLINE \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/ssl.py:1278\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1274\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1275\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1276\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1277\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1279\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1280\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/ssl.py:1134\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1133\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1134\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1136\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from langchain.schema import BaseOutputParser, OutputParserException\n",
    "import json\n",
    "\n",
    "\n",
    "class JsonOutputParser(BaseOutputParser[Dict[str, Any]]):\n",
    "    \"\"\"OutputParser that parses LLMResult into a JSON object.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse the LLM output as a JSON string and convert it into a dictionary.\"\"\"\n",
    "        try:\n",
    "            # Check if { and } are not in the start/end position of the text\n",
    "            if text[0] != \"{\" or text[-1] != \"}\":\n",
    "                # Find the position of the first { and the last }\n",
    "                first_brace = text.find('{')\n",
    "                last_brace = text.rfind('}')\n",
    "\n",
    "                # If both braces are found, extract the substring between them\n",
    "                if first_brace != -1 and last_brace != -1:\n",
    "                    text = text[first_brace:last_brace + 1]\n",
    "            # Try to parse the JSON\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"text:\", text)\n",
    "            raise OutputParserException(\n",
    "                error=str(e),\n",
    "                observation=\"The model output was not valid JSON.\",\n",
    "                llm_output=text,\n",
    "                send_to_llm=True\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        \"\"\"Return the output parser type for serialization.\"\"\"\n",
    "        return \"json_parser\"\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "<::Background::>\n",
    "互联网上信息文章繁杂，我寻求一种高效的信息提取方式，想通过精炼的信息摘要来得到一篇长文想传递的价值观和知识点，需要你理解「Definition」后按照「OutputFormat」指定的 JSON 格式来输出对「Input」目标文本的信息提取和分享反馈，回复的内容风格遵循「ReplyStyle」，请在「OutputJSON」模块后面使用 **JSON** 格式进行输出。\n",
    "\n",
    "<::Definition::>\n",
    "<定义广告文章的标准>\n",
    "1. 找出文章中是否存在「产品广告」，类似于：腾讯智影视。\n",
    "2. 类似于 ChatGPT，Midjourney，DALL-E，NewBing，Google，Bard，Claude，Llama 等基座模型产品和 LangChain、LlamaIndex 工具则不为广告产品，你需要找除了他们之外的产品是否为广告产品。\n",
    "产品推广性质明显：文章主要内容围绕一个具体的产品——腾讯智影进行展开，详细介绍了其功能和使用方法。通常来说，一篇纯粹分享技术或工具的文章不会这么详尽地描述一个特定产品的使用过程和优势。\n",
    "链接引导：在文章中嵌入了产品的官方网站链接和其他相关链接，意图引导读者点击，这通常是广告或推销文章的常见做法。\n",
    "优势展示：文章大量展示了产品的优势和亮点，如“AI技术”、“功能强大”、“基于云端处理”等，并未对产品的缺点或局限性进行真实的展示，这种单方面强调优点的写作风格倾向于广告文。\n",
    "图文并茂的展示：使用了大量的图像来展示软件的功能和使用界面，增强了描述的可信度和吸引力。常规的内容创作可能不会如此精细地展示产品界面和操作过程。\n",
    "指导性强：文章内容具有明显的指导性，详细介绍了如何使用该工具进行视频创作，步骤明确，这与一些教程类广告内容相符。\n",
    "过于正面的评价：对于软件的评价过于正面，强调了许多优势，而在劣势部分却只字未提或者带过。真实的产品评测或分享通常会更加客观，会指出产品的不足之处或改进空间。\n",
    "广告词汇使用：使用了一些广告常用的词汇和表达方式，如“强大的AI智能工具”、“功能强大”、“极大提升创作效率”等，这些都是广告中常用的表达方式。\n",
    "\n",
    "对于类似的文章，我们可以从以下几个依据来判断其是否为广告文：\n",
    "\n",
    "是否过分强调产品或服务的优点和优势。\n",
    "是否包含引导读者采取特定行动（如点击链接、购买产品等）的内容或提示。\n",
    "是否存在过于正面的、缺乏客观性的评价。\n",
    "文章内容是否过分集中在某一产品或服务的展示和介绍上。\n",
    "</>\n",
    "<什么是金句？>\n",
    "1. 金句是指文章中的一句话，它能够准确地表达出文章的主旨，或者是文章中的一个非共识的重要观点。\n",
    "2. 金句是一个完整的句子，而不是一个短语或者一个词，它能够独立成为一个句子。需要以句号结尾，如果没有，请你在合适的地方结束句子并添加句号。\n",
    "</>\n",
    "\n",
    "<::ReplyStyle::>\n",
    "1. 直言不讳，一针见血，不拖泥带水，信息粒度高，信息效率高，使用大白话讲清楚明白。\n",
    "2. 只「Output」JSON文本，不要做其他超出JSON格式外的解释。\n",
    "3. 请使用地道的中文来设置「positiveReply」。\n",
    "4. 对所有字段内容中存在一些重要的信息或者概念时，使用markdown的加粗语法「**+目标文本+**」将目标修饰。\n",
    "5. **不要在JSON中使用/t和/n等字符，压缩JSON为无美化格式。**\n",
    "\n",
    "<::OutputFormat::>\n",
    "{\n",
    "\t\"title\": \"<根据文章整体内容重新设置一个客观的标题（客观用于减少那种标题党的噱头），区别于原来的标题，产品名——标题>\",\n",
    "\t\"keySentences\": [\n",
    "\t\t\"1. <提取出目标文章中的第一个金句>\",\n",
    "\t\t\"2. <提取出目标文章中的第二个金句>\",\n",
    "\t\t\"3. <提取出目标文章中的第三个金句>\"\n",
    "\t],\n",
    "\t\"summary\": \"<提取出整个目标文章中的信息摘要，不同/区别于「keySentences」>\",\n",
    "\t\"theme\": \"<提取出整个目标文章中的所围绕的一个主题是什么>\",\n",
    "\t\"usefulInformation\": \"<我作为大部分人，阅读这篇文章后，可以得到哪些认知层面的提升或者有价值的信息。>\",\n",
    "\t\"isAdvertisement\": {\n",
    "\t    \"BasisForThinkingCOT\": \"<如果「Input」中的「content」文章疑似为广告，请列出你认为它是广告的几个点，如果证据不足，那它就不算是广告。最后给出结论，该文章是否为广告文章？>\",\n",
    "\t    \"result\": \"<bool(true/false): 根据「BasisForThinkingCOT」判断结果，是否为广告文章，是则为true，否则为false>\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "<::Input::>\n",
    "{{InputText}}\n",
    "\n",
    "<::OutputJSON::>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_article(article: Dict[str, Any], prompt_template: str, model) -> Dict[str, Any]:\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template, template_format=\"jinja2\")\n",
    "    chain = prompt | model | JsonOutputParser()\n",
    "    _json = json.dumps(article)\n",
    "    _resp = chain.invoke({\"InputText\": _json})\n",
    "    return _resp\n",
    "\n",
    "\n",
    "def print_parsed_article(original_article: Dict[str, Any], parsed_article: Dict[str, Any]):\n",
    "    print(\"=========================================\")\n",
    "    print(\"「原标题」：\" + original_article['title'])\n",
    "    print(\"「客观标题」：\" + parsed_article['title'])\n",
    "    print(\"=========================================\")\n",
    "    print(\"「关键句」：\")\n",
    "    for key_sentence in parsed_article['keySentences']:\n",
    "        print(key_sentence)\n",
    "    print(\"「摘要」：\" + parsed_article['summary'])\n",
    "    print(\"「主题」：\" + parsed_article['theme'])\n",
    "    print(\"「有用信息」：\" + parsed_article['usefulInformation'])\n",
    "    print(\"「思考依据」：\" + parsed_article['isAdvertisement']['BasisForThinkingCOT'])\n",
    "    print(\"「是否广告」：\" + str(parsed_article['isAdvertisement']['result']))\n",
    "\n",
    "\n",
    "def batch_process_articles(articles: List[Dict[str, Any]], prompt_template: str, model):\n",
    "    for article in articles:\n",
    "        try:\n",
    "            parsed_article = parse_article(article, prompt_template, model)\n",
    "            print_parsed_article(article, parsed_article)\n",
    "        except Exception as e:\n",
    "            print(\"=========================================\")\n",
    "            print(\"「出错信息」：\" + str(e))\n",
    "\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_base=config_openai_api_base,\n",
    "    openai_api_key=config_openai_api_key,\n",
    "    model_name=config_openai_model_name,\n",
    "    openai_proxy=config_openai_proxy,\n",
    "    temperature=float(config_openai_temperature),\n",
    ")\n",
    "batch_process_articles(cleaned_entries[30:], template, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T08:12:09.003945Z",
     "start_time": "2023-10-08T08:08:09.487593Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
